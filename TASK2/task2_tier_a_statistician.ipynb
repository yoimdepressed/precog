{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2f20b11",
   "metadata": {},
   "source": [
    "# TASK 2 - TIER A: The Statistician\n",
    "\n",
    "## Goal\n",
    "Build a **Random Forest/XGBoost classifier** using only numerical linguistic features from Task 1.\n",
    "\n",
    "---\n",
    "\n",
    "## The Approach\n",
    "\n",
    "### What We're Building\n",
    "- **Input:** Numerical features (sentence variance, punctuation, readability, etc.)\n",
    "- **Output:** 3-class prediction (Human, AI Vanilla, AI Styled)\n",
    "- **Method:** Traditional ML (Random Forest first, then XGBoost)\n",
    "\n",
    "### The Challenge\n",
    "**Dataset (Twain + Austen):**\n",
    "- Class 1 (Human): 470 paragraphs\n",
    "- Class 2 (AI Vanilla): 464 paragraphs  \n",
    "- Class 3 (AI Styled): 494 paragraphs\n",
    "- **Total:** 1428 paragraphs (relatively balanced!)\n",
    "\n",
    "**Key Insights from Task 1:**\n",
    "1. **Sentence variance is THE STRONGEST signal** (22œÉ effect!)\n",
    "2. Length bias: Human 134 words, AI 90-97 words (37-44 word gap)\n",
    "3. Punctuation: Human uses 3x-31x more dramatic punctuation\n",
    "\n",
    "**Expected Challenges:**\n",
    "1. Length bias may help OR hurt (confounds TTR/Hapax)\n",
    "2. AI Vanilla vs AI Styled may be hard to distinguish (both AI)\n",
    "3. Will the 22œÉ sentence variance translate to high accuracy?\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Set (10 Features)\n",
    "\n",
    "Based on Task 1 analysis:\n",
    "\n",
    "### ‚úÖ Strong Discriminative Features\n",
    "1. **Sentence Length Mean** (avg words per sentence)\n",
    "2. **Sentence Length Variance** ‚≠ê (Human 3x more, strongest signal)\n",
    "3. **Paragraph Length** (total words - controls for length bias)\n",
    "4. **TTR** (Type-Token Ratio - vocabulary diversity)\n",
    "5. **Flesch-Kincaid Grade Mean** (readability)\n",
    "6. **Flesch-Kincaid Grade Variance** (Human 2x more)\n",
    "7. **Semicolons per 1000 words** (Human 3x more)\n",
    "8. **Em-dashes per 1000 words** (Human 1.6x more)\n",
    "9. **Exclamations per 1000 words** (Human >> AI)\n",
    "10. **Commas per 1000 words**\n",
    "\n",
    "### ‚ùå Excluded Features\n",
    "- **Hapax Ratio** (redundant with TTR, same length bias)\n",
    "- **Adj/Noun Ratio** (p=0.812, no signal)\n",
    "\n",
    "---\n",
    "\n",
    "## Prediction\n",
    "\n",
    "**If Features Work:**\n",
    "- Expected accuracy: 70-85% (3-way classification)\n",
    "- Human should be easily distinguished (sentence variance)\n",
    "- AI Vanilla vs AI Styled may overlap\n",
    "\n",
    "**If Features Struggle:**\n",
    "- Accuracy: 40-60% (barely better than random 33%)\n",
    "- Root cause: Era differences mask human/AI signal OR AI styled successfully mimics Victorian patterns\n",
    "\n",
    "**Either outcome is valid research!** We document WHY."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e885d27",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206889ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from textstat import flesch_kincaid_grade\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    classification_report, \n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "# XGBoost (optional - we'll use Random Forest as primary model)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "    print(\"‚úÖ XGBoost available\")\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è XGBoost not installed (optional - we'll focus on Random Forest)\")\n",
    "    print(\"   To install: pip install xgboost\")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ All required libraries imported successfully!\")\n",
    "print(\"‚úÖ Random Forest will be our primary model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb04d1a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Download NLTK Data (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53e2b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "import nltk\n",
    "\n",
    "print(\"Downloading NLTK data (this may take a moment)...\\n\")\n",
    "\n",
    "# Download punkt tokenizer\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"‚úÖ NLTK punkt already downloaded\")\n",
    "except LookupError:\n",
    "    print(\"üì• Downloading NLTK punkt tokenizer...\")\n",
    "    nltk.download('punkt', quiet=False)\n",
    "    print(\"‚úÖ punkt downloaded!\")\n",
    "\n",
    "# Download punkt_tab (newer version needed by some NLTK versions)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "    print(\"‚úÖ NLTK punkt_tab already downloaded\")\n",
    "except LookupError:\n",
    "    print(\"üì• Downloading NLTK punkt_tab...\")\n",
    "    nltk.download('punkt_tab', quiet=False)\n",
    "    print(\"‚úÖ punkt_tab downloaded!\")\n",
    "\n",
    "# Download POS tagger\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "    print(\"‚úÖ NLTK POS tagger already downloaded\")\n",
    "except LookupError:\n",
    "    print(\"üì• Downloading NLTK POS tagger...\")\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=False)\n",
    "    print(\"‚úÖ POS tagger downloaded!\")\n",
    "\n",
    "# Download averaged_perceptron_tagger_eng (newer version)\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger_eng')\n",
    "    print(\"‚úÖ NLTK POS tagger (eng) already downloaded\")\n",
    "except LookupError:\n",
    "    print(\"üì• Downloading NLTK POS tagger (eng)...\")\n",
    "    nltk.download('averaged_perceptron_tagger_eng', quiet=False)\n",
    "    print(\"‚úÖ POS tagger (eng) downloaded!\")\n",
    "\n",
    "print(\"\\n‚úÖ All NLTK data ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3949a48b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Load Dataset\n",
    "\n",
    "Load all three classes from Task 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56144918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths (updated for Twain + Austen dataset)\n",
    "DATA_DIR = Path('../TASK0/data/dataset/twain_austen')\n",
    "CLASS1_FILE = DATA_DIR / 'class1_human.jsonl'\n",
    "CLASS2_FILE = DATA_DIR / 'class2.txt'\n",
    "CLASS3_FILE = DATA_DIR / 'class3.txt'\n",
    "\n",
    "print(\"Loading datasets...\\n\")\n",
    "\n",
    "# Load Class 1 (Human - JSONL format)\n",
    "class1_texts = []\n",
    "if CLASS1_FILE.exists():    \n",
    "    with open(CLASS1_FILE, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():  # Skip empty lines\n",
    "                data = json.loads(line.strip())\n",
    "                class1_texts.append(data['text'])\n",
    "    print(f\"‚úÖ Class 1 (Human): {len(class1_texts)} paragraphs loaded\")\n",
    "else:\n",
    "    print(f\"‚ùå Class 1 file not found: {CLASS1_FILE}\")\n",
    "\n",
    "# Load Class 2 (AI Vanilla - TXT format, one paragraph per line)\n",
    "class2_texts = []\n",
    "if CLASS2_FILE.exists():\n",
    "    with open(CLASS2_FILE, 'r', encoding='utf-8') as f:\n",
    "        class2_texts = [line.strip() for line in f if line.strip()]\n",
    "    print(f\"‚úÖ Class 2 (AI Vanilla): {len(class2_texts)} paragraphs loaded\")\n",
    "else:\n",
    "    print(f\"‚ùå Class 2 file not found: {CLASS2_FILE}\")\n",
    "\n",
    "# Load Class 3 (AI Styled - TXT format, one paragraph per line)\n",
    "class3_texts = []\n",
    "if CLASS3_FILE.exists():\n",
    "    with open(CLASS3_FILE, 'r', encoding='utf-8') as f:\n",
    "        class3_texts = [line.strip() for line in f if line.strip()]\n",
    "    print(f\"‚úÖ Class 3 (AI Styled): {len(class3_texts)} paragraphs loaded\")\n",
    "else:\n",
    "    print(f\"‚ùå Class 3 file not found: {CLASS3_FILE}\")\n",
    "\n",
    "# Create combined dataset\n",
    "all_texts = class1_texts + class2_texts + class3_texts\n",
    "all_labels = (\n",
    "    ['Human'] * len(class1_texts) + \n",
    "    ['AI_Vanilla'] * len(class2_texts) + \n",
    "    ['AI_Styled'] * len(class3_texts)\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Total dataset: {len(all_texts)} paragraphs\")\n",
    "\n",
    "if len(all_texts) > 0:\n",
    "    print(f\"   - Human: {len(class1_texts)} ({len(class1_texts)/len(all_texts)*100:.1f}%)\")\n",
    "    print(f\"   - AI Vanilla: {len(class2_texts)} ({len(class2_texts)/len(all_texts)*100:.1f}%)\")\n",
    "    print(f\"   - AI Styled: {len(class3_texts)} ({len(class3_texts)/len(all_texts)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"‚ùå ERROR: No data loaded!\")\n",
    "    print(\"   Please run task0_data_creation.ipynb first to generate the dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3c2cb6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Feature Engineering\n",
    "\n",
    "Extract all 10 numerical features from each paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf1a089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    \"\"\"\n",
    "    Extract 10 linguistic features from a text paragraph.\n",
    "    \n",
    "    Returns dict with features:\n",
    "    1. sent_length_mean\n",
    "    2. sent_length_variance\n",
    "    3. para_length (total words)\n",
    "    4. ttr (Type-Token Ratio)\n",
    "    5. fk_grade_mean\n",
    "    6. fk_grade_variance\n",
    "    7. semicolons_per_1k\n",
    "    8. emdashes_per_1k\n",
    "    9. exclamations_per_1k\n",
    "    10. commas_per_1k\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Tokenize words (alphanumeric only, lowercase)\n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [w for w in words if w.isalnum()]\n",
    "    \n",
    "    # 1-2: Sentence length statistics\n",
    "    sent_lengths = [len(word_tokenize(s)) for s in sentences]\n",
    "    sent_length_mean = np.mean(sent_lengths) if sent_lengths else 0\n",
    "    sent_length_variance = np.var(sent_lengths) if len(sent_lengths) > 1 else 0\n",
    "    \n",
    "    # 3: Paragraph length\n",
    "    para_length = len(words)\n",
    "    \n",
    "    # 4: Type-Token Ratio (TTR)\n",
    "    unique_words = set(words)\n",
    "    ttr = len(unique_words) / len(words) if words else 0\n",
    "    \n",
    "    # 5-6: Flesch-Kincaid Grade statistics\n",
    "    fk_grades = []\n",
    "    for sent in sentences:\n",
    "        if len(sent.split()) > 3:  # Need at least 3 words for FK\n",
    "            try:\n",
    "                grade = flesch_kincaid_grade(sent)\n",
    "                fk_grades.append(grade)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    fk_grade_mean = np.mean(fk_grades) if fk_grades else 0\n",
    "    fk_grade_variance = np.var(fk_grades) if len(fk_grades) > 1 else 0\n",
    "    \n",
    "    # 7-10: Punctuation density (per 1000 words)\n",
    "    # Normalize by word count to handle length differences\n",
    "    word_count_normalized = max(para_length / 1000, 0.001)  # Avoid division by zero\n",
    "    \n",
    "    semicolons = text.count(';')\n",
    "    emdashes = text.count('‚Äî') + text.count('--')  # Count both em-dash variants\n",
    "    exclamations = text.count('!')\n",
    "    commas = text.count(',')\n",
    "    \n",
    "    semicolons_per_1k = semicolons / word_count_normalized\n",
    "    emdashes_per_1k = emdashes / word_count_normalized\n",
    "    exclamations_per_1k = exclamations / word_count_normalized\n",
    "    commas_per_1k = commas / word_count_normalized\n",
    "    \n",
    "    return {\n",
    "        'sent_length_mean': sent_length_mean,\n",
    "        'sent_length_variance': sent_length_variance,\n",
    "        'para_length': para_length,\n",
    "        'ttr': ttr,\n",
    "        'fk_grade_mean': fk_grade_mean,\n",
    "        'fk_grade_variance': fk_grade_variance,\n",
    "        'semicolons_per_1k': semicolons_per_1k,\n",
    "        'emdashes_per_1k': emdashes_per_1k,\n",
    "        'exclamations_per_1k': exclamations_per_1k,\n",
    "        'commas_per_1k': commas_per_1k\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Feature extraction function defined\")\n",
    "print(\"\\nTesting on sample text...\")\n",
    "\n",
    "# Test on first paragraph\n",
    "if all_texts:\n",
    "    sample_features = extract_features(all_texts[0])\n",
    "    print(\"\\nSample features:\")\n",
    "    for feature, value in sample_features.items():\n",
    "        print(f\"  {feature}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da116718",
   "metadata": {},
   "source": [
    "### Extract Features for All Paragraphs\n",
    "\n",
    "This may take 1-2 minutes for 1000 paragraphs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7632fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting features for all paragraphs...\")\n",
    "print(\"This may take 1-2 minutes...\\n\")\n",
    "\n",
    "# Extract features for all texts\n",
    "features_list = []\n",
    "for i, text in enumerate(all_texts):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"  Processed {i + 1}/{len(all_texts)} paragraphs...\")\n",
    "    \n",
    "    features = extract_features(text)\n",
    "    features['label'] = all_labels[i]\n",
    "    features['text_preview'] = text[:100] + '...'  # For debugging\n",
    "    features_list.append(features)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(features_list)\n",
    "\n",
    "print(f\"\\n‚úÖ Feature extraction complete!\")\n",
    "print(f\"   Dataset shape: {df.shape}\")\n",
    "print(f\"   Features: {len(df.columns) - 2} numerical + 1 label + 1 text preview\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nüìä First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df25eefd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Exploratory Data Analysis (EDA)\n",
    "\n",
    "Visualize feature distributions across classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a927bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics by class\n",
    "print(\"üìä FEATURE STATISTICS BY CLASS\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "feature_cols = [\n",
    "    'sent_length_mean', 'sent_length_variance', 'para_length', 'ttr',\n",
    "    'fk_grade_mean', 'fk_grade_variance', 'semicolons_per_1k', \n",
    "    'emdashes_per_1k', 'exclamations_per_1k', 'commas_per_1k'\n",
    "]\n",
    "\n",
    "for feature in feature_cols:\n",
    "    print(f\"\\n{feature.upper().replace('_', ' ')}:\")\n",
    "    print(\"-\" * 80)\n",
    "    stats = df.groupby('label')[feature].agg(['mean', 'std', 'min', 'max'])\n",
    "    print(stats.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c09edf",
   "metadata": {},
   "source": [
    "### Visualize Key Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fe337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for top 6 features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Distribution of Key Features by Class', fontsize=16, fontweight='bold')\n",
    "\n",
    "top_features = [\n",
    "    'sent_length_variance',\n",
    "    'semicolons_per_1k',\n",
    "    'fk_grade_variance',\n",
    "    'para_length',\n",
    "    'ttr',\n",
    "    'exclamations_per_1k'\n",
    "]\n",
    "\n",
    "for idx, feature in enumerate(top_features):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    # Box plot\n",
    "    df.boxplot(column=feature, by='label', ax=ax)\n",
    "    ax.set_title(feature.replace('_', ' ').title())\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Value')\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualizations complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fee275",
   "metadata": {},
   "source": [
    "### Feature Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2491561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df[feature_cols].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1)\n",
    "plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Key Observations:\")\n",
    "print(\"   - High correlations (>0.7) indicate redundant features\")\n",
    "print(\"   - Look for TTR vs para_length correlation (length bias)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cc6550",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Prepare Data for ML\n",
    "\n",
    "Split into train/test sets with stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b47312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and labels\n",
    "X = df[feature_cols].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Encode labels (Human=0, AI_Vanilla=1, AI_Styled=2)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(\"Label encoding:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {label}: {i}\")\n",
    "\n",
    "# Train/test split (80/20) with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_encoded  # Maintain class proportions\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Data Split:\")\n",
    "print(f\"   Training set: {len(X_train)} samples\")\n",
    "print(f\"   Test set: {len(X_test)} samples\")\n",
    "\n",
    "# Show class distribution in train/test\n",
    "print(f\"\\n   Train class distribution:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    count = np.sum(y_train == i)\n",
    "    print(f\"      {label}: {count} ({count/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n   Test class distribution:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    count = np.sum(y_test == i)\n",
    "    print(f\"      {label}: {count} ({count/len(y_test)*100:.1f}%)\")\n",
    "\n",
    "# Feature scaling (standardize to mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\n‚úÖ Feature scaling complete (mean=0, std=1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a6529c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Baseline Model - Random Forest\n",
    "\n",
    "Train a Random Forest classifier with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbe4329",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üå≤ Training Random Forest Classifier...\\n\")\n",
    "\n",
    "# Random Forest with balanced class weights\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,          # 100 trees\n",
    "    max_depth=10,              # Prevent overfitting\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    class_weight='balanced',   # Handle class imbalance\n",
    "    random_state=42,\n",
    "    n_jobs=-1                  # Use all CPU cores\n",
    ")\n",
    "\n",
    "# Train model\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"‚úÖ Training complete!\")\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = rf_model.predict(X_train_scaled)\n",
    "y_test_pred = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracies\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\nüìä RESULTS - Random Forest\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   Training Accuracy: {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "print(f\"   Test Accuracy:     {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"   Baseline (random): {1/3:.4f} ({100/3:.2f}%)\")\n",
    "\n",
    "# Check for overfitting\n",
    "if train_acc - test_acc > 0.1:\n",
    "    print(f\"\\n‚ö†Ô∏è Warning: Possible overfitting (train-test gap: {(train_acc-test_acc)*100:.1f}%)\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Good generalization (train-test gap: {(train_acc-test_acc)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29087651",
   "metadata": {},
   "source": [
    "### Detailed Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d569532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"\\nüìã CLASSIFICATION REPORT (Test Set)\\n\")\n",
    "print(\"=\" * 70)\n",
    "print(classification_report(\n",
    "    y_test, \n",
    "    y_test_pred, \n",
    "    target_names=label_encoder.classes_,\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nüìä PER-CLASS ACCURACY:\\n\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    mask = y_test == i\n",
    "    if np.sum(mask) > 0:\n",
    "        class_acc = np.sum((y_test[mask] == y_test_pred[mask])) / np.sum(mask)\n",
    "        print(f\"   {label}: {class_acc:.4f} ({class_acc*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd95505",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac388e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Random Forest', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Confusion Matrix Analysis:\")\n",
    "print(\"   Diagonal = Correct predictions\")\n",
    "print(\"   Off-diagonal = Misclassifications\")\n",
    "print(\"\\n   Look for:\")\n",
    "print(\"   - Are AI_Vanilla and AI_Styled confused with each other?\")\n",
    "print(\"   - Is Human well-separated from both AI classes?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f452fb3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: Feature Importance Analysis\n",
    "\n",
    "Which features matter most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2244f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"üîç FEATURE IMPORTANCE (Random Forest)\\n\")\n",
    "print(\"=\" * 60)\n",
    "for idx, row in feature_importance.iterrows():\n",
    "    print(f\"   {row['feature']:25s}: {row['importance']:.4f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=feature_importance, x='importance', y='feature', palette='viridis')\n",
    "plt.title('Feature Importance - Random Forest', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Top 3 most important features:\")\n",
    "for i in range(min(3, len(feature_importance))):\n",
    "    row = feature_importance.iloc[i]\n",
    "    print(f\"   {i+1}. {row['feature']}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82855e0b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 9: Cross-Validation\n",
    "\n",
    "Verify robustness with 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48defaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running 5-fold cross-validation...\")\n",
    "print(\"This may take 1-2 minutes...\\n\")\n",
    "\n",
    "# Stratified K-Fold (maintains class proportions in each fold)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Cross-validation scores\n",
    "cv_scores = cross_val_score(\n",
    "    rf_model, \n",
    "    X_train_scaled, \n",
    "    y_train, \n",
    "    cv=cv, \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"üìä CROSS-VALIDATION RESULTS (5-Fold)\\n\")\n",
    "print(\"=\" * 60)\n",
    "for i, score in enumerate(cv_scores, 1):\n",
    "    print(f\"   Fold {i}: {score:.4f} ({score*100:.2f}%)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   Mean:    {cv_scores.mean():.4f} ({cv_scores.mean()*100:.2f}%)\")\n",
    "print(f\"   Std Dev: {cv_scores.std():.4f}\")\n",
    "print(f\"\\n‚úÖ Low std dev = consistent performance across folds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
