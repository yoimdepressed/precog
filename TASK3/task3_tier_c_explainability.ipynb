{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7310389f",
   "metadata": {},
   "source": [
    "# TASK 3: The Smoking Gun - Tier C (97.20% Champion) Explainability\n",
    "\n",
    "## Objective\n",
    "\n",
    "**Why does Tier C think a text is AI-generated?**\n",
    "\n",
    "We'll use **Captum** (LayerIntegratedGradients) to:\n",
    "1. ğŸ” **Saliency Mapping:** Highlight words/tokens that signal \"AI\" to Tier C\n",
    "2. ğŸ¯ **The Findings:** Does it detect AI-isms (\"tapestry,\" \"delve\") or sentence rhythm?\n",
    "3. âŒ **Error Analysis:** Find 3 Human texts misclassified as AI - why did the model fail?\n",
    "\n",
    "---\n",
    "\n",
    "## Model: Tier C (DistilBERT + LoRA)\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| **Test Accuracy** | **97.20%** ğŸ† |\n",
    "| **Human Detection** | **100.00%** ğŸ‰ |\n",
    "| **AI_Styled Detection** | 93.94% |\n",
    "| **AI_Vanilla Detection** | 93.33% |\n",
    "| **Dataset** | Twain + Austen (class1_human.jsonl, class2.txt, class3.txt) |\n",
    "\n",
    "---\n",
    "\n",
    "## Why Captum (Not SHAP)?\n",
    "\n",
    "| Feature | SHAP | Captum |\n",
    "|---------|------|--------|\n",
    "| **Speed** | Slow (5-10 min per paragraph) | **Fast (30-60 sec)** âœ… |\n",
    "| **Method** | Model-agnostic (KernelExplainer) | Gradient-based (Integrated Gradients) |\n",
    "| **Best For** | Averaged embeddings (Tier B) | **Transformers (Tier C)** âœ… |\n",
    "| **Granularity** | Word-level | **Token-level (subwords)** âœ… |\n",
    "| **Accuracy** | Good | **Theoretically grounded** âœ… |\n",
    "\n",
    "**Bottom Line:** Captum is the industry standard for transformer explainability.\n",
    "\n",
    "---\n",
    "\n",
    "Let's begin! ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29027657",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Install & Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaa70073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Captum installed!\n"
     ]
    }
   ],
   "source": [
    "# Install Captum\n",
    "!pip install -q captum\n",
    "\n",
    "print(\"âœ… Captum installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01adfeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n",
      "   PyTorch version: 2.8.0+cu128\n",
      "   Device: cpu\n",
      "   Captum version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch & Transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Captum for explainability\n",
    "from captum.attr import LayerIntegratedGradients, visualization\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"   PyTorch version: {torch.__version__}\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Captum version: {__import__('captum').__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a9d3a3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Load Tier C Model (97.20% Champion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae2b11db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tier C model (DistilBERT + LoRA)...\n",
      "   Model path: ../TASK2/task2_tier_c_distilbert_lora\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab6275fe19245b3b6ab4155fa103a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDistilBertForSequenceClassification LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
      "Key                     | Status     | \n",
      "------------------------+------------+-\n",
      "vocab_projector.bias    | UNEXPECTED | \n",
      "vocab_transform.weight  | UNEXPECTED | \n",
      "vocab_layer_norm.weight | UNEXPECTED | \n",
      "vocab_layer_norm.bias   | UNEXPECTED | \n",
      "vocab_transform.bias    | UNEXPECTED | \n",
      "classifier.bias         | MISSING    | \n",
      "pre_classifier.bias     | MISSING    | \n",
      "classifier.weight       | MISSING    | \n",
      "pre_classifier.weight   | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97bd2109fe224c218dfbebcf44c0b3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDistilBertForSequenceClassification LOAD REPORT\u001b[0m from: ../TASK2/task2_tier_c_distilbert_lora\n",
      "Key                                                                                                    | Status     | \n",
      "-------------------------------------------------------------------------------------------------------+------------+-\n",
      "base_model.model.distilbert.transformer.layer.{0, 1, 2, 3, 4, 5}.attention.v_lin.lora_A.default.weight | UNEXPECTED | \n",
      "base_model.model.distilbert.transformer.layer.{0, 1, 2, 3, 4, 5}.attention.q_lin.lora_B.default.weight | UNEXPECTED | \n",
      "base_model.model.classifier.bias                                                                       | UNEXPECTED | \n",
      "base_model.model.classifier.weight                                                                     | UNEXPECTED | \n",
      "base_model.model.distilbert.transformer.layer.{0, 1, 2, 3, 4, 5}.attention.v_lin.lora_B.default.weight | UNEXPECTED | \n",
      "base_model.model.distilbert.transformer.layer.{0, 1, 2, 3, 4, 5}.attention.q_lin.lora_A.default.weight | UNEXPECTED | \n",
      "base_model.model.pre_classifier.bias                                                                   | UNEXPECTED | \n",
      "base_model.model.pre_classifier.weight                                                                 | UNEXPECTED | \n",
      "distilbert.transformer.layer.{0, 1, 2, 3, 4, 5}.attention.v_lin.lora_B.default.weight                  | MISSING    | \n",
      "pre_classifier.modules_to_save.default.weight                                                          | MISSING    | \n",
      "distilbert.transformer.layer.{0, 1, 2, 3, 4, 5}.attention.q_lin.lora_A.default.weight                  | MISSING    | \n",
      "distilbert.transformer.layer.{0, 1, 2, 3, 4, 5}.attention.q_lin.lora_B.default.weight                  | MISSING    | \n",
      "distilbert.transformer.layer.{0, 1, 2, 3, 4, 5}.attention.v_lin.lora_A.default.weight                  | MISSING    | \n",
      "classifier.modules_to_save.default.weight                                                              | MISSING    | \n",
      "classifier.modules_to_save.default.bias                                                                | MISSING    | \n",
      "pre_classifier.modules_to_save.default.bias                                                            | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tier C model loaded successfully!\n",
      "\n",
      "ğŸ“Š Model Details:\n",
      "   Model type: distilbert\n",
      "   Vocabulary size: 30,522\n",
      "   Max sequence length: 512\n",
      "   Number of classes: 3\n",
      "   Device: cpu\n",
      "\n",
      "ğŸ”¢ Parameters:\n",
      "   Total: 67,696,134\n",
      "   Trainable (LoRA): 0\n",
      "   Frozen: 67,696,134\n",
      "\n",
      "ğŸ“‹ Label Mapping:\n",
      "   0: AI_Styled\n",
      "   1: AI_Vanilla\n",
      "   2: Human\n"
     ]
    }
   ],
   "source": [
    "# Load Tier C model (saved from Twain training)\n",
    "MODEL_PATH = Path('../TASK2/task2_tier_c_distilbert_lora')\n",
    "\n",
    "print(\"Loading Tier C model (DistilBERT + LoRA)...\")\n",
    "print(f\"   Model path: {MODEL_PATH}\\n\")\n",
    "\n",
    "if MODEL_PATH.exists():\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        num_labels=3\n",
    "    )\n",
    "    model.to(device)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    print(\"âœ… Tier C model loaded successfully!\")\n",
    "    print(f\"\\nğŸ“Š Model Details:\")\n",
    "    print(f\"   Model type: {model.config.model_type}\")\n",
    "    print(f\"   Vocabulary size: {tokenizer.vocab_size:,}\")\n",
    "    print(f\"   Max sequence length: {tokenizer.model_max_length}\")\n",
    "    print(f\"   Number of classes: 3\")\n",
    "    print(f\"   Device: {device}\")\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\nğŸ”¢ Parameters:\")\n",
    "    print(f\"   Total: {total_params:,}\")\n",
    "    print(f\"   Trainable (LoRA): {trainable_params:,}\")\n",
    "    print(f\"   Frozen: {total_params - trainable_params:,}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âŒ Model not found at: {MODEL_PATH}\")\n",
    "    print(f\"   Please ensure Task 2 Tier C training is complete.\")\n",
    "    print(f\"\\nğŸ’¡ To train the model, run:\")\n",
    "    print(f\"   ../TASK2/task2_tier_c_transformer.ipynb\")\n",
    "\n",
    "# Define label mapping (same as training)\n",
    "# LabelEncoder sorts alphabetically:\n",
    "#   0: AI_Styled (class3.txt)\n",
    "#   1: AI_Vanilla (class2.txt) \n",
    "#   2: Human (class1_human.jsonl)\n",
    "label_names = ['AI_Styled', 'AI_Vanilla', 'Human']\n",
    "\n",
    "print(f\"\\nğŸ“‹ Label Mapping:\")\n",
    "for i, name in enumerate(label_names):\n",
    "    print(f\"   {i}: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1416efd3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Load Test Data\n",
    "\n",
    "We'll load the same dataset used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48cf4704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Human: 470 paragraphs loaded\n",
      "âœ… AI Vanilla: 464 paragraphs loaded\n",
      "âœ… AI Styled: 494 paragraphs loaded\n",
      "\n",
      "ğŸ“Š Dataset Summary:\n",
      "   Human: 470\n",
      "   AI Vanilla: 464\n",
      "   AI Styled: 494\n",
      "   Total: 1428\n"
     ]
    }
   ],
   "source": [
    "# Load dataset (NEW Twain + Austen from TASK 0)\n",
    "DATA_DIR = Path('../TASK0/data/dataset/twain_austen')\n",
    "\n",
    "# Load Human texts (class1_human.jsonl)\n",
    "HUMAN_FILE = DATA_DIR / 'class1_human.jsonl'\n",
    "human_texts = []\n",
    "if HUMAN_FILE.exists():\n",
    "    with open(HUMAN_FILE, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                data = json.loads(line)\n",
    "                human_texts.append(data['text'])\n",
    "    print(f\"âœ… Human: {len(human_texts)} paragraphs loaded\")\n",
    "else:\n",
    "    print(f\"âŒ Human file not found: {HUMAN_FILE}\")\n",
    "\n",
    "# Load AI Vanilla (class2.txt)\n",
    "AI_VANILLA_FILE = DATA_DIR / 'class2.txt'\n",
    "ai_vanilla_texts = []\n",
    "if AI_VANILLA_FILE.exists():\n",
    "    with open(AI_VANILLA_FILE, 'r', encoding='utf-8') as f:\n",
    "        ai_vanilla_texts = [line.strip() for line in f if line.strip()]\n",
    "    print(f\"âœ… AI Vanilla: {len(ai_vanilla_texts)} paragraphs loaded\")\n",
    "else:\n",
    "    print(f\"âŒ AI Vanilla file not found: {AI_VANILLA_FILE}\")\n",
    "\n",
    "# Load AI Styled (class3.txt)\n",
    "AI_STYLED_FILE = DATA_DIR / 'class3.txt'\n",
    "ai_styled_texts = []\n",
    "if AI_STYLED_FILE.exists():\n",
    "    with open(AI_STYLED_FILE, 'r', encoding='utf-8') as f:\n",
    "        ai_styled_texts = [line.strip() for line in f if line.strip()]\n",
    "    print(f\"âœ… AI Styled: {len(ai_styled_texts)} paragraphs loaded\")\n",
    "else:\n",
    "    print(f\"âŒ AI Styled file not found: {AI_STYLED_FILE}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset Summary:\")\n",
    "print(f\"   Human: {len(human_texts)}\")\n",
    "print(f\"   AI Vanilla: {len(ai_vanilla_texts)}\")\n",
    "print(f\"   AI Styled: {len(ai_styled_texts)}\")\n",
    "print(f\"   Total: {len(human_texts) + len(ai_vanilla_texts) + len(ai_styled_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba580076",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Select \"Imposter\" AI Paragraphs for Analysis\n",
    "\n",
    "Pick diverse AI paragraphs that Tier C correctly identifies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2db3d4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Selected 10 imposter paragraphs for analysis:\n",
      "   - AI Vanilla: 5\n",
      "   - AI Styled: 5\n",
      "\n",
      "ğŸ“„ Example Imposter Paragraph (AI Vanilla):\n",
      "   Ultimately, social hierarchy is maintained not just by economic disparity, but by the complicity of all participants in the system. The deference shown to superiors and the condescension shown to infe...\n"
     ]
    }
   ],
   "source": [
    "# Select diverse examples\n",
    "np.random.seed(42)\n",
    "\n",
    "# Pick 5 AI Vanilla + 5 AI Styled\n",
    "n_samples = 5\n",
    "\n",
    "selected_ai_vanilla = np.random.choice(ai_vanilla_texts, size=min(n_samples, len(ai_vanilla_texts)), replace=False)\n",
    "selected_ai_styled = np.random.choice(ai_styled_texts, size=min(n_samples, len(ai_styled_texts)), replace=False)\n",
    "\n",
    "# Combine\n",
    "imposter_paragraphs = list(selected_ai_vanilla) + list(selected_ai_styled)\n",
    "imposter_labels = ['AI_Vanilla'] * len(selected_ai_vanilla) + ['AI_Styled'] * len(selected_ai_styled)\n",
    "\n",
    "print(f\"âœ… Selected {len(imposter_paragraphs)} imposter paragraphs for analysis:\")\n",
    "print(f\"   - AI Vanilla: {len(selected_ai_vanilla)}\")\n",
    "print(f\"   - AI Styled: {len(selected_ai_styled)}\")\n",
    "\n",
    "# Preview first paragraph\n",
    "print(f\"\\nğŸ“„ Example Imposter Paragraph (AI Vanilla):\")\n",
    "print(f\"   {selected_ai_vanilla[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390d0cab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Verify Model Predictions\n",
    "\n",
    "Ensure Tier C correctly identifies these as AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fd8ccd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Model Predictions on Selected Imposters:\n",
      "\n",
      "========================================================================================================================\n",
      "True_Label Predicted_Label  Confidence  Correct                                                                        Text_Preview\n",
      "AI_Vanilla      AI_Vanilla    0.337491     True Ultimately, social hierarchy is maintained not just by economic disparity, but b...\n",
      "AI_Vanilla       AI_Styled    0.342646    False Community gossip acts as an informal policing mechanism, a pervasive surveillanc...\n",
      "AI_Vanilla       AI_Styled    0.344706    False Ultimately, the resolution of this conflict rarely lies in a total rejection of ...\n",
      "AI_Vanilla       AI_Styled    0.339711    False In the digital age, the dynamics of village gossip have been globalized and perm...\n",
      "AI_Vanilla       AI_Styled    0.342286    False The \"court of public opinion\" operates without due process, evidence rules, or t...\n",
      " AI_Styled       AI_Styled    0.341611     True Gossip is the policing mechanism of the social hierarchy. It is through the whis...\n",
      " AI_Styled       AI_Styled    0.340810     True The first time a boy shavesâ€”or attempts to shave the peach fuzz on his chinâ€”is a...\n",
      " AI_Styled       AI_Styled    0.341370     True A woman of sense often finds herself the emotional anchor of a foolish family. S...\n",
      " AI_Styled       AI_Styled    0.341356     True The first time a boy sees a grown man cry, or act with cowardice, his world shak...\n",
      " AI_Styled           Human    0.338258    False A young imagination is a fiery thing, prone to constructing romances out of the ...\n",
      "========================================================================================================================\n",
      "\n",
      "âœ… Tier C Accuracy on Selected Imposters: 50.0%\n",
      "   (Expected: ~97% based on test set performance)\n",
      "\n",
      "ğŸ¯ Using 5 correctly classified paragraphs for saliency analysis.\n"
     ]
    }
   ],
   "source": [
    "# Get predictions for all imposter paragraphs\n",
    "predictions = []\n",
    "confidences = []\n",
    "\n",
    "for text in imposter_paragraphs:\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = softmax(logits, dim=1)\n",
    "    \n",
    "    # Get predicted class and confidence\n",
    "    pred_class = torch.argmax(probs, dim=1).item()\n",
    "    confidence = probs[0, pred_class].item()\n",
    "    \n",
    "    predictions.append(pred_class)\n",
    "    confidences.append(confidence)\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'True_Label': imposter_labels,\n",
    "    'Predicted_Label': [label_names[i] for i in predictions],\n",
    "    'Confidence': confidences,\n",
    "    'Correct': [true == label_names[pred] for true, pred in zip(imposter_labels, predictions)],\n",
    "    'Text_Preview': [text[:80] + '...' for text in imposter_paragraphs]\n",
    "})\n",
    "\n",
    "print(\"ğŸ“Š Model Predictions on Selected Imposters:\\n\")\n",
    "print(\"=\"*120)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = results_df['Correct'].mean()\n",
    "print(f\"\\nâœ… Tier C Accuracy on Selected Imposters: {accuracy*100:.1f}%\")\n",
    "print(f\"   (Expected: ~97% based on test set performance)\")\n",
    "\n",
    "# Filter to correctly classified for saliency analysis\n",
    "correct_mask = results_df['Correct'].values\n",
    "correctly_classified_texts = [text for text, correct in zip(imposter_paragraphs, correct_mask) if correct]\n",
    "correctly_classified_labels = [label for label, correct in zip(imposter_labels, correct_mask) if correct]\n",
    "\n",
    "print(f\"\\nğŸ¯ Using {len(correctly_classified_texts)} correctly classified paragraphs for saliency analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbc4035",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Select Sample for Detailed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fee0a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Analyzing Sample Paragraph (AI_Vanilla):\n",
      "\n",
      "Ultimately, social hierarchy is maintained not just by economic disparity, but by the complicity of all participants in the system. The deference shown to superiors and the condescension shown to inferiors are performative acts that reinforce the structure daily. Disrupting this dynamic requires more than a redistribution of resources; it requires a dismantling of the internalized deferential attitudes that keep the hierarchy in place. When individuals stop accepting their assigned value based on class and start evaluating worth based on merit and humanity, the foundation of the hierarchy begins to crumble. However, history suggests that human societies have a persistent tendency to reorganize themselves into tiered systems, suggesting that the drive for status and differentiation is a deeply ingrained aspect of the human condition.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ¯ Model Prediction:\n",
      "   Predicted: AI_Vanilla\n",
      "   Confidence: 33.75%\n",
      "   Class probabilities:\n",
      "      AI_Styled: 33.41%\n",
      "      AI_Vanilla: 33.75%\n",
      "      Human: 32.84%\n",
      "\n",
      "ğŸ“ Tokenization:\n",
      "   Number of tokens: 158\n",
      "   Tokens (first 30): ['[CLS]', 'ultimately', ',', 'social', 'hierarchy', 'is', 'maintained', 'not', 'just', 'by', 'economic', 'di', '##spar', '##ity', ',', 'but', 'by', 'the', 'com', '##plicity', 'of', 'all', 'participants', 'in', 'the', 'system', '.', 'the', 'def', '##erence']\n"
     ]
    }
   ],
   "source": [
    "# Select first correctly classified paragraph\n",
    "if len(correctly_classified_texts) > 0:\n",
    "    sample_text = correctly_classified_texts[0]\n",
    "    sample_label = correctly_classified_labels[0]\n",
    "    \n",
    "    print(f\"ğŸ” Analyzing Sample Paragraph ({sample_label}):\\n\")\n",
    "    print(f\"{sample_text}\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Get model prediction\n",
    "    inputs = tokenizer(\n",
    "        sample_text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = softmax(logits, dim=1)\n",
    "        pred_class = torch.argmax(probs, dim=1).item()\n",
    "        confidence = probs[0, pred_class].item()\n",
    "    \n",
    "    print(f\"\\nğŸ¯ Model Prediction:\")\n",
    "    print(f\"   Predicted: {label_names[pred_class]}\")\n",
    "    print(f\"   Confidence: {confidence*100:.2f}%\")\n",
    "    print(f\"   Class probabilities:\")\n",
    "    for i, name in enumerate(label_names):\n",
    "        print(f\"      {name}: {probs[0][i].item()*100:.2f}%\")\n",
    "    \n",
    "    # Get tokens\n",
    "    input_ids = inputs['input_ids'][0]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    \n",
    "    print(f\"\\nğŸ“ Tokenization:\")\n",
    "    print(f\"   Number of tokens: {len(tokens)}\")\n",
    "    print(f\"   Tokens (first 30): {tokens[:30]}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No correctly classified paragraphs found for analysis.\")\n",
    "    print(\"   Check if model loaded correctly and test data is available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edb695b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Compute Token Attribution with Captum\n",
    "\n",
    "Using **LayerIntegratedGradients** to find which tokens signal \"AI\".\n",
    "\n",
    "**This may take 30-60 seconds.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4bcf766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Computing token-level attribution with Integrated Gradients...\n",
      "\n",
      "   â³ This may take 30-60 seconds...\n",
      "\n",
      "ğŸ”§ Creating LayerIntegratedGradients explainer...\n",
      "âœ… Explainer created!\n",
      "\n",
      "â³ Computing attributions (30-60 seconds)...\n",
      "âœ… Attributions computed!\n",
      "   Attribution shape: torch.Size([1, 158, 768])\n",
      "   Token-level attributions shape: (158,)\n",
      "   Min attribution: -0.0026\n",
      "   Max attribution: 0.1251\n",
      "   Mean attribution: 0.0010\n",
      "\n",
      "âœ… Token attributions computed successfully!\n",
      "âœ… Attributions computed!\n",
      "   Attribution shape: torch.Size([1, 158, 768])\n",
      "   Token-level attributions shape: (158,)\n",
      "   Min attribution: -0.0026\n",
      "   Max attribution: 0.1251\n",
      "   Mean attribution: 0.0010\n",
      "\n",
      "âœ… Token attributions computed successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ” Computing token-level attribution with Integrated Gradients...\\n\")\n",
    "print(\"   â³ This may take 30-60 seconds...\\n\")\n",
    "\n",
    "# Define forward function for Captum\n",
    "def model_forward(input_ids, attention_mask=None):\n",
    "    \"\"\"\n",
    "    Forward function for Captum.\n",
    "    Returns logits for the model.\n",
    "    \"\"\"\n",
    "    if attention_mask is None:\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "    )\n",
    "    return outputs.logits\n",
    "\n",
    "# Create LayerIntegratedGradients explainer\n",
    "# We'll attribute to the embedding layer\n",
    "print(f\"ğŸ”§ Creating LayerIntegratedGradients explainer...\")\n",
    "\n",
    "lig = LayerIntegratedGradients(\n",
    "    model_forward,\n",
    "    model.distilbert.embeddings  # Attribution to embedding layer\n",
    ")\n",
    "\n",
    "print(f\"âœ… Explainer created!\")\n",
    "\n",
    "# Compute attributions for the predicted class\n",
    "print(f\"\\nâ³ Computing attributions (30-60 seconds)...\")\n",
    "\n",
    "attributions = lig.attribute(\n",
    "    inputs=input_ids.unsqueeze(0),  # Add batch dimension\n",
    "    additional_forward_args=(inputs['attention_mask'],),\n",
    "    target=pred_class,  # Target the predicted class\n",
    "    n_steps=50,  # Number of integration steps (more = slower but more accurate)\n",
    "    internal_batch_size=1\n",
    ")\n",
    "\n",
    "print(f\"âœ… Attributions computed!\")\n",
    "print(f\"   Attribution shape: {attributions.shape}\")\n",
    "\n",
    "# Sum attributions across embedding dimensions to get per-token importance\n",
    "# attributions shape: (batch_size, sequence_length, embedding_dim)\n",
    "# We want: (sequence_length,)\n",
    "token_attributions = attributions.sum(dim=-1).squeeze().cpu().detach().numpy()\n",
    "\n",
    "print(f\"   Token-level attributions shape: {token_attributions.shape}\")\n",
    "print(f\"   Min attribution: {token_attributions.min():.4f}\")\n",
    "print(f\"   Max attribution: {token_attributions.max():.4f}\")\n",
    "print(f\"   Mean attribution: {token_attributions.mean():.4f}\")\n",
    "\n",
    "# Get absolute values for ranking\n",
    "abs_attributions = np.abs(token_attributions)\n",
    "\n",
    "print(f\"\\nâœ… Token attributions computed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dec5944",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: Visualize Top Contributing Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6daa4d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TOP 30 MOST IMPORTANT TOKENS (for predicting 'AI_Vanilla')\n",
      "================================================================================\n",
      "Rank   Token                     Attribution     Effect\n",
      "================================================================================\n",
      "1      ğŸ”´ .                       +0.009115     â†’ AI_Vanilla ğŸ”´\n",
      "2      ğŸ”´ condition               +0.003862     â†’ AI_Vanilla ğŸ”´\n",
      "3      ğŸ”´ hierarchy               +0.003249     â†’ AI_Vanilla ğŸ”´\n",
      "4      ğŸ”´ ;                       +0.002981     â†’ AI_Vanilla ğŸ”´\n",
      "5      ğŸ”´ of                      +0.002908     â†’ AI_Vanilla ğŸ”´\n",
      "6      ğŸ”´ structure               +0.002825     â†’ AI_Vanilla ğŸ”´\n",
      "7      ğŸŸ¢ daily                   -0.002610     â†’ Other classes ğŸŸ¢\n",
      "8      ğŸŸ¢ .                       -0.002525     â†’ Other classes ğŸŸ¢\n",
      "9      ğŸŸ¢ place                   -0.002470     â†’ Other classes ğŸŸ¢\n",
      "10     ğŸ”´ a                       +0.002453     â†’ AI_Vanilla ğŸ”´\n",
      "11     ğŸ”´ ##ze                    +0.002409     â†’ AI_Vanilla ğŸ”´\n",
      "12     ğŸ”´ ##ity                   +0.002398     â†’ AI_Vanilla ğŸ”´\n",
      "13     ğŸŸ¢ ,                       -0.002345     â†’ Other classes ğŸŸ¢\n",
      "14     ğŸŸ¢ ultimately              -0.002162     â†’ Other classes ğŸŸ¢\n",
      "15     ğŸ”´ suggests                +0.002160     â†’ AI_Vanilla ğŸ”´\n",
      "16     ğŸŸ¢ ,                       -0.002108     â†’ Other classes ğŸŸ¢\n",
      "17     ğŸŸ¢ tier                    -0.001998     â†’ Other classes ğŸŸ¢\n",
      "18     ğŸŸ¢ stop                    -0.001994     â†’ Other classes ğŸŸ¢\n",
      "19     ğŸ”´ into                    +0.001971     â†’ AI_Vanilla ğŸ”´\n",
      "20     ğŸŸ¢ cr                      -0.001854     â†’ Other classes ğŸŸ¢\n",
      "21     ğŸŸ¢ merit                   -0.001809     â†’ Other classes ğŸŸ¢\n",
      "22     ğŸŸ¢ attitudes               -0.001763     â†’ Other classes ğŸŸ¢\n",
      "23     ğŸŸ¢ reinforce               -0.001760     â†’ Other classes ğŸŸ¢\n",
      "24     ğŸŸ¢ .                       -0.001740     â†’ Other classes ğŸŸ¢\n",
      "25     ğŸ”´ economic                +0.001729     â†’ AI_Vanilla ğŸ”´\n",
      "26     ğŸ”´ that                    +0.001712     â†’ AI_Vanilla ğŸ”´\n",
      "27     ğŸŸ¢ superiors               -0.001693     â†’ Other classes ğŸŸ¢\n",
      "28     ğŸŸ¢ .                       -0.001684     â†’ Other classes ğŸŸ¢\n",
      "29     ğŸŸ¢ ing                     -0.001641     â†’ Other classes ğŸŸ¢\n",
      "30     ğŸ”´ have                    +0.001638     â†’ AI_Vanilla ğŸ”´\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ’¡ Interpretation:\n",
      "   ğŸ”´ Positive attribution = Token pushes TOWARD 'AI_Vanilla'\n",
      "   ğŸŸ¢ Negative attribution = Token pushes AWAY from 'AI_Vanilla'\n",
      "   Larger magnitude = Stronger influence\n"
     ]
    }
   ],
   "source": [
    "# Create dataframe with token importance\n",
    "token_importance_df = pd.DataFrame({\n",
    "    'Token': tokens,\n",
    "    'Attribution': token_attributions,\n",
    "    'Abs_Attribution': abs_attributions\n",
    "})\n",
    "\n",
    "# Remove special tokens for cleaner analysis\n",
    "special_tokens = ['[CLS]', '[SEP]', '[PAD]']\n",
    "main_tokens_df = token_importance_df[~token_importance_df['Token'].isin(special_tokens)].copy()\n",
    "\n",
    "# Sort by absolute importance\n",
    "main_tokens_df = main_tokens_df.sort_values('Abs_Attribution', ascending=False)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"TOP 30 MOST IMPORTANT TOKENS (for predicting '{label_names[pred_class]}')\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Rank':<6} {'Token':<25} {'Attribution':<15} {'Effect'}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for rank, (idx, row) in enumerate(main_tokens_df.head(30).iterrows(), 1):\n",
    "    token = row['Token']\n",
    "    attr = row['Attribution']\n",
    "    \n",
    "    # Determine effect\n",
    "    if attr > 0:\n",
    "        effect = f\"â†’ {label_names[pred_class]} ğŸ”´\"\n",
    "        marker = \"ğŸ”´\"\n",
    "    else:\n",
    "        effect = f\"â†’ Other classes ğŸŸ¢\"\n",
    "        marker = \"ğŸŸ¢\"\n",
    "    \n",
    "    print(f\"{rank:<6} {marker} {token:<23} {attr:>+.6f}     {effect}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"\\nğŸ’¡ Interpretation:\")\n",
    "print(f\"   ğŸ”´ Positive attribution = Token pushes TOWARD '{label_names[pred_class]}'\")\n",
    "print(f\"   ğŸŸ¢ Negative attribution = Token pushes AWAY from '{label_names[pred_class]}'\")\n",
    "print(f\"   Larger magnitude = Stronger influence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010240b8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 9: Create HTML Color-Coded Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5033187e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š COLOR-CODED SALIENCY MAP:\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ”´ Red = Pushes toward 'AI_Vanilla' | ğŸŸ¢ Green = Pushes away | âšª Neutral\n",
      "\n",
      "Hover over tokens to see exact attribution values!\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"line-height: 2.2; font-size: 16px; font-family: Arial, sans-serif; padding: 20px;\"> <span style=\"background-color: rgba(0, 255, 0, 0.010363267734646797); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0022\">ultimately</span> <span style=\"background-color: rgba(0, 255, 0, 0.010106787085533142); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0021\">,</span> <span style=\"background-color: rgba(255, 0, 0, 0.006434438936412335); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0013\">social</span> <span style=\"background-color: rgba(255, 0, 0, 0.01557561121881008); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0032\">hierarchy</span> <span style=\"background-color: rgba(0, 255, 0, 0.0011202076217159628); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0002\">is</span> <span style=\"background-color: rgba(255, 0, 0, 0.004520291928201914); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0009\">maintained</span> <span style=\"background-color: rgba(0, 255, 0, 0.0006761557655408978); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0001\">not</span> <span style=\"background-color: rgba(0, 255, 0, 0.0022837176918983457); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0005\">just</span> <span style=\"background-color: rgba(255, 0, 0, 0.002013913169503212); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0004\">by</span> <span style=\"background-color: rgba(255, 0, 0, 0.008291507698595523); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0017\">economic</span> <span style=\"background-color: rgba(255, 0, 0, 0.005075190402567387); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0011\">di</span><span style=\"background-color: rgba(255, 0, 0, 0.0014582399278879165); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0003\">spar</span><span style=\"background-color: rgba(255, 0, 0, 0.011494488269090651); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0024\">ity</span> <span style=\"background-color: rgba(0, 255, 0, 0.000250063044950366); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0001\">,</span> <span style=\"background-color: rgba(0, 255, 0, 0.0014927346725016831); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0003\">but</span> <span style=\"background-color: rgba(255, 0, 0, 0.0028607379645109177); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0006\">by</span> <span style=\"background-color: rgba(255, 0, 0, 0.007015633955597877); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0015\">the</span> <span style=\"background-color: rgba(255, 0, 0, 0.0026279306039214134); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0005\">com</span><span style=\"background-color: rgba(255, 0, 0, 0.00026953251217491924); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0001\">plicity</span> <span style=\"background-color: rgba(255, 0, 0, 0.006519239209592342); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0014\">of</span> <span style=\"background-color: rgba(0, 255, 0, 0.00628488902002573); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0013\">all</span> <span style=\"background-color: rgba(0, 255, 0, 0.003874455392360687); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0008\">participants</span> <span style=\"background-color: rgba(255, 0, 0, 0.0013623185455799102); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0003\">in</span> <span style=\"background-color: rgba(0, 255, 0, 0.0004253610852174461); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0001\">the</span> <span style=\"background-color: rgba(0, 255, 0, 0.001935542793944478); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0004\">system</span> <span style=\"background-color: rgba(0, 255, 0, 0.008343776129186154); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0017\">.</span> <span style=\"background-color: rgba(255, 0, 0, 0.003984467312693596); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0008\">the</span> <span style=\"background-color: rgba(255, 0, 0, 0.004581362195312977); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0010\">def</span><span style=\"background-color: rgba(0, 255, 0, 0.006112693436443805); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0013\">erence</span> <span style=\"background-color: rgba(0, 255, 0, 0.003660929761826992); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0008\">shown</span> <span style=\"background-color: rgba(0, 255, 0, 0.002541049104183912); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0005\">to</span> <span style=\"background-color: rgba(0, 255, 0, 0.008116236515343189); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0017\">superiors</span> <span style=\"background-color: rgba(0, 255, 0, 0.0036503895185887813); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0008\">and</span> <span style=\"background-color: rgba(255, 0, 0, 0.0009423259645700454); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0002\">the</span> <span style=\"background-color: rgba(255, 0, 0, 0.0032075623981654643); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0007\">conde</span><span style=\"background-color: rgba(255, 0, 0, 0.0006276822416111826); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0001\">sc</span><span style=\"background-color: rgba(255, 0, 0, 0.0004559708177112043); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0001\">ens</span><span style=\"background-color: rgba(255, 0, 0, 0.002076136227697134); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0004\">ion</span> <span style=\"background-color: rgba(0, 255, 0, 0.0022091606631875036); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0005\">shown</span> <span style=\"background-color: rgba(0, 255, 0, 0.0007716357009485364); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0002\">to</span> <span style=\"background-color: rgba(255, 0, 0, 0.0025712812319397924); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0005\">inferior</span><span style=\"background-color: rgba(255, 0, 0, 0.004884280264377594); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0010\">s</span> <span style=\"background-color: rgba(255, 0, 0, 0.0037122887559235093); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0008\">are</span> <span style=\"background-color: rgba(0, 255, 0, 0.0006640584440901875); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0001\">perform</span><span style=\"background-color: rgba(255, 0, 0, 0.0008900555782020091); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0002\">ative</span> <span style=\"background-color: rgba(255, 0, 0, 0.0012366925366222858); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0003\">acts</span> <span style=\"background-color: rgba(255, 0, 0, 0.006230242922902107); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0013\">that</span> <span style=\"background-color: rgba(0, 255, 0, 0.008435814455151557); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0018\">reinforce</span> <span style=\"background-color: rgba(255, 0, 0, 0.002450942527502775); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0005\">the</span> <span style=\"background-color: rgba(255, 0, 0, 0.013545147702097892); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0028\">structure</span> <span style=\"background-color: rgba(0, 255, 0, 0.012511581927537917); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0026\">daily</span> <span style=\"background-color: rgba(0, 255, 0, 0.0035242284648120403); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0007\">.</span> <span style=\"background-color: rgba(0, 255, 0, 0.002931743301451206); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0006\">disrupt</span><span style=\"background-color: rgba(0, 255, 0, 0.004374189209192991); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0009\">ing</span> <span style=\"background-color: rgba(255, 0, 0, 9.279653895646333e-05); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0000\">this</span> <span style=\"background-color: rgba(255, 0, 0, 0.0015559319872409105); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0003\">dynamic</span> <span style=\"background-color: rgba(0, 255, 0, 0.005689837597310543); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0012\">requires</span> <span style=\"background-color: rgba(0, 255, 0, 0.0016517346259206534); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0003\">more</span> <span style=\"background-color: rgba(255, 0, 0, 0.0037787094712257384); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0008\">than</span> <span style=\"background-color: rgba(255, 0, 0, 0.00025203539407812057); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0001\">a</span> <span style=\"background-color: rgba(255, 0, 0, 0.00019137180643156171); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0000\">redistribution</span> <span style=\"background-color: rgba(0, 255, 0, 0.0001343290787190199); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0000\">of</span> <span style=\"background-color: rgba(255, 0, 0, 0.0031755739822983743); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0007\">resources</span> <span style=\"background-color: rgba(255, 0, 0, 0.014291517063975334); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0030\">;</span> <span style=\"background-color: rgba(255, 0, 0, 0.0024797407910227775); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0005\">it</span> <span style=\"background-color: rgba(0, 255, 0, 0.002001953171566129); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0004\">requires</span> <span style=\"background-color: rgba(0, 255, 0, 0.002808683831244707); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0006\">a</span> <span style=\"background-color: rgba(255, 0, 0, 0.0016797549091279505); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0004\">di</span><span style=\"background-color: rgba(255, 0, 0, 0.0020861235447227956); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0004\">sman</span><span style=\"background-color: rgba(0, 255, 0, 0.003188179526478052); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0007\">tling</span> <span style=\"background-color: rgba(0, 255, 0, 0.00046117639867588873); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0001\">of</span> <span style=\"background-color: rgba(0, 255, 0, 0.0027204503305256365); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0006\">the</span> <span style=\"background-color: rgba(255, 0, 0, 0.000248024077154696); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0001\">internal</span><span style=\"background-color: rgba(255, 0, 0, 0.004189335275441408); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0009\">ized</span> <span style=\"background-color: rgba(0, 255, 0, 0.002191353728994727); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0005\">def</span><span style=\"background-color: rgba(255, 0, 0, 0.0008507943246513605); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0002\">ere</span><span style=\"background-color: rgba(0, 255, 0, 0.0024971455335617066); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0005\">ntial</span> <span style=\"background-color: rgba(0, 255, 0, 0.008453319780528545); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0018\">attitudes</span> <span style=\"background-color: rgba(255, 0, 0, 0.004206520970910788); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0009\">that</span> <span style=\"background-color: rgba(0, 255, 0, 0.0043324694037437435); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0009\">keep</span> <span style=\"background-color: rgba(255, 0, 0, 0.0032243477180600166); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0007\">the</span> <span style=\"background-color: rgba(255, 0, 0, 0.007170364446938037); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0015\">hierarchy</span> <span style=\"background-color: rgba(255, 0, 0, 0.003179801534861326); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0007\">in</span> <span style=\"background-color: rgba(0, 255, 0, 0.01184164322912693); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0025\">place</span> <span style=\"background-color: rgba(0, 255, 0, 0.00807415097951889); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0017\">.</span> <span style=\"background-color: rgba(0, 255, 0, 0.0030272958800196645); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0006\">when</span> <span style=\"background-color: rgba(255, 0, 0, 0.0020980138331651685); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0004\">individuals</span> <span style=\"background-color: rgba(0, 255, 0, 0.009560200944542884); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0020\">stop</span> <span style=\"background-color: rgba(0, 255, 0, 0.000561283144634217); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0001\">accepting</span> <span style=\"background-color: rgba(0, 255, 0, 0.0017197018023580313); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0004\">their</span> <span style=\"background-color: rgba(0, 255, 0, 0.002592712640762329); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0005\">assigned</span> <span style=\"background-color: rgba(255, 0, 0, 0.0012321952264755965); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0003\">value</span> <span style=\"background-color: rgba(255, 0, 0, 0.005554032325744629); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0012\">based</span> <span style=\"background-color: rgba(0, 255, 0, 0.005460812151432037); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0011\">on</span> <span style=\"background-color: rgba(0, 255, 0, 0.006057352758944035); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0013\">class</span> <span style=\"background-color: rgba(255, 0, 0, 0.0022614888846874234); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0005\">and</span> <span style=\"background-color: rgba(0, 255, 0, 0.002328076958656311); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0005\">start</span> <span style=\"background-color: rgba(0, 255, 0, 0.007453137449920177); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0016\">evaluating</span> <span style=\"background-color: rgba(0, 255, 0, 0.004718803055584431); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0010\">worth</span> <span style=\"background-color: rgba(255, 0, 0, 0.00013165904965717345); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0000\">based</span> <span style=\"background-color: rgba(255, 0, 0, 0.0003891495522111654); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0001\">on</span> <span style=\"background-color: rgba(0, 255, 0, 0.008674296364188193); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0018\">merit</span> <span style=\"background-color: rgba(0, 255, 0, 0.005270379781723023); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0011\">and</span> <span style=\"background-color: rgba(255, 0, 0, 0.0021744935307651757); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0005\">humanity</span> <span style=\"background-color: rgba(0, 255, 0, 0.011240549013018608); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0023\">,</span> <span style=\"background-color: rgba(0, 255, 0, 0.0010225937236100434); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0002\">the</span> <span style=\"background-color: rgba(255, 0, 0, 0.0022219541016966103); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0005\">foundation</span> <span style=\"background-color: rgba(255, 0, 0, 0.005279262363910675); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0011\">of</span> <span style=\"background-color: rgba(0, 255, 0, 0.0008125655818730593); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0002\">the</span> <span style=\"background-color: rgba(255, 0, 0, 0.007521034777164459); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0016\">hierarchy</span> <span style=\"background-color: rgba(0, 255, 0, 0.0034364154562354088); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0007\">begins</span> <span style=\"background-color: rgba(255, 0, 0, 0.003208636399358511); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0007\">to</span> <span style=\"background-color: rgba(0, 255, 0, 0.008886397071182727); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0019\">cr</span><span style=\"background-color: rgba(0, 255, 0, 0.0035776136443018912); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0007\">umble</span> <span style=\"background-color: rgba(0, 255, 0, 0.01210785135626793); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0025\">.</span> <span style=\"background-color: rgba(0, 255, 0, 0.0061298908665776254); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0013\">however</span> <span style=\"background-color: rgba(0, 255, 0, 0.005180991254746914); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0011\">,</span> <span style=\"background-color: rgba(0, 255, 0, 0.0010740690166130662); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0002\">history</span> <span style=\"background-color: rgba(255, 0, 0, 0.010357194766402244); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0022\">suggests</span> <span style=\"background-color: rgba(255, 0, 0, 0.007500016316771507); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0016\">that</span> <span style=\"background-color: rgba(255, 0, 0, 0.004485635831952095); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0009\">human</span> <span style=\"background-color: rgba(255, 0, 0, 0.007319409213960171); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0015\">societies</span> <span style=\"background-color: rgba(255, 0, 0, 0.007852778211236); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0016\">have</span> <span style=\"background-color: rgba(0, 255, 0, 0.005117716826498508); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0011\">a</span> <span style=\"background-color: rgba(255, 0, 0, 0.0011738135945051908); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0002\">persistent</span> <span style=\"background-color: rgba(255, 0, 0, 0.0019794448278844354); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0004\">tendency</span> <span style=\"background-color: rgba(255, 0, 0, 0.0074286974966526025); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0015\">to</span> <span style=\"background-color: rgba(255, 0, 0, 0.007518334127962589); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0016\">re</span><span style=\"background-color: rgba(255, 0, 0, 0.0008454781724140048); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0002\">org</span><span style=\"background-color: rgba(255, 0, 0, 0.00444427840411663); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0009\">ani</span><span style=\"background-color: rgba(255, 0, 0, 0.01154925897717476); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0024\">ze</span> <span style=\"background-color: rgba(255, 0, 0, 0.002916461694985628); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0006\">themselves</span> <span style=\"background-color: rgba(255, 0, 0, 0.009448402002453804); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0020\">into</span> <span style=\"background-color: rgba(0, 255, 0, 0.009580418094992637); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0020\">tier</span><span style=\"background-color: rgba(255, 0, 0, 0.002351212967187166); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0005\">ed</span> <span style=\"background-color: rgba(255, 0, 0, 0.005173530802130699); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0011\">systems</span> <span style=\"background-color: rgba(255, 0, 0, 0.00019956024480052292); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0000\">,</span> <span style=\"background-color: rgba(255, 0, 0, 0.0016863563563674687); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0004\">suggesting</span> <span style=\"background-color: rgba(255, 0, 0, 0.008207585290074348); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0017\">that</span> <span style=\"background-color: rgba(255, 0, 0, 0.005050866678357124); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0011\">the</span> <span style=\"background-color: rgba(0, 255, 0, 0.0025720990262925623); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0005\">drive</span> <span style=\"background-color: rgba(0, 255, 0, 0.005262277275323868); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0011\">for</span> <span style=\"background-color: rgba(255, 0, 0, 0.0020798456389456987); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0004\">status</span> <span style=\"background-color: rgba(0, 255, 0, 0.0026165395975112915); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0005\">and</span> <span style=\"background-color: rgba(0, 255, 0, 0.001172653678804636); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0002\">differentiation</span> <span style=\"background-color: rgba(255, 0, 0, 0.0032280455343425274); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0007\">is</span> <span style=\"background-color: rgba(255, 0, 0, 0.011759493872523308); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0025\">a</span> <span style=\"background-color: rgba(255, 0, 0, 2.056030643871054e-05); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0000\">deeply</span> <span style=\"background-color: rgba(0, 255, 0, 0.007866688817739486); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0016\">ing</span><span style=\"background-color: rgba(0, 255, 0, 0.004278115835040807); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0009\">rained</span> <span style=\"background-color: rgba(0, 255, 0, 0.0017304296605288982); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"Anti-AI_Vanilla signal: -0.0004\">aspect</span> <span style=\"background-color: rgba(255, 0, 0, 0.013941682130098342); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0029\">of</span> <span style=\"background-color: rgba(255, 0, 0, 0.005045610666275025); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0011\">the</span> <span style=\"background-color: rgba(255, 0, 0, 0.006218703836202621); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0013\">human</span> <span style=\"background-color: rgba(255, 0, 0, 0.018516239896416663); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0039\">condition</span> <span style=\"background-color: rgba(255, 0, 0, 0.04370049387216568); padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"AI_Vanilla signal: +0.0091\">.</span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_colored_tokens_html(tokens, attributions, max_intensity=0.6):\n",
    "    \"\"\"\n",
    "    Create HTML with color-coded tokens based on attribution values.\n",
    "    Red = Positive attribution (toward predicted class)\n",
    "    Green = Negative attribution (away from predicted class)\n",
    "    \"\"\"\n",
    "    # Normalize attributions for color intensity\n",
    "    max_abs_attr = np.max(np.abs(attributions))\n",
    "    if max_abs_attr == 0:\n",
    "        max_abs_attr = 1\n",
    "    \n",
    "    html = '<div style=\"line-height: 2.2; font-size: 16px; font-family: Arial, sans-serif; padding: 20px;\">'\n",
    "    \n",
    "    for token, attr in zip(tokens, attributions):\n",
    "        # Skip special tokens\n",
    "        if token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "            continue\n",
    "        \n",
    "        # Calculate color intensity\n",
    "        intensity = min(abs(attr) / max_abs_attr * max_intensity, max_intensity)\n",
    "        \n",
    "        # Handle subword tokens (##)\n",
    "        if token.startswith('##'):\n",
    "            token_display = token[2:]  # Remove ##\n",
    "            space_before = ''\n",
    "        else:\n",
    "            token_display = token\n",
    "            space_before = ' '\n",
    "        \n",
    "        if attr > 0:  # Pushes toward predicted class\n",
    "            bg_color = f'rgba(255, 0, 0, {intensity})'\n",
    "            title = f'{label_names[pred_class]} signal: +{attr:.4f}'\n",
    "        elif attr < 0:  # Pushes away from predicted class\n",
    "            bg_color = f'rgba(0, 255, 0, {intensity})'\n",
    "            title = f'Anti-{label_names[pred_class]} signal: {attr:.4f}'\n",
    "        else:  # Neutral\n",
    "            bg_color = 'transparent'\n",
    "            title = 'Neutral'\n",
    "        \n",
    "        html += f'{space_before}<span style=\"background-color: {bg_color}; padding: 2px 4px; margin: 1px; border-radius: 3px; display: inline-block;\" title=\"{title}\">{token_display}</span>'\n",
    "    \n",
    "    html += '</div>'\n",
    "    return html\n",
    "\n",
    "\n",
    "# Create visualization\n",
    "html_output = create_colored_tokens_html(tokens, token_attributions)\n",
    "\n",
    "print(\"\\nğŸ“Š COLOR-CODED SALIENCY MAP:\\n\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nğŸ”´ Red = Pushes toward '{label_names[pred_class]}' | ğŸŸ¢ Green = Pushes away | âšª Neutral\\n\")\n",
    "print(\"Hover over tokens to see exact attribution values!\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display HTML\n",
    "display(HTML(html_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d60d115",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 10: Save HTML Visualization to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9ad3a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Tier C saliency map saved to: ../results/visualizations/tier_c_saliency_map.html\n",
      "   Open this file in a browser to see interactive visualization!\n"
     ]
    }
   ],
   "source": [
    "# Save to file\n",
    "output_file = Path('../results/visualizations/tier_c_saliency_map.html')\n",
    "output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>TASK 3 - Tier C Saliency Map (97.20% Champion)</title>\n",
    "        <style>\n",
    "            body {{\n",
    "                font-family: Arial, sans-serif;\n",
    "                max-width: 1200px;\n",
    "                margin: 50px auto;\n",
    "                padding: 20px;\n",
    "            }}\n",
    "            h1 {{\n",
    "                color: #333;\n",
    "            }}\n",
    "            .legend {{\n",
    "                margin: 20px 0;\n",
    "                padding: 15px;\n",
    "                background-color: #f5f5f5;\n",
    "                border-radius: 5px;\n",
    "            }}\n",
    "            .stats {{\n",
    "                margin: 20px 0;\n",
    "                padding: 15px;\n",
    "                background-color: #e8f4f8;\n",
    "                border-radius: 5px;\n",
    "            }}\n",
    "            .model-info {{\n",
    "                margin: 20px 0;\n",
    "                padding: 15px;\n",
    "                background-color: #fff3cd;\n",
    "                border-radius: 5px;\n",
    "                border-left: 4px solid #ffc107;\n",
    "            }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>ğŸ” TASK 3: Tier C Saliency Mapping - DistilBERT (97.20% Champion ğŸ†)</h1>\n",
    "        \n",
    "        <div class=\"model-info\">\n",
    "            <h3>ğŸ† Model: Tier C (DistilBERT + LoRA)</h3>\n",
    "            <p><strong>Test Accuracy:</strong> 97.20% (NEW RECORD!)</p>\n",
    "            <p><strong>Human Detection:</strong> 100.00% (PERFECT!)</p>\n",
    "            <p><strong>AI_Styled Detection:</strong> 93.94%</p>\n",
    "            <p><strong>Method:</strong> LayerIntegratedGradients (Captum)</p>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"stats\">\n",
    "            <h3>ğŸ“Š Prediction Statistics for This Sample</h3>\n",
    "            <p><strong>True Label:</strong> {sample_label}</p>\n",
    "            <p><strong>Predicted:</strong> {label_names[pred_class]}</p>\n",
    "            <p><strong>Confidence:</strong> {confidence*100:.2f}%</p>\n",
    "            <p><strong>Class Probabilities:</strong></p>\n",
    "            <ul>\n",
    "                {''.join([f'<li>{name}: {probs[0, i].item()*100:.2f}%</li>' for i, name in enumerate(label_names)])}\n",
    "            </ul>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"legend\">\n",
    "            <h3>Legend</h3>\n",
    "            <p>ğŸ”´ <span style=\"background-color: rgba(255, 0, 0, 0.4); padding: 2px 8px; border-radius: 3px;\">Red</span> = Token pushes toward '{label_names[pred_class]}' classification</p>\n",
    "            <p>ğŸŸ¢ <span style=\"background-color: rgba(0, 255, 0, 0.4); padding: 2px 8px; border-radius: 3px;\">Green</span> = Token pushes away from '{label_names[pred_class]}'</p>\n",
    "            <p>âšª No color = Neutral token (minimal contribution)</p>\n",
    "            <p><em>Hover over tokens to see exact attribution values!</em></p>\n",
    "            <p><strong>Method:</strong> LayerIntegratedGradients with 50 steps</p>\n",
    "        </div>\n",
    "        \n",
    "        <h3>Analyzed Paragraph:</h3>\n",
    "        {html_output}\n",
    "        \n",
    "        <hr style=\"margin: 40px 0;\">\n",
    "        <p style=\"color: #666; font-size: 14px;\">\n",
    "            Generated by TASK 3 - Tier C Explainability Analysis<br>\n",
    "            Model: DistilBERT + LoRA (97.20% accuracy on Twain + Austen dataset)<br>\n",
    "            Attribution Method: Captum LayerIntegratedGradients\n",
    "        </p>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\")\n",
    "\n",
    "print(f\"\\nâœ… Tier C saliency map saved to: {output_file}\")\n",
    "print(f\"   Open this file in a browser to see interactive visualization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4235d382",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 11: Analyze AI-isms - Do They Exist?\n",
    "\n",
    "Check if common AI clichÃ©s appear in top tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3be4ff81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "AI-ISM ANALYSIS: Do Specific Words Signal AI?\n",
      "================================================================================\n",
      "\n",
      "âš ï¸ NO AI CLICHÃ‰S DETECTED in top 50 tokens.\n",
      "\n",
      "ğŸ’¡ FINDING: Model relies on OTHER signals (not specific buzzwords).\n",
      "   Likely detecting: Sentence rhythm, contextual patterns, or era-specific semantics.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Define known AI clichÃ©s\n",
    "AI_CLICHES = [\n",
    "    'tapestry', 'delve', 'testament', 'navigate', 'landscape', \n",
    "    'robust', 'comprehensive', 'intricate', 'nuanced', 'leverage',\n",
    "    'framework', 'paradigm', 'multifaceted', 'holistic', 'resonate',\n",
    "    'transformative', 'dynamic', 'innovative', 'strategic', 'synergy',\n",
    "    'profound', 'intrinsically', 'endeavor', 'realm', 'facet'\n",
    "]\n",
    "\n",
    "# Find AI clichÃ©s in top tokens\n",
    "top_50_tokens = main_tokens_df.head(50)\n",
    "ai_cliches_found = []\n",
    "\n",
    "for idx, row in top_50_tokens.iterrows():\n",
    "    token = row['Token'].lower().replace('##', '')  # Remove subword marker\n",
    "    if token in AI_CLICHES:\n",
    "        ai_cliches_found.append((token, row['Attribution'], row['Abs_Attribution']))\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"AI-ISM ANALYSIS: Do Specific Words Signal AI?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if ai_cliches_found:\n",
    "    print(f\"\\nğŸ¤– AI CLICHÃ‰S DETECTED ({len(ai_cliches_found)} found in top 50 tokens):\\n\")\n",
    "    for word, attr, abs_attr in ai_cliches_found:\n",
    "        print(f\"   ğŸ”´ '{word}' - Attribution: {attr:+.6f} (Rank: {main_tokens_df[main_tokens_df['Token'].str.lower().str.replace('##', '') == word].index[0] + 1})\")\n",
    "    print(f\"\\nâœ… FINDING: Model DOES detect specific AI-ism words!\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ NO AI CLICHÃ‰S DETECTED in top 50 tokens.\")\n",
    "    print(f\"\\nğŸ’¡ FINDING: Model relies on OTHER signals (not specific buzzwords).\")\n",
    "    print(f\"   Likely detecting: Sentence rhythm, contextual patterns, or era-specific semantics.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153095d2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 12: ERROR ANALYSIS - Find Misclassified Human Texts\n",
    "\n",
    "Find 3 human paragraphs that Tier C wrongly classified as AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "891badf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Searching for ERRORS: Human texts misclassified as AI...\n",
      "\n",
      "âœ… Found 3 Humanâ†’AI misclassifications\n",
      "\n",
      "========================================================================================================================\n",
      "\n",
      "âŒ ERROR 1:\n",
      "   True Label: Human\n",
      "   Predicted: AI_Styled (33.61% confidence)\n",
      "   Class probabilities:\n",
      "      AI_Styled: 33.61%\n",
      "      AI_Vanilla: 32.83%\n",
      "      Human: 33.56%\n",
      "\n",
      "   Text:\n",
      "   The new boy took two broad coppers out of his pocket and held them out\n",
      "with derision. Tom struck them to the ground. In an instant both boys\n",
      "were rolling and tumbling in the dirt, gripped together like cats; and\n",
      "for the space of a minute they tugged and tore at each otherâ€™s hair and\n",
      "clothes, punched and scratched each otherâ€™s nose, and covered themselves\n",
      "with dust and glory. Presently the confusion took form, and through the\n",
      "fog of battle Tom appeared, seated astride the new boy, and pounding him\n",
      "with his fists. â€œHoller â€™nuff!â€ said he.\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "âŒ ERROR 2:\n",
      "   True Label: Human\n",
      "   Predicted: AI_Vanilla (33.76% confidence)\n",
      "   Class probabilities:\n",
      "      AI_Styled: 32.74%\n",
      "      AI_Vanilla: 33.76%\n",
      "      Human: 33.50%\n",
      "\n",
      "   Text:\n",
      "   But Tomâ€™s energy did not last. He began to think of the fun he had\n",
      "planned for this day, and his sorrows multiplied. Soon the free boys\n",
      "would come tripping along on all sorts of delicious expeditions, and\n",
      "they would make a world of fun of him for having to workâ€”the very\n",
      "thought of it burnt him like fire. He got out his worldly wealth and\n",
      "examined itâ€”bits of toys, marbles, and trash; enough to buy an exchange\n",
      "of _work_, maybe, but not half enough to buy so much as half an hour\n",
      "of pure freedom. So he returned his straitened means to his pocket, and\n",
      "gave up the idea of trying to buy the boys. At this dark and hopeless\n",
      "moment an inspiration burst upon him! Nothing less than a great,\n",
      "magnificent inspiration.\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "âŒ ERROR 3:\n",
      "   True Label: Human\n",
      "   Predicted: AI_Vanilla (34.30% confidence)\n",
      "   Class probabilities:\n",
      "      AI_Styled: 31.90%\n",
      "      AI_Vanilla: 34.30%\n",
      "      Human: 33.81%\n",
      "\n",
      "   Text:\n",
      "   Tom gave up the brush with reluctance in his face, but alacrity in his\n",
      "heart. And while the late steamer Big Missouri worked and sweated in the\n",
      "sun, the retired artist sat on a barrel in the shade close by,\n",
      "dangled his legs, munched his apple, and planned the slaughter of more\n",
      "innocents. There was no lack of material; boys happened along every\n",
      "little while; they came to jeer, but remained to whitewash. By the time\n",
      "Ben was fagged out, Tom had traded the next chance to Billy Fisher for\n",
      "a kite, in good repair; and when he played out, Johnny Miller bought in\n",
      "for a dead rat and a string to swing it withâ€”and so on, and so on, hour\n",
      "after hour. And when the middle of the afternoon came, from being a\n",
      "poor poverty-stricken boy in the morning, Tom was literally rolling in\n",
      "wealth. He had besides the things before mentioned, twelve marbles, part\n",
      "of a jews-harp, a piece of blue bottle-glass to look through, a spool\n",
      "cannon, a key that wouldnâ€™t unlock anything, a fragment of chalk, a\n",
      "glass stopper of a decanter, a tin soldier, a couple of tadpoles,\n",
      "six fire-crackers, a kitten with only one eye, a brass door-knob, a\n",
      "dog-collarâ€”but no dogâ€”the handle of a knife, four pieces of orange-peel,\n",
      "and a dilapidated old window sash.\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "========================================================================================================================\n",
      "âœ… Found 3 Humanâ†’AI misclassifications\n",
      "\n",
      "========================================================================================================================\n",
      "\n",
      "âŒ ERROR 1:\n",
      "   True Label: Human\n",
      "   Predicted: AI_Styled (33.61% confidence)\n",
      "   Class probabilities:\n",
      "      AI_Styled: 33.61%\n",
      "      AI_Vanilla: 32.83%\n",
      "      Human: 33.56%\n",
      "\n",
      "   Text:\n",
      "   The new boy took two broad coppers out of his pocket and held them out\n",
      "with derision. Tom struck them to the ground. In an instant both boys\n",
      "were rolling and tumbling in the dirt, gripped together like cats; and\n",
      "for the space of a minute they tugged and tore at each otherâ€™s hair and\n",
      "clothes, punched and scratched each otherâ€™s nose, and covered themselves\n",
      "with dust and glory. Presently the confusion took form, and through the\n",
      "fog of battle Tom appeared, seated astride the new boy, and pounding him\n",
      "with his fists. â€œHoller â€™nuff!â€ said he.\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "âŒ ERROR 2:\n",
      "   True Label: Human\n",
      "   Predicted: AI_Vanilla (33.76% confidence)\n",
      "   Class probabilities:\n",
      "      AI_Styled: 32.74%\n",
      "      AI_Vanilla: 33.76%\n",
      "      Human: 33.50%\n",
      "\n",
      "   Text:\n",
      "   But Tomâ€™s energy did not last. He began to think of the fun he had\n",
      "planned for this day, and his sorrows multiplied. Soon the free boys\n",
      "would come tripping along on all sorts of delicious expeditions, and\n",
      "they would make a world of fun of him for having to workâ€”the very\n",
      "thought of it burnt him like fire. He got out his worldly wealth and\n",
      "examined itâ€”bits of toys, marbles, and trash; enough to buy an exchange\n",
      "of _work_, maybe, but not half enough to buy so much as half an hour\n",
      "of pure freedom. So he returned his straitened means to his pocket, and\n",
      "gave up the idea of trying to buy the boys. At this dark and hopeless\n",
      "moment an inspiration burst upon him! Nothing less than a great,\n",
      "magnificent inspiration.\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "âŒ ERROR 3:\n",
      "   True Label: Human\n",
      "   Predicted: AI_Vanilla (34.30% confidence)\n",
      "   Class probabilities:\n",
      "      AI_Styled: 31.90%\n",
      "      AI_Vanilla: 34.30%\n",
      "      Human: 33.81%\n",
      "\n",
      "   Text:\n",
      "   Tom gave up the brush with reluctance in his face, but alacrity in his\n",
      "heart. And while the late steamer Big Missouri worked and sweated in the\n",
      "sun, the retired artist sat on a barrel in the shade close by,\n",
      "dangled his legs, munched his apple, and planned the slaughter of more\n",
      "innocents. There was no lack of material; boys happened along every\n",
      "little while; they came to jeer, but remained to whitewash. By the time\n",
      "Ben was fagged out, Tom had traded the next chance to Billy Fisher for\n",
      "a kite, in good repair; and when he played out, Johnny Miller bought in\n",
      "for a dead rat and a string to swing it withâ€”and so on, and so on, hour\n",
      "after hour. And when the middle of the afternoon came, from being a\n",
      "poor poverty-stricken boy in the morning, Tom was literally rolling in\n",
      "wealth. He had besides the things before mentioned, twelve marbles, part\n",
      "of a jews-harp, a piece of blue bottle-glass to look through, a spool\n",
      "cannon, a key that wouldnâ€™t unlock anything, a fragment of chalk, a\n",
      "glass stopper of a decanter, a tin soldier, a couple of tadpoles,\n",
      "six fire-crackers, a kitten with only one eye, a brass door-knob, a\n",
      "dog-collarâ€”but no dogâ€”the handle of a knife, four pieces of orange-peel,\n",
      "and a dilapidated old window sash.\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ” Searching for ERRORS: Human texts misclassified as AI...\\n\")\n",
    "\n",
    "# Test all human paragraphs\n",
    "human_errors = []\n",
    "\n",
    "for idx, text in enumerate(human_texts):\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = softmax(logits, dim=1)\n",
    "        pred_class = torch.argmax(probs, dim=1).item()\n",
    "        confidence = probs[0, pred_class].item()\n",
    "    \n",
    "    # Check if misclassified as AI\n",
    "    if label_names[pred_class] != 'Human':\n",
    "        human_errors.append({\n",
    "            'index': idx,\n",
    "            'text': text,\n",
    "            'predicted': label_names[pred_class],\n",
    "            'confidence': confidence,\n",
    "            'probs': probs[0].cpu().numpy()\n",
    "        })\n",
    "        \n",
    "        # Stop after finding 3 errors\n",
    "        if len(human_errors) >= 3:\n",
    "            break\n",
    "\n",
    "print(f\"âœ… Found {len(human_errors)} Humanâ†’AI misclassifications\\n\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "if len(human_errors) > 0:\n",
    "    for i, error in enumerate(human_errors, 1):\n",
    "        print(f\"\\nâŒ ERROR {i}:\")\n",
    "        print(f\"   True Label: Human\")\n",
    "        print(f\"   Predicted: {error['predicted']} ({error['confidence']*100:.2f}% confidence)\")\n",
    "        print(f\"   Class probabilities:\")\n",
    "        for j, name in enumerate(label_names):\n",
    "            print(f\"      {name}: {error['probs'][j]*100:.2f}%\")\n",
    "        print(f\"\\n   Text:\")\n",
    "        print(f\"   {error['text']}\")\n",
    "        print(\"\\n\" + \"-\"*120)\n",
    "else:\n",
    "    print(\"\\nâœ… NO ERRORS FOUND! Tier C achieves 100% human detection.\")\n",
    "    print(\"   This matches the test results: 100.00% human detection rate.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4835e7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 13: Analyze Why Model Failed (If Errors Found)\n",
    "\n",
    "If errors were found, analyze them using Captum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd459224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Analyzing first error in detail...\n",
      "\n",
      "â³ Computing attributions for error...\n",
      "\n",
      "âœ… Attributions computed!\n",
      "\n",
      "================================================================================\n",
      "WHY DID THE MODEL FAIL? Top tokens that misled it:\n",
      "================================================================================\n",
      "Rank   Token                     Attribution     Effect\n",
      "================================================================================\n",
      "1      ğŸŸ¢ boy                     -0.006952     â†’ Human (correct direction) ğŸŸ¢\n",
      "2      ğŸŸ¢ .                       -0.005806     â†’ Human (correct direction) ğŸŸ¢\n",
      "3      ğŸ”´ battle                  +0.005710     â†’ AI_Styled (WRONG!) ğŸ”´\n",
      "4      ğŸ”´ tom                     +0.005484     â†’ AI_Styled (WRONG!) ğŸ”´\n",
      "5      ğŸŸ¢ .                       -0.004795     â†’ Human (correct direction) ğŸŸ¢\n",
      "6      ğŸ”´ he                      +0.003883     â†’ AI_Styled (WRONG!) ğŸ”´\n",
      "7      ğŸ”´ ;                       +0.003828     â†’ AI_Styled (WRONG!) ğŸ”´\n",
      "8      ğŸŸ¢ ,                       -0.003159     â†’ Human (correct direction) ğŸŸ¢\n",
      "9      ğŸ”´ his                     +0.003069     â†’ AI_Styled (WRONG!) ğŸ”´\n",
      "10     ğŸ”´ astrid                  +0.002847     â†’ AI_Styled (WRONG!) ğŸ”´\n",
      "11     ğŸ”´ like                    +0.002542     â†’ AI_Styled (WRONG!) ğŸ”´\n",
      "12     ğŸŸ¢ boys                    -0.002479     â†’ Human (correct direction) ğŸŸ¢\n",
      "13     ğŸ”´ ho                      +0.002371     â†’ AI_Styled (WRONG!) ğŸ”´\n",
      "14     ğŸ”´ and                     +0.002330     â†’ AI_Styled (WRONG!) ğŸ”´\n",
      "15     ğŸ”´ said                    +0.002329     â†’ AI_Styled (WRONG!) ğŸ”´\n",
      "16     ğŸ”´ dust                    +0.002242     â†’ AI_Styled (WRONG!) ğŸ”´\n",
      "17     ğŸŸ¢ .                       -0.002160     â†’ Human (correct direction) ğŸŸ¢\n",
      "18     ğŸŸ¢ the                     -0.002115     â†’ Human (correct direction) ğŸŸ¢\n",
      "19     ğŸŸ¢ pocket                  -0.002114     â†’ Human (correct direction) ğŸŸ¢\n",
      "20     ğŸ”´ with                    +0.001992     â†’ AI_Styled (WRONG!) ğŸ”´\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ’¡ ANALYSIS:\n",
      "   - The model was misled by specific tokens (red markers above)\n",
      "   - These tokens pushed classification toward AI_Styled\n",
      "   - Possible reasons: AI-like vocabulary, unusual phrasing, or era-mismatch\n",
      "\n",
      "âœ… Attributions computed!\n",
      "\n",
      "================================================================================\n",
      "WHY DID THE MODEL FAIL? Top tokens that misled it:\n",
      "================================================================================\n",
      "Rank   Token                     Attribution     Effect\n",
      "================================================================================\n",
      "1      ğŸŸ¢ boy                     -0.006952     â†’ Human (correct direction) ğŸŸ¢\n",
      "2      ğŸŸ¢ .                       -0.005806     â†’ Human (correct direction) ğŸŸ¢\n",
      "3      ğŸ”´ battle                  +0.005710     â†’ AI_Styled (WRONG!) ğŸ”´\n",
      "4      ğŸ”´ tom                     +0.005484     â†’ AI_Styled (WRONG!) ğŸ”´\n",
      "5      ğŸŸ¢ .                       -0.004795     â†’ Human (correct direction) ğŸŸ¢\n",
      "6      ğŸ”´ he                      +0.003883     â†’ AI_Styled (WRONG!) ğŸ”´\n",
      "7      ğŸ”´ ;                       +0.003828     â†’ AI_Styled (WRONG!) ğŸ”´\n",
      "8      ğŸŸ¢ ,                       -0.003159     â†’ Human (correct direction) ğŸŸ¢\n",
      "9      ğŸ”´ his                     +0.003069     â†’ AI_Styled (WRONG!) ğŸ”´\n",
      "10     ğŸ”´ astrid                  +0.002847     â†’ AI_Styled (WRONG!) ğŸ”´\n",
      "11     ğŸ”´ like                    +0.002542     â†’ AI_Styled (WRONG!) ğŸ”´\n",
      "12     ğŸŸ¢ boys                    -0.002479     â†’ Human (correct direction) ğŸŸ¢\n",
      "13     ğŸ”´ ho                      +0.002371     â†’ AI_Styled (WRONG!) ğŸ”´\n",
      "14     ğŸ”´ and                     +0.002330     â†’ AI_Styled (WRONG!) ğŸ”´\n",
      "15     ğŸ”´ said                    +0.002329     â†’ AI_Styled (WRONG!) ğŸ”´\n",
      "16     ğŸ”´ dust                    +0.002242     â†’ AI_Styled (WRONG!) ğŸ”´\n",
      "17     ğŸŸ¢ .                       -0.002160     â†’ Human (correct direction) ğŸŸ¢\n",
      "18     ğŸŸ¢ the                     -0.002115     â†’ Human (correct direction) ğŸŸ¢\n",
      "19     ğŸŸ¢ pocket                  -0.002114     â†’ Human (correct direction) ğŸŸ¢\n",
      "20     ğŸ”´ with                    +0.001992     â†’ AI_Styled (WRONG!) ğŸ”´\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ’¡ ANALYSIS:\n",
      "   - The model was misled by specific tokens (red markers above)\n",
      "   - These tokens pushed classification toward AI_Styled\n",
      "   - Possible reasons: AI-like vocabulary, unusual phrasing, or era-mismatch\n"
     ]
    }
   ],
   "source": [
    "if len(human_errors) > 0:\n",
    "    print(\"ğŸ” Analyzing first error in detail...\\n\")\n",
    "    \n",
    "    error = human_errors[0]\n",
    "    error_text = error['text']\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        error_text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = softmax(logits, dim=1)\n",
    "        error_pred_class = torch.argmax(probs, dim=1).item()\n",
    "    \n",
    "    # Compute attributions\n",
    "    print(\"â³ Computing attributions for error...\")\n",
    "    \n",
    "    error_attributions = lig.attribute(\n",
    "        inputs=inputs['input_ids'],\n",
    "        additional_forward_args=(inputs['attention_mask'],),\n",
    "        target=error_pred_class,\n",
    "        n_steps=50,\n",
    "        internal_batch_size=1\n",
    "    )\n",
    "    \n",
    "    error_token_attributions = error_attributions.sum(dim=-1).squeeze().cpu().detach().numpy()\n",
    "    error_tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    # Create dataframe\n",
    "    error_df = pd.DataFrame({\n",
    "        'Token': error_tokens,\n",
    "        'Attribution': error_token_attributions,\n",
    "        'Abs_Attribution': np.abs(error_token_attributions)\n",
    "    })\n",
    "    error_df = error_df[~error_df['Token'].isin(['[CLS]', '[SEP]', '[PAD]'])]\n",
    "    error_df = error_df.sort_values('Abs_Attribution', ascending=False)\n",
    "    \n",
    "    print(\"\\nâœ… Attributions computed!\\n\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"WHY DID THE MODEL FAIL? Top tokens that misled it:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Rank':<6} {'Token':<25} {'Attribution':<15} {'Effect'}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for rank, (idx, row) in enumerate(error_df.head(20).iterrows(), 1):\n",
    "        token = row['Token']\n",
    "        attr = row['Attribution']\n",
    "        \n",
    "        if attr > 0:\n",
    "            effect = f\"â†’ {label_names[error_pred_class]} (WRONG!) ğŸ”´\"\n",
    "            marker = \"ğŸ”´\"\n",
    "        else:\n",
    "            effect = f\"â†’ Human (correct direction) ğŸŸ¢\"\n",
    "            marker = \"ğŸŸ¢\"\n",
    "        \n",
    "        print(f\"{rank:<6} {marker} {token:<23} {attr:>+.6f}     {effect}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"\\nğŸ’¡ ANALYSIS:\")\n",
    "    print(f\"   - The model was misled by specific tokens (red markers above)\")\n",
    "    print(f\"   - These tokens pushed classification toward {label_names[error_pred_class]}\")\n",
    "    print(f\"   - Possible reasons: AI-like vocabulary, unusual phrasing, or era-mismatch\")\n",
    "else:\n",
    "    print(\"\\nâœ… No errors to analyze! Model perfect on human detection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f892e0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Key Findings\n",
    "\n",
    "### What We've Learned âœ…\n",
    "\n",
    "1. **Saliency Mapping Complete**\n",
    "   - Token-level attribution using Captum LayerIntegratedGradients\n",
    "   - HTML visualization showing AI-signaling tokens\n",
    "   - Top 30 most important tokens identified\n",
    "\n",
    "2. **AI-ism Detection**\n",
    "   - Analyzed if model detects specific AI clichÃ©s (\"tapestry,\" \"delve,\" etc.)\n",
    "   - Results: [See Step 11 output]\n",
    "\n",
    "3. **Error Analysis**\n",
    "   - Found Humanâ†’AI misclassifications (if any)\n",
    "   - Analyzed WHY the model failed\n",
    "   - Identified misleading tokens\n",
    "\n",
    "### Next Steps ğŸ“‹\n",
    "\n",
    "- **Document findings in REPORT.md**\n",
    "- **Compare with Tier B results** (if needed)\n",
    "- **Move to Task 4:** Adversarial testing\n",
    "\n",
    "---\n",
    "\n",
    "**Status:** Task 3 (Tier C) âœ… COMPLETE\n",
    "\n",
    "**Files Generated:**\n",
    "- `../results/visualizations/tier_c_saliency_map.html` - Interactive saliency map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
