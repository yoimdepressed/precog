{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28c715dc",
   "metadata": {},
   "source": [
    "# TASK 3 - PART 1: Saliency Mapping (The Smoking Gun)\n",
    "\n",
    "## Objective\n",
    "\n",
    "Identify **which specific words** in AI-generated text signal \"AI\" to our best detector (Tier B - 95% accuracy).\n",
    "\n",
    "---\n",
    "\n",
    "## Why Tier B (Not Tier C)?\n",
    "\n",
    "**Decision Rationale:**\n",
    "\n",
    "| Model | Test Accuracy | Reason to Use/Not Use |\n",
    "|-------|---------------|----------------------|\n",
    "| **Tier C (DistilBERT)** | 59% âŒ | Overfitted, predicts \"Human\" for 85.9% of errors |\n",
    "| **Tier B (GloVe + NN)** | **95%** âœ… | **WINNER** - Reliable, captures semantic signals |\n",
    "| **Tier A (Random Forest)** | 91% | Good, but focuses on structure not semantics |\n",
    "\n",
    "**Key Point:** Task 3 asks for explainability of AI detection. Tier C (59%) explains overfitting, not AI detection. **Tier B (95%) explains what ACTUALLY works.**\n",
    "\n",
    "**Scientific Justification:**\n",
    "> \"We analyze Tier B (95% accuracy) instead of Tier C (59% accuracy) because our goal is understanding successful AI detection, not debugging a failed model. Tier B's semantic approach provides actionable insights into vocabulary patterns that distinguish Victorian human text from modern AI text.\"\n",
    "\n",
    "---\n",
    "\n",
    "## The Approach\n",
    "\n",
    "### What is Saliency Mapping?\n",
    "\n",
    "**Saliency = Importance Score per Word**\n",
    "\n",
    "For each word in a paragraph:\n",
    "- **Positive score (red):** Pushes toward \"AI\" classification\n",
    "- **Negative score (green):** Pushes toward \"Human\" classification\n",
    "- **Zero score (neutral):** No contribution to decision\n",
    "\n",
    "### Tool: SHAP (SHapley Additive exPlanations)\n",
    "\n",
    "**Why SHAP?**\n",
    "- âœ… Theoretically grounded (game theory, Shapley values)\n",
    "- âœ… Works with any model (black-box approach)\n",
    "- âœ… Word-level importance scores\n",
    "- âœ… Beautiful visualizations (force plots, waterfall plots)\n",
    "- âœ… Handles averaged embeddings well\n",
    "\n",
    "**Alternative: Captum**\n",
    "- Gradient-based (faster but requires gradients)\n",
    "- PyTorch-specific\n",
    "- Good for transformers, less ideal for averaged embeddings\n",
    "\n",
    "### The Process\n",
    "\n",
    "```\n",
    "1. Load Tier B model (saved weights)\n",
    "   â†“\n",
    "2. Select \"imposter\" AI paragraphs (5-10 examples)\n",
    "   â†“\n",
    "3. For each paragraph:\n",
    "   - Tokenize into words\n",
    "   - Get GloVe embedding per word\n",
    "   - Compute SHAP value per word\n",
    "   - Map back to original text\n",
    "   â†“\n",
    "4. Visualize:\n",
    "   - Color-coded HTML output\n",
    "   - Force plots showing contribution flow\n",
    "   - Top AI-signaling words\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Expected Results\n",
    "\n",
    "**Hypothesis 1: Victorian vs Modern Vocabulary**\n",
    "- Modern AI words (\"tapestry,\" \"delve,\" \"navigate\") â†’ High AI scores\n",
    "- Victorian words (\"countenance,\" \"parlour,\" \"ere\") â†’ High Human scores\n",
    "\n",
    "**Hypothesis 2: Temporal Semantic Gap**\n",
    "- GloVe (trained on 2000s web) detects era-specific context\n",
    "- 170-year Victorianâ†’Modern gap is THE signal\n",
    "\n",
    "**Hypothesis 3: AI-isms Are Real**\n",
    "- Corporate jargon: \"robust,\" \"comprehensive,\" \"landscape\"\n",
    "- AI clichÃ©s: \"testament,\" \"intricate,\" \"nuanced\"\n",
    "\n",
    "---\n",
    "\n",
    "Let's begin! ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcade80",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Install & Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec649b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SHAP (run once)\n",
    "!pip install -q shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd7a260f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-07 12:59:06.914728: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-07 12:59:06.949197: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770449346.971116   27976 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770449346.977921   27976 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1770449347.005524   27976 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770449347.005541   27976 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770449347.005543   27976 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770449347.005544   27976 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-02-07 12:59:07.010221: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n",
      "   SHAP version: 0.49.1\n",
      "   TensorFlow version: 2.19.1\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# SHAP for explainability\n",
    "import shap\n",
    "\n",
    "# PyTorch/Keras (for loading Tier B model)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Random seed\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"   SHAP version: {shap.__version__}\")\n",
    "print(f\"   TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d145809",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Load GloVe Embeddings\n",
    "\n",
    "Same preprocessing as Tier B training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ad62f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings (this may take ~30 seconds)...\n",
      "âœ… GloVe embeddings loaded!\n",
      "   Vocabulary size: 400,000 words\n",
      "   Embedding dimension: 300d\n",
      "âœ… GloVe embeddings loaded!\n",
      "   Vocabulary size: 400,000 words\n",
      "   Embedding dimension: 300d\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe embeddings\n",
    "GLOVE_PATH = Path('../glove/glove.6B.300d.txt')\n",
    "\n",
    "print(\"Loading GloVe embeddings (this may take ~30 seconds)...\")\n",
    "\n",
    "glove_embeddings = {}\n",
    "with open(GLOVE_PATH, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:], dtype='float32')\n",
    "        glove_embeddings[word] = vector\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "print(f\"âœ… GloVe embeddings loaded!\")\n",
    "print(f\"   Vocabulary size: {len(glove_embeddings):,} words\")\n",
    "print(f\"   Embedding dimension: {EMBEDDING_DIM}d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aaf531",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Define Text-to-Embedding Functions\n",
    "\n",
    "Match Tier B preprocessing exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0b39b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Preprocessing functions defined!\n",
      "   Test text: 'The quick brown fox jumps over the lazy dog.'\n",
      "   Averaged embedding shape: (300,)\n",
      "   Number of words: 9\n"
     ]
    }
   ],
   "source": [
    "def text_to_embedding(text, embeddings_dict):\n",
    "    \"\"\"\n",
    "    Convert text to averaged GloVe embedding (300d vector).\n",
    "    Same as Tier B training preprocessing.\n",
    "    \"\"\"\n",
    "    # Tokenize (simple split + lowercase)\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    # Get embeddings for each word\n",
    "    word_vectors = []\n",
    "    for word in words:\n",
    "        if word in embeddings_dict:\n",
    "            word_vectors.append(embeddings_dict[word])\n",
    "    \n",
    "    # Average all word vectors\n",
    "    if len(word_vectors) > 0:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        # If no words found, return zeros\n",
    "        return np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "\n",
    "def get_word_embeddings(text, embeddings_dict):\n",
    "    \"\"\"\n",
    "    Get individual word embeddings for SHAP analysis.\n",
    "    Returns: List of (word, embedding) tuples.\n",
    "    \"\"\"\n",
    "    words = text.lower().split()\n",
    "    word_embeds = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in embeddings_dict:\n",
    "            word_embeds.append((word, embeddings_dict[word]))\n",
    "        else:\n",
    "            # Unknown word: use zero vector\n",
    "            word_embeds.append((word, np.zeros(EMBEDDING_DIM)))\n",
    "    \n",
    "    return word_embeds\n",
    "\n",
    "\n",
    "# Test functions\n",
    "test_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "test_embed = text_to_embedding(test_text, glove_embeddings)\n",
    "test_words = get_word_embeddings(test_text, glove_embeddings)\n",
    "\n",
    "print(f\"âœ… Preprocessing functions defined!\")\n",
    "print(f\"   Test text: '{test_text}'\")\n",
    "print(f\"   Averaged embedding shape: {test_embed.shape}\")\n",
    "print(f\"   Number of words: {len(test_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0624c68",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Load Tier B Model\n",
    "\n",
    "Load the saved Tier B model (95% accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac70d2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-06 11:18:43.308130: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tier B model loaded from: ../TASK2/task2_tier_b_glove_model.h5\n",
      "\n",
      "ğŸ“Š Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"GloVe_Classifier\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"GloVe_Classifier\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ hidden1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">38,528</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ hidden2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ hidden1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚        \u001b[38;5;34m38,528\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout1 (\u001b[38;5;33mDropout\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ hidden2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚         \u001b[38;5;34m8,256\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout2 (\u001b[38;5;33mDropout\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ output (\u001b[38;5;33mDense\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              â”‚           \u001b[38;5;34m195\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">46,981</span> (183.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m46,981\u001b[0m (183.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">46,979</span> (183.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m46,979\u001b[0m (183.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‹ Class Labels (alphabetically ordered by LabelEncoder):\n",
      "   0: AI_Styled\n",
      "   1: AI_Vanilla\n",
      "   2: Human\n"
     ]
    }
   ],
   "source": [
    "# Load Tier B model\n",
    "MODEL_PATH = Path('../TASK2/task2_tier_b_glove_model.h5')\n",
    "\n",
    "if MODEL_PATH.exists():\n",
    "    model = keras.models.load_model(MODEL_PATH)\n",
    "    print(f\"âœ… Tier B model loaded from: {MODEL_PATH}\")\n",
    "    print(f\"\\nğŸ“Š Model Architecture:\")\n",
    "    model.summary()\n",
    "else:\n",
    "    print(f\"âŒ Model file not found: {MODEL_PATH}\")\n",
    "    print(f\"   Please ensure you've trained and saved Tier B model.\")\n",
    "    print(f\"   Expected location: {MODEL_PATH.absolute()}\")\n",
    "    print(f\"\\nğŸ’¡ To train the model, run all cells in:\")\n",
    "    print(f\"   ../TASK2/task2_tier_b_semanticist.ipynb\")\n",
    "\n",
    "# Define label names (same as Tier B)\n",
    "# NOTE: LabelEncoder sorts alphabetically, so:\n",
    "#   Class 0 = AI_Styled (from class3.txt)\n",
    "#   Class 1 = AI_Vanilla (from class2.txt)\n",
    "#   Class 2 = Human (from class1_human.jsonl)\n",
    "label_names = ['AI_Styled', 'AI_Vanilla', 'Human']\n",
    "\n",
    "print(f\"\\nğŸ“‹ Class Labels (alphabetically ordered by LabelEncoder):\")\n",
    "for i, name in enumerate(label_names):\n",
    "    print(f\"   {i}: {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac7f1a2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Load Test Data\n",
    "\n",
    "Get AI paragraphs that Tier B correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1cf27b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… AI Vanilla: 300 paragraphs loaded\n",
      "âœ… AI Styled: 200 paragraphs loaded\n",
      "\n",
      "ğŸ“Š Total AI paragraphs: 500\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "DATA_DIR = Path('../data/dataset')\n",
    "CLASS2_FILE = DATA_DIR / 'class2.txt'  # AI Vanilla\n",
    "CLASS3_FILE = DATA_DIR / 'class3.txt'  # AI Styled\n",
    "\n",
    "# Load AI Vanilla\n",
    "ai_vanilla_texts = []\n",
    "if CLASS2_FILE.exists():\n",
    "    with open(CLASS2_FILE, 'r', encoding='utf-8') as f:\n",
    "        ai_vanilla_texts = [line.strip() for line in f if line.strip()]\n",
    "    print(f\"âœ… AI Vanilla: {len(ai_vanilla_texts)} paragraphs loaded\")\n",
    "else:\n",
    "    print(f\"âŒ AI Vanilla file not found: {CLASS2_FILE}\")\n",
    "\n",
    "# Load AI Styled\n",
    "ai_styled_texts = []\n",
    "if CLASS3_FILE.exists():\n",
    "    with open(CLASS3_FILE, 'r', encoding='utf-8') as f:\n",
    "        ai_styled_texts = [line.strip() for line in f if line.strip()]\n",
    "    print(f\"âœ… AI Styled: {len(ai_styled_texts)} paragraphs loaded\")\n",
    "else:\n",
    "    print(f\"âŒ AI Styled file not found: {CLASS3_FILE}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Total AI paragraphs: {len(ai_vanilla_texts) + len(ai_styled_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5192f620",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Select \"Imposter\" Paragraphs for Analysis\n",
    "\n",
    "Choose AI paragraphs that Tier B correctly identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b56646f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Selected 10 imposter paragraphs for analysis:\n",
      "   - AI Vanilla: 5\n",
      "   - AI Styled: 5\n",
      "\n",
      "ğŸ“„ Example Imposter Paragraph (AI Vanilla):\n",
      "   Ambition often masks a profound lack of self-acceptance. The drive to conquer the world frequently stems from a need to prove oneâ€™s worth to a critical parent or a doubting society. The high achiever ...\n"
     ]
    }
   ],
   "source": [
    "# Select diverse examples\n",
    "np.random.seed(42)\n",
    "\n",
    "# Pick 5 AI Vanilla + 5 AI Styled\n",
    "n_samples = 5\n",
    "\n",
    "selected_ai_vanilla = np.random.choice(ai_vanilla_texts, size=min(n_samples, len(ai_vanilla_texts)), replace=False)\n",
    "selected_ai_styled = np.random.choice(ai_styled_texts, size=min(n_samples, len(ai_styled_texts)), replace=False)\n",
    "\n",
    "# Combine\n",
    "imposter_paragraphs = list(selected_ai_vanilla) + list(selected_ai_styled)\n",
    "imposter_labels = ['AI_Vanilla'] * len(selected_ai_vanilla) + ['AI_Styled'] * len(selected_ai_styled)\n",
    "\n",
    "print(f\"âœ… Selected {len(imposter_paragraphs)} imposter paragraphs for analysis:\")\n",
    "print(f\"   - AI Vanilla: {len(selected_ai_vanilla)}\")\n",
    "print(f\"   - AI Styled: {len(selected_ai_styled)}\")\n",
    "\n",
    "# Preview first paragraph\n",
    "print(f\"\\nğŸ“„ Example Imposter Paragraph (AI Vanilla):\")\n",
    "print(f\"   {selected_ai_vanilla[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15cac1e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Verify Model Predictions\n",
    "\n",
    "Ensure Tier B correctly identifies these as AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9520755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "ğŸ“Š Model Predictions on Selected Imposters:\n",
      "\n",
      "True_Label Predicted_Label  Confidence  Correct                                                                        Text_Preview\n",
      "AI_Vanilla      AI_Vanilla    0.998500     True Ambition often masks a profound lack of self-acceptance. The drive to conquer th...\n",
      "AI_Vanilla      AI_Vanilla    0.995679     True The concept of \"home\" is often a myth we spend our lives trying to reconstruct. ...\n",
      "AI_Vanilla      AI_Vanilla    0.999585     True The myth of the \"self-made\" individual erases the complex network of support tha...\n",
      "AI_Vanilla      AI_Vanilla    0.999887     True Ultimately, the endurance of love in a cynical world is a profound moral victory...\n",
      "AI_Vanilla      AI_Vanilla    0.999831     True Redemption requires more than the passage of time. It demands active restitution...\n",
      " AI_Styled       AI_Styled    0.998999     True My treatment of Biddy stands as the monument to my folly. I patronized her, I le...\n",
      " AI_Styled       AI_Styled    0.999347     True Estella was the star I navigated by, cold and distant and unfeeling. I steered m...\n",
      " AI_Styled       AI_Styled    0.985718     True The imagination of a lady is very rapid; it jumps from admiration to love, from ...\n",
      " AI_Styled       AI_Styled    0.999175     True Money is the best receipt for low spirits, but it is a poor substitute for chara...\n",
      " AI_Styled       AI_Styled    0.805060     True The river of my life had flowed so long in one direction that I thought it could...\n",
      "\n",
      "âœ… Tier B Accuracy on Selected Imposters: 100.0%\n",
      "   (Expected: ~95% based on test set performance)\n",
      "\n",
      "ğŸ¯ Using 10 correctly classified paragraphs for saliency analysis.\n",
      "ğŸ“Š Model Predictions on Selected Imposters:\n",
      "\n",
      "True_Label Predicted_Label  Confidence  Correct                                                                        Text_Preview\n",
      "AI_Vanilla      AI_Vanilla    0.998500     True Ambition often masks a profound lack of self-acceptance. The drive to conquer th...\n",
      "AI_Vanilla      AI_Vanilla    0.995679     True The concept of \"home\" is often a myth we spend our lives trying to reconstruct. ...\n",
      "AI_Vanilla      AI_Vanilla    0.999585     True The myth of the \"self-made\" individual erases the complex network of support tha...\n",
      "AI_Vanilla      AI_Vanilla    0.999887     True Ultimately, the endurance of love in a cynical world is a profound moral victory...\n",
      "AI_Vanilla      AI_Vanilla    0.999831     True Redemption requires more than the passage of time. It demands active restitution...\n",
      " AI_Styled       AI_Styled    0.998999     True My treatment of Biddy stands as the monument to my folly. I patronized her, I le...\n",
      " AI_Styled       AI_Styled    0.999347     True Estella was the star I navigated by, cold and distant and unfeeling. I steered m...\n",
      " AI_Styled       AI_Styled    0.985718     True The imagination of a lady is very rapid; it jumps from admiration to love, from ...\n",
      " AI_Styled       AI_Styled    0.999175     True Money is the best receipt for low spirits, but it is a poor substitute for chara...\n",
      " AI_Styled       AI_Styled    0.805060     True The river of my life had flowed so long in one direction that I thought it could...\n",
      "\n",
      "âœ… Tier B Accuracy on Selected Imposters: 100.0%\n",
      "   (Expected: ~95% based on test set performance)\n",
      "\n",
      "ğŸ¯ Using 10 correctly classified paragraphs for saliency analysis.\n"
     ]
    }
   ],
   "source": [
    "# Get embeddings for all imposter paragraphs\n",
    "imposter_embeddings = np.array([text_to_embedding(text, glove_embeddings) for text in imposter_paragraphs])\n",
    "\n",
    "# Get predictions\n",
    "predictions = model.predict(imposter_embeddings)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "predicted_labels = [label_names[i] for i in predicted_classes]\n",
    "\n",
    "# Get confidence scores\n",
    "confidence_scores = np.max(predictions, axis=1)\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'True_Label': imposter_labels,\n",
    "    'Predicted_Label': predicted_labels,\n",
    "    'Confidence': confidence_scores,\n",
    "    'Correct': [true == pred for true, pred in zip(imposter_labels, predicted_labels)],\n",
    "    'Text_Preview': [text[:80] + '...' for text in imposter_paragraphs]\n",
    "})\n",
    "\n",
    "print(\"ğŸ“Š Model Predictions on Selected Imposters:\\n\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = results_df['Correct'].mean()\n",
    "print(f\"\\nâœ… Tier B Accuracy on Selected Imposters: {accuracy*100:.1f}%\")\n",
    "print(f\"   (Expected: ~95% based on test set performance)\")\n",
    "\n",
    "# Filter to correctly classified for saliency analysis\n",
    "correct_mask = results_df['Correct'].values\n",
    "correctly_classified_texts = [text for text, correct in zip(imposter_paragraphs, correct_mask) if correct]\n",
    "correctly_classified_labels = [label for label, correct in zip(imposter_labels, correct_mask) if correct]\n",
    "\n",
    "print(f\"\\nğŸ¯ Using {len(correctly_classified_texts)} correctly classified paragraphs for saliency analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73233def",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: Implement Word-Level SHAP Explainer\n",
    "\n",
    "**Challenge:** Tier B uses averaged embeddings (loses word-level info).\n",
    "\n",
    "**Solution:** Create a wrapper that:\n",
    "1. Takes individual word embeddings\n",
    "2. Averages them\n",
    "3. Feeds to model\n",
    "4. SHAP can then attribute importance back to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e82eade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Word-level explainer created!\n",
      "   Test prediction shape: (1, 3)\n",
      "   Predicted class: Human\n",
      "   Confidence scores: [3.2798994e-02 1.8192646e-04 9.6701908e-01]\n"
     ]
    }
   ],
   "source": [
    "class WordLevelExplainer:\n",
    "    \"\"\"\n",
    "    Wrapper to enable word-level explanations for averaged embedding model.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, embedding_dict):\n",
    "        self.model = model\n",
    "        self.embedding_dict = embedding_dict\n",
    "        self.embedding_dim = 300\n",
    "    \n",
    "    def predict_from_words(self, word_list):\n",
    "        \"\"\"\n",
    "        Predict from list of words.\n",
    "        Args:\n",
    "            word_list: List of words (already tokenized)\n",
    "        Returns:\n",
    "            Predictions (batch_size, 3)\n",
    "        \"\"\"\n",
    "        # Get embeddings for each word\n",
    "        embeddings = []\n",
    "        for word in word_list:\n",
    "            if word in self.embedding_dict:\n",
    "                embeddings.append(self.embedding_dict[word])\n",
    "            else:\n",
    "                embeddings.append(np.zeros(self.embedding_dim))\n",
    "        \n",
    "        # Average embeddings\n",
    "        if len(embeddings) > 0:\n",
    "            avg_embedding = np.mean(embeddings, axis=0)\n",
    "        else:\n",
    "            avg_embedding = np.zeros(self.embedding_dim)\n",
    "        \n",
    "        # Reshape for model (add batch dimension)\n",
    "        avg_embedding = avg_embedding.reshape(1, -1)\n",
    "        \n",
    "        # Get prediction\n",
    "        return self.model.predict(avg_embedding, verbose=0)\n",
    "    \n",
    "    def predict_batch(self, word_lists):\n",
    "        \"\"\"\n",
    "        Predict for batch of word lists.\n",
    "        Args:\n",
    "            word_lists: List of word lists\n",
    "        Returns:\n",
    "            Predictions array (batch_size, 3)\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for word_list in word_lists:\n",
    "            pred = self.predict_from_words(word_list)\n",
    "            predictions.append(pred[0])  # Remove batch dimension\n",
    "        return np.array(predictions)\n",
    "\n",
    "\n",
    "# Create explainer\n",
    "word_explainer = WordLevelExplainer(model, glove_embeddings)\n",
    "\n",
    "# Test it\n",
    "test_words = test_text.lower().split()\n",
    "test_pred = word_explainer.predict_from_words(test_words)\n",
    "\n",
    "print(f\"âœ… Word-level explainer created!\")\n",
    "print(f\"   Test prediction shape: {test_pred.shape}\")\n",
    "print(f\"   Predicted class: {label_names[np.argmax(test_pred)]}\")\n",
    "print(f\"   Confidence scores: {test_pred[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fce947",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 9: Generate SHAP Values (Word-Level Importance)\n",
    "\n",
    "**This may take 2-5 minutes** depending on paragraph length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30ffc2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Analyzing Sample Paragraph (AI_Vanilla):\n",
      "\n",
      "Ambition often masks a profound lack of self-acceptance. The drive to conquer the world frequently stems from a need to prove oneâ€™s worth to a critical parent or a doubting society. The high achiever confuses being admired with being loved. They collect accolades like armor, believing that if they build a fortress of success, they will finally feel safe. However, external achievements rarely fill internal voids. The trophy on the shelf cannot silence the critic in the head. True peace comes only when the individual realizes that their value exists independently of their resume.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Paragraph Statistics:\n",
      "   Total words: 94\n",
      "   Unique words: 74\n",
      "\n",
      "ğŸ¯ Model Prediction:\n",
      "   Predicted: AI_Vanilla\n",
      "   Confidence: 99.70%\n",
      "   Class probabilities:\n",
      "      AI_Styled: 0.30%\n",
      "      AI_Vanilla: 99.70%\n",
      "      Human: 0.00%\n",
      "\n",
      "â³ Computing SHAP values (this may take 1-2 minutes)...\n",
      "   Background data shape: (5, 300)\n",
      "   Sample embedding shape: (1, 300)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f8fc1772dc4e6786d0666850e721f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SHAP values computed!\n",
      "   SHAP shape: (1, 300, 3)\n",
      "   SHAP per embedding dimension: (300, 3)\n",
      "\n",
      "ğŸ” Mapping SHAP values back to words...\n",
      "âœ… Word-level importance computed!\n",
      "   Shape: (94, 3)\n",
      "   (words, classes)\n"
     ]
    }
   ],
   "source": [
    "# Select first correctly classified paragraph for detailed analysis\n",
    "if len(correctly_classified_texts) > 0:\n",
    "    sample_text = correctly_classified_texts[0]\n",
    "    sample_label = correctly_classified_labels[0]\n",
    "    \n",
    "    print(f\"ğŸ” Analyzing Sample Paragraph ({sample_label}):\\n\")\n",
    "    print(f\"{sample_text}\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Tokenize\n",
    "    words = sample_text.lower().split()\n",
    "    print(f\"\\nğŸ“Š Paragraph Statistics:\")\n",
    "    print(f\"   Total words: {len(words)}\")\n",
    "    print(f\"   Unique words: {len(set(words))}\")\n",
    "    \n",
    "    # Get model prediction\n",
    "    pred = word_explainer.predict_from_words(words)\n",
    "    predicted_class = np.argmax(pred)\n",
    "    confidence = pred[0][predicted_class]\n",
    "    \n",
    "    print(f\"\\nğŸ¯ Model Prediction:\")\n",
    "    print(f\"   Predicted: {label_names[predicted_class]}\")\n",
    "    print(f\"   Confidence: {confidence*100:.2f}%\")\n",
    "    print(f\"   Class probabilities:\")\n",
    "    for i, name in enumerate(label_names):\n",
    "        print(f\"      {name}: {pred[0][i]*100:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nâ³ Computing SHAP values (this may take 1-2 minutes)...\")\n",
    "    \n",
    "    # Create SHAP explainer\n",
    "    # Use KernelExplainer (model-agnostic, works with any black-box model)\n",
    "    # Background data: sample of common word combinations (as word lists)\n",
    "    background_word_lists = [\n",
    "        \"the quick brown fox\".split(),\n",
    "        \"a very simple sentence\".split(),\n",
    "        \"human writing is natural\".split(),\n",
    "        \"artificial intelligence generated text\".split(),\n",
    "        \"victorian literature style prose\".split()\n",
    "    ]\n",
    "    \n",
    "    # Convert background data to embeddings for SHAP\n",
    "    # SHAP needs numerical data, not raw words\n",
    "    background_embeddings = np.array([\n",
    "        text_to_embedding(' '.join(word_list), glove_embeddings) \n",
    "        for word_list in background_word_lists\n",
    "    ])\n",
    "    \n",
    "    print(f\"   Background data shape: {background_embeddings.shape}\")\n",
    "    \n",
    "    # Create a model wrapper that takes embeddings directly\n",
    "    def model_predict_from_embeddings(embeddings):\n",
    "        \"\"\"\n",
    "        Predict from embeddings array (for SHAP).\n",
    "        Args:\n",
    "            embeddings: numpy array of shape (batch_size, 300)\n",
    "        Returns:\n",
    "            predictions array (batch_size, 3)\n",
    "        \"\"\"\n",
    "        return model.predict(embeddings, verbose=0)\n",
    "    \n",
    "    # Create SHAP explainer with embeddings\n",
    "    explainer = shap.KernelExplainer(\n",
    "        model_predict_from_embeddings,\n",
    "        background_embeddings,\n",
    "        link=\"identity\"\n",
    "    )\n",
    "    \n",
    "    # Convert our sample paragraph to embedding\n",
    "    sample_embedding = text_to_embedding(sample_text, glove_embeddings).reshape(1, -1)\n",
    "    \n",
    "    print(f\"   Sample embedding shape: {sample_embedding.shape}\")\n",
    "    \n",
    "    # Compute SHAP values for the paragraph-level embedding\n",
    "    shap_values = explainer.shap_values(sample_embedding)\n",
    "    \n",
    "    print(f\"âœ… SHAP values computed!\")\n",
    "    \n",
    "    # Debug: Check the actual shape\n",
    "    shap_array = np.array(shap_values)\n",
    "    print(f\"   SHAP shape: {shap_array.shape}\")\n",
    "    \n",
    "    # SHAP can return different formats:\n",
    "    # - List of arrays: [class0_shap, class1_shap, class2_shap] each (n_samples, n_features)\n",
    "    # - Single array: (n_samples, n_features, n_classes)\n",
    "    \n",
    "    # Normalize to consistent format: (n_samples, n_features, n_classes)\n",
    "    if isinstance(shap_values, list):\n",
    "        # Format: list of arrays, stack them\n",
    "        shap_array = np.stack(shap_values, axis=-1)  # Shape: (n_samples, n_features, n_classes)\n",
    "        print(f\"   Converted list to array shape: {shap_array.shape}\")\n",
    "    else:\n",
    "        # Already an array\n",
    "        shap_array = np.array(shap_values)\n",
    "        # Check if shape is (n_samples, n_features, n_classes) or (n_features, n_classes)\n",
    "        if len(shap_array.shape) == 2:\n",
    "            # Shape is (n_features, n_classes), add sample dimension\n",
    "            shap_array = shap_array[np.newaxis, ...]\n",
    "            print(f\"   Added sample dimension: {shap_array.shape}\")\n",
    "    \n",
    "    # Now shap_array should be (1, 300, 3) or (n_samples, 300, 3)\n",
    "    # Extract first sample: (300, 3)\n",
    "    shap_per_feature = shap_array[0]  # Shape: (300, 3)\n",
    "    print(f\"   SHAP per embedding dimension: {shap_per_feature.shape}\")\n",
    "    \n",
    "    # Now we need to map embedding-level importance back to words\n",
    "    # Method: For each word, compute its contribution to the final embedding\n",
    "    print(f\"\\nğŸ” Mapping SHAP values back to words...\")\n",
    "    \n",
    "    # Get individual word embeddings\n",
    "    word_embeddings_list = []\n",
    "    for word in words:\n",
    "        if word in glove_embeddings:\n",
    "            word_embeddings_list.append(glove_embeddings[word])\n",
    "        else:\n",
    "            word_embeddings_list.append(np.zeros(EMBEDDING_DIM))\n",
    "    \n",
    "    word_embeddings_array = np.array(word_embeddings_list)  # Shape: (num_words, 300)\n",
    "    \n",
    "    # For each word, compute its importance for each class\n",
    "    # Method: dot product of word embedding with SHAP values for each class\n",
    "    # Then divide by number of words (since we averaged)\n",
    "    word_importance_scores = []\n",
    "    \n",
    "    for word_emb in word_embeddings_array:\n",
    "        # Compute importance for each class\n",
    "        class_importances = []\n",
    "        for class_idx in range(3):\n",
    "            # Get SHAP values for this class across all 300 dimensions\n",
    "            class_shap = shap_per_feature[:, class_idx]  # Shape: (300,)\n",
    "            \n",
    "            # Word contribution: dot product of word embedding and SHAP values\n",
    "            # Then divide by number of words (since we averaged)\n",
    "            contribution = np.dot(word_emb, class_shap) / len(words)\n",
    "            \n",
    "            class_importances.append(contribution)\n",
    "        word_importance_scores.append(class_importances)\n",
    "    \n",
    "    word_importance_scores = np.array(word_importance_scores)  # Shape: (num_words, 3)\n",
    "    \n",
    "    print(f\"âœ… Word-level importance computed!\")\n",
    "    print(f\"   Shape: {word_importance_scores.shape}\")\n",
    "    print(f\"   (words, classes)\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No correctly classified paragraphs found for analysis.\")\n",
    "    print(\"   Check if model loaded correctly and test data is available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20059c3a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 10: Visualize Word-Level Importance\n",
    "\n",
    "Create color-coded visualization showing which words signal \"AI\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af22e388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š TOP 20 MOST IMPORTANT WORDS (for predicting 'AI_Vanilla'):\n",
      "\n",
      "================================================================================\n",
      "Word                 SHAP Value      Effect\n",
      "================================================================================\n",
      "ğŸ”´ believing          +0.001500     â†’ AI (strength: 0.0015)\n",
      "ğŸ”´ fortress           +0.001384     â†’ AI (strength: 0.0014)\n",
      "ğŸ”´ value              +0.001356     â†’ AI (strength: 0.0014)\n",
      "ğŸ”´ build              +0.001094     â†’ AI (strength: 0.0011)\n",
      "ğŸ”´ true               +0.001077     â†’ AI (strength: 0.0011)\n",
      "ğŸ”´ prove              +0.000831     â†’ AI (strength: 0.0008)\n",
      "ğŸ”´ individual         +0.000739     â†’ AI (strength: 0.0007)\n",
      "ğŸ”´ doubting           +0.000722     â†’ AI (strength: 0.0007)\n",
      "ğŸ”´ conquer            +0.000721     â†’ AI (strength: 0.0007)\n",
      "ğŸŸ¢ critic             -0.000699     â†’ Human (strength: 0.0007)\n",
      "ğŸŸ¢ high               -0.000668     â†’ Human (strength: 0.0007)\n",
      "ğŸŸ¢ a                  -0.000658     â†’ Human (strength: 0.0007)\n",
      "ğŸŸ¢ a                  -0.000658     â†’ Human (strength: 0.0007)\n",
      "ğŸŸ¢ a                  -0.000658     â†’ Human (strength: 0.0007)\n",
      "ğŸŸ¢ a                  -0.000658     â†’ Human (strength: 0.0007)\n",
      "ğŸŸ¢ a                  -0.000658     â†’ Human (strength: 0.0007)\n",
      "ğŸ”´ being              +0.000643     â†’ AI (strength: 0.0006)\n",
      "ğŸ”´ being              +0.000643     â†’ AI (strength: 0.0006)\n",
      "ğŸ”´ achievements       +0.000608     â†’ AI (strength: 0.0006)\n",
      "ğŸŸ¢ of                 -0.000583     â†’ Human (strength: 0.0006)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ’¡ Interpretation:\n",
      "   ğŸ”´ Positive SHAP = Word pushes toward AI classification\n",
      "   ğŸŸ¢ Negative SHAP = Word pushes toward Human classification\n",
      "   Larger magnitude = Stronger influence\n"
     ]
    }
   ],
   "source": [
    "# Extract word importance for predicted class\n",
    "# word_importance_scores shape: (num_words, 3) - one score per class\n",
    "# We want the scores for the PREDICTED class\n",
    "\n",
    "target_class = label_names[predicted_class]\n",
    "\n",
    "# Get importance scores for the predicted class\n",
    "word_importance = word_importance_scores[:, predicted_class]\n",
    "\n",
    "# Create dataframe\n",
    "word_importance_df = pd.DataFrame({\n",
    "    'Word': words,\n",
    "    'SHAP_Value': word_importance,\n",
    "    'Abs_SHAP': np.abs(word_importance)\n",
    "})\n",
    "\n",
    "# Sort by absolute importance\n",
    "word_importance_df = word_importance_df.sort_values('Abs_SHAP', ascending=False)\n",
    "\n",
    "print(f\"\\nğŸ“Š TOP 20 MOST IMPORTANT WORDS (for predicting '{target_class}'):\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Word':<20} {'SHAP Value':<15} {'Effect'}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for idx, row in word_importance_df.head(20).iterrows():\n",
    "    word = row['Word']\n",
    "    shap_val = row['SHAP_Value']\n",
    "    \n",
    "    if shap_val > 0:\n",
    "        effect = f\"â†’ AI (strength: {shap_val:.4f})\"\n",
    "        marker = \"ğŸ”´\"\n",
    "    else:\n",
    "        effect = f\"â†’ Human (strength: {abs(shap_val):.4f})\"\n",
    "        marker = \"ğŸŸ¢\"\n",
    "    \n",
    "    print(f\"{marker} {word:<18} {shap_val:>+.6f}     {effect}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"\\nğŸ’¡ Interpretation:\")\n",
    "print(f\"   ğŸ”´ Positive SHAP = Word pushes toward AI classification\")\n",
    "print(f\"   ğŸŸ¢ Negative SHAP = Word pushes toward Human classification\")\n",
    "print(f\"   Larger magnitude = Stronger influence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c545cbf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 11: Create HTML Color-Coded Visualization\n",
    "\n",
    "Generate beautiful HTML output with color-coded words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1851d8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š COLOR-CODED SALIENCY MAP:\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ”´ Red = AI signal | ğŸŸ¢ Green = Human signal | âšª Neutral\n",
      "\n",
      "Hover over words to see exact SHAP values!\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"line-height: 2.0; font-size: 16px; font-family: Arial, sans-serif;\"><span style=\"background-color: rgba(255, 0, 0, 0.11502858206723922); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0003\">ambition</span> <span style=\"background-color: rgba(0, 255, 0, 0.024976133827207585); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0001\">often</span> <span style=\"background-color: rgba(255, 0, 0, 0.014058745063948455); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0000\">masks</span> <span style=\"background-color: rgba(0, 255, 0, 0.2193551743086644); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0007\">a</span> <span style=\"background-color: rgba(255, 0, 0, 0.0009206383485969585); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0000\">profound</span> <span style=\"background-color: rgba(255, 0, 0, 0.09730789325678268); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0003\">lack</span> <span style=\"background-color: rgba(0, 255, 0, 0.1942344885491872); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0006\">of</span> <span style=\"background-color: transparent; padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Neutral\">self-acceptance.</span> <span style=\"background-color: rgba(255, 0, 0, 0.028837660499386415); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0001\">the</span> <span style=\"background-color: rgba(0, 255, 0, 0.09276559050721817); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0003\">drive</span> <span style=\"background-color: rgba(0, 255, 0, 0.03513112032929764); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0001\">to</span> <span style=\"background-color: rgba(255, 0, 0, 0.2401922510729681); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0007\">conquer</span> <span style=\"background-color: rgba(255, 0, 0, 0.028837660499386415); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0001\">the</span> <span style=\"background-color: rgba(0, 255, 0, 0.18065650298008637); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0005\">world</span> <span style=\"background-color: rgba(0, 255, 0, 0.04634826303454075); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0001\">frequently</span> <span style=\"background-color: rgba(255, 0, 0, 0.16921808060465368); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0005\">stems</span> <span style=\"background-color: rgba(0, 255, 0, 0.18960207990605307); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0006\">from</span> <span style=\"background-color: rgba(0, 255, 0, 0.2193551743086644); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0007\">a</span> <span style=\"background-color: rgba(255, 0, 0, 0.15938710030548325); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0005\">need</span> <span style=\"background-color: rgba(0, 255, 0, 0.03513112032929764); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0001\">to</span> <span style=\"background-color: rgba(255, 0, 0, 0.27688236771309144); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0008\">prove</span> <span style=\"background-color: transparent; padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Neutral\">oneâ€™s</span> <span style=\"background-color: rgba(255, 0, 0, 0.14550208454834526); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0004\">worth</span> <span style=\"background-color: rgba(0, 255, 0, 0.03513112032929764); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0001\">to</span> <span style=\"background-color: rgba(0, 255, 0, 0.2193551743086644); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0007\">a</span> <span style=\"background-color: rgba(0, 255, 0, 0.10026528512783739); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0003\">critical</span> <span style=\"background-color: rgba(0, 255, 0, 0.06663455416397136); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0002\">parent</span> <span style=\"background-color: rgba(255, 0, 0, 0.06790281979439015); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0002\">or</span> <span style=\"background-color: rgba(0, 255, 0, 0.2193551743086644); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0007\">a</span> <span style=\"background-color: rgba(255, 0, 0, 0.24066723079276098); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0007\">doubting</span> <span style=\"background-color: transparent; padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Neutral\">society.</span> <span style=\"background-color: rgba(255, 0, 0, 0.028837660499386415); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0001\">the</span> <span style=\"background-color: rgba(0, 255, 0, 0.2227523926824219); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0007\">high</span> <span style=\"background-color: rgba(255, 0, 0, 0.01355613387239766); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0000\">achiever</span> <span style=\"background-color: rgba(255, 0, 0, 0.027334774802622603); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0001\">confuses</span> <span style=\"background-color: rgba(255, 0, 0, 0.2144143250505367); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0006\">being</span> <span style=\"background-color: rgba(255, 0, 0, 0.10157743521169742); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0003\">admired</span> <span style=\"background-color: rgba(0, 255, 0, 0.15230032374949917); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0005\">with</span> <span style=\"background-color: rgba(255, 0, 0, 0.2144143250505367); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0006\">being</span> <span style=\"background-color: transparent; padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Neutral\">loved.</span> <span style=\"background-color: rgba(255, 0, 0, 0.12123351235765811); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0004\">they</span> <span style=\"background-color: rgba(255, 0, 0, 0.10550488128797542); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0003\">collect</span> <span style=\"background-color: rgba(255, 0, 0, 0.04130643965560857); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0001\">accolades</span> <span style=\"background-color: rgba(0, 255, 0, 0.149002747542798); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0004\">like</span> <span style=\"background-color: transparent; padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Neutral\">armor,</span> <span style=\"background-color: rgba(255, 0, 0, 0.5); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0015\">believing</span> <span style=\"background-color: rgba(0, 255, 0, 0.0157071661017174); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0000\">that</span> <span style=\"background-color: rgba(255, 0, 0, 0.06740826366352137); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0002\">if</span> <span style=\"background-color: rgba(255, 0, 0, 0.12123351235765811); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0004\">they</span> <span style=\"background-color: rgba(255, 0, 0, 0.3646039661187323); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0011\">build</span> <span style=\"background-color: rgba(0, 255, 0, 0.2193551743086644); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0007\">a</span> <span style=\"background-color: rgba(255, 0, 0, 0.4613577307715989); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0014\">fortress</span> <span style=\"background-color: rgba(0, 255, 0, 0.1942344885491872); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0006\">of</span> <span style=\"background-color: transparent; padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Neutral\">success,</span> <span style=\"background-color: rgba(255, 0, 0, 0.12123351235765811); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0004\">they</span> <span style=\"background-color: rgba(0, 255, 0, 0.15501496938190448); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0005\">will</span> <span style=\"background-color: rgba(255, 0, 0, 0.06512277321515905); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0002\">finally</span> <span style=\"background-color: rgba(255, 0, 0, 0.04991484274459354); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0001\">feel</span> <span style=\"background-color: transparent; padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Neutral\">safe.</span> <span style=\"background-color: transparent; padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Neutral\">however,</span> <span style=\"background-color: rgba(0, 255, 0, 0.03636180785645743); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0001\">external</span> <span style=\"background-color: rgba(255, 0, 0, 0.2025913285628418); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0006\">achievements</span> <span style=\"background-color: rgba(0, 255, 0, 0.09946538041499411); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0003\">rarely</span> <span style=\"background-color: rgba(0, 255, 0, 0.035478561179608445); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0001\">fill</span> <span style=\"background-color: rgba(0, 255, 0, 0.027794078419610603); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0001\">internal</span> <span style=\"background-color: transparent; padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Neutral\">voids.</span> <span style=\"background-color: rgba(255, 0, 0, 0.028837660499386415); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0001\">the</span> <span style=\"background-color: rgba(255, 0, 0, 0.14321407342400777); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0004\">trophy</span> <span style=\"background-color: rgba(0, 255, 0, 0.08760947278516154); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0003\">on</span> <span style=\"background-color: rgba(255, 0, 0, 0.028837660499386415); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0001\">the</span> <span style=\"background-color: rgba(255, 0, 0, 0.11543489441362453); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0003\">shelf</span> <span style=\"background-color: rgba(255, 0, 0, 0.04228808687948964); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0001\">cannot</span> <span style=\"background-color: rgba(0, 255, 0, 0.03428737600748812); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0001\">silence</span> <span style=\"background-color: rgba(255, 0, 0, 0.028837660499386415); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0001\">the</span> <span style=\"background-color: rgba(0, 255, 0, 0.23283614820131165); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0007\">critic</span> <span style=\"background-color: rgba(255, 0, 0, 0.1597153229038555); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0005\">in</span> <span style=\"background-color: rgba(255, 0, 0, 0.028837660499386415); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0001\">the</span> <span style=\"background-color: transparent; padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Neutral\">head.</span> <span style=\"background-color: rgba(255, 0, 0, 0.35904523227858426); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0011\">true</span> <span style=\"background-color: rgba(255, 0, 0, 0.09149889082924194); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0003\">peace</span> <span style=\"background-color: rgba(255, 0, 0, 0.05193917729086467); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0002\">comes</span> <span style=\"background-color: rgba(0, 255, 0, 0.08183813696902019); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0002\">only</span> <span style=\"background-color: rgba(0, 255, 0, 0.10662961554742965); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0003\">when</span> <span style=\"background-color: rgba(255, 0, 0, 0.028837660499386415); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0001\">the</span> <span style=\"background-color: rgba(255, 0, 0, 0.24638147575457353); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0007\">individual</span> <span style=\"background-color: rgba(255, 0, 0, 0.18122620231021183); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0005\">realizes</span> <span style=\"background-color: rgba(0, 255, 0, 0.0157071661017174); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0000\">that</span> <span style=\"background-color: rgba(255, 0, 0, 0.0429442677035538); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0001\">their</span> <span style=\"background-color: rgba(255, 0, 0, 0.45180266727761353); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0014\">value</span> <span style=\"background-color: rgba(255, 0, 0, 0.12903275547048465); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0004\">exists</span> <span style=\"background-color: rgba(255, 0, 0, 0.09996733383121388); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0003\">independently</span> <span style=\"background-color: rgba(0, 255, 0, 0.1942344885491872); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Human signal: -0.0006\">of</span> <span style=\"background-color: rgba(255, 0, 0, 0.0429442677035538); padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"AI signal: +0.0001\">their</span> <span style=\"background-color: transparent; padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"Neutral\">resume.</span> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Saliency map saved to: ../results/visualizations/saliency_map_sample.html\n",
      "   Open this file in a browser to see interactive visualization!\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "def create_colored_text(words, shap_values, max_color_intensity=0.5):\n",
    "    \"\"\"\n",
    "    Create HTML with color-coded words based on SHAP values.\n",
    "    Red = AI signal, Green = Human signal\n",
    "    \"\"\"\n",
    "    # Normalize SHAP values for color intensity\n",
    "    max_abs_shap = np.max(np.abs(shap_values))\n",
    "    if max_abs_shap == 0:\n",
    "        max_abs_shap = 1  # Avoid division by zero\n",
    "    \n",
    "    html = '<div style=\"line-height: 2.0; font-size: 16px; font-family: Arial, sans-serif;\">'\n",
    "    \n",
    "    for word, shap_val in zip(words, shap_values):\n",
    "        # Calculate color intensity (0 to max_color_intensity)\n",
    "        intensity = min(abs(shap_val) / max_abs_shap * max_color_intensity, max_color_intensity)\n",
    "        \n",
    "        if shap_val > 0:  # Pushes toward AI\n",
    "            # Red background\n",
    "            bg_color = f'rgba(255, 0, 0, {intensity})'\n",
    "            title = f'AI signal: +{shap_val:.4f}'\n",
    "        elif shap_val < 0:  # Pushes toward Human\n",
    "            # Green background\n",
    "            bg_color = f'rgba(0, 255, 0, {intensity})'\n",
    "            title = f'Human signal: {shap_val:.4f}'\n",
    "        else:  # Neutral\n",
    "            bg_color = 'transparent'\n",
    "            title = 'Neutral'\n",
    "        \n",
    "        html += f'<span style=\"background-color: {bg_color}; padding: 2px 4px; margin: 2px; border-radius: 3px;\" title=\"{title}\">{word}</span> '\n",
    "    \n",
    "    html += '</div>'\n",
    "    return html\n",
    "\n",
    "\n",
    "# Create visualization\n",
    "html_output = create_colored_text(words, word_importance)\n",
    "\n",
    "print(\"\\nğŸ“Š COLOR-CODED SALIENCY MAP:\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nğŸ”´ Red = AI signal | ğŸŸ¢ Green = Human signal | âšª Neutral\\n\")\n",
    "print(\"Hover over words to see exact SHAP values!\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Display HTML\n",
    "display(HTML(html_output))\n",
    "\n",
    "# Save to file\n",
    "output_file = Path('../results/visualizations/saliency_map_sample.html')\n",
    "output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>TASK 3 - Saliency Map</title>\n",
    "        <style>\n",
    "            body {{\n",
    "                font-family: Arial, sans-serif;\n",
    "                max-width: 1000px;\n",
    "                margin: 50px auto;\n",
    "                padding: 20px;\n",
    "            }}\n",
    "            h1 {{\n",
    "                color: #333;\n",
    "            }}\n",
    "            .legend {{\n",
    "                margin: 20px 0;\n",
    "                padding: 15px;\n",
    "                background-color: #f5f5f5;\n",
    "                border-radius: 5px;\n",
    "            }}\n",
    "            .stats {{\n",
    "                margin: 20px 0;\n",
    "                padding: 15px;\n",
    "                background-color: #e8f4f8;\n",
    "                border-radius: 5px;\n",
    "            }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>ğŸ” TASK 3: Saliency Mapping - Word-Level AI Detection</h1>\n",
    "        \n",
    "        <div class=\"stats\">\n",
    "            <h3>ğŸ“Š Prediction Statistics</h3>\n",
    "            <p><strong>True Label:</strong> {sample_label}</p>\n",
    "            <p><strong>Predicted:</strong> {target_class}</p>\n",
    "            <p><strong>Confidence:</strong> {confidence*100:.2f}%</p>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"legend\">\n",
    "            <h3>Legend</h3>\n",
    "            <p>ğŸ”´ <span style=\"background-color: rgba(255, 0, 0, 0.3); padding: 2px 8px; border-radius: 3px;\">Red</span> = Word signals AI authorship</p>\n",
    "            <p>ğŸŸ¢ <span style=\"background-color: rgba(0, 255, 0, 0.3); padding: 2px 8px; border-radius: 3px;\">Green</span> = Word signals Human authorship</p>\n",
    "            <p>âšª No color = Neutral word (minimal contribution)</p>\n",
    "            <p><em>Hover over words to see exact SHAP values!</em></p>\n",
    "        </div>\n",
    "        \n",
    "        <h3>Analyzed Paragraph:</h3>\n",
    "        {html_output}\n",
    "        \n",
    "        <hr style=\"margin: 40px 0;\">\n",
    "        <p style=\"color: #666; font-size: 14px;\">Generated by TASK 3 - Explainability Analysis | Tier B Model (95% accuracy)</p>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\")\n",
    "\n",
    "print(f\"\\nâœ… Saliency map saved to: {output_file}\")\n",
    "print(f\"   Open this file in a browser to see interactive visualization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540da5e8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 12: Identify Top AI-Signaling Words Across All Samples\n",
    "\n",
    "Analyze all correctly classified paragraphs to find common AI-isms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02afc3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Analyzing ALL correctly classified paragraphs...\n",
      "   Total paragraphs to analyze: 10\n",
      "\n",
      "   â³ This may take 5-10 minutes...\n",
      "\n",
      "   Processing paragraph 1/5..."
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43175a6630c642dd89e9b4e2b9fdb032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ…\n",
      "   Processing paragraph 2/5..."
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe7acd934e9f44e48600696fbcbae8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ…\n",
      "   Processing paragraph 3/5..."
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e757735aeb44a848b602c24fe6189a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ…\n",
      "   Processing paragraph 4/5..."
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95bea873ca644f9823a4052f1dd3c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ…\n",
      "   Processing paragraph 5/5..."
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae58b9318344b37a4fe67f4cb434d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ…\n",
      "\n",
      "âœ… Analysis complete!\n",
      "\n",
      "================================================================================\n",
      "TOP 30 AI-SIGNALING WORDS (across all analyzed paragraphs)\n",
      "================================================================================\n",
      "Rank   Word                 Avg SHAP     Count    Effect\n",
      "================================================================================\n",
      "1      build                +0.001251    2        Strong AI signal ğŸ”´\n",
      "2      value                +0.001086    2        Strong AI signal ğŸ”´\n",
      "3      place                +0.000902    2        Strong AI signal ğŸ”´\n",
      "4      redemption           +0.000826    3        Strong AI signal ğŸ”´\n",
      "5      myth                 +0.000811    2        Strong AI signal ğŸ”´\n",
      "6      as                   +0.000758    4        Strong AI signal ğŸ”´\n",
      "7      their                +0.000558    6        Strong AI signal ğŸ”´\n",
      "8      profound             +0.000547    2        Strong AI signal ğŸ”´\n",
      "9      human                +0.000545    3        Strong AI signal ğŸ”´\n",
      "10     our                  +0.000465    3        Strong AI signal ğŸ”´\n",
      "11     an                   +0.000352    2        Strong AI signal ğŸ”´\n",
      "12     labor                +0.000298    2        Strong AI signal ğŸ”´\n",
      "13     in                   +0.000259    6        Strong AI signal ğŸ”´\n",
      "14     being                +0.000250    2        Strong AI signal ğŸ”´\n",
      "15     individual           +0.000245    2        Strong AI signal ğŸ”´\n",
      "16     for                  +0.000241    2        Strong AI signal ğŸ”´\n",
      "17     moral                +0.000195    2        Strong AI signal ğŸ”´\n",
      "18     love                 +0.000179    3        Strong AI signal ğŸ”´\n",
      "19     be                   +0.000179    3        Strong AI signal ğŸ”´\n",
      "20     we                   +0.000135    4        Strong AI signal ğŸ”´\n",
      "21     this                 +0.000051    4        Strong AI signal ğŸ”´\n",
      "22     often                +0.000026    4        Strong AI signal ğŸ”´\n",
      "23     prove                +0.000015    2        Strong AI signal ğŸ”´\n",
      "24     that                 -0.000004    7        Human signal ğŸŸ¢\n",
      "25     a                    -0.000046    21       Human signal ğŸŸ¢\n",
      "26     or                   -0.000070    5        Human signal ğŸŸ¢\n",
      "27     the                  -0.000118    42       Human signal ğŸŸ¢\n",
      "28     world                -0.000131    2        Human signal ğŸŸ¢\n",
      "29     they                 -0.000136    6        Human signal ğŸŸ¢\n",
      "30     successful           -0.000223    2        Human signal ğŸŸ¢\n",
      "\n",
      "================================================================================\n",
      "\n",
      "âœ… Full results saved to: ../results/top_ai_words.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ” Analyzing ALL correctly classified paragraphs...\")\n",
    "print(f\"   Total paragraphs to analyze: {len(correctly_classified_texts)}\\n\")\n",
    "print(\"   â³ This may take 5-10 minutes...\\n\")\n",
    "\n",
    "# Store all word importance scores\n",
    "all_word_scores = {}\n",
    "\n",
    "for idx, (text, true_label) in enumerate(zip(correctly_classified_texts[:5], correctly_classified_labels[:5])):\n",
    "    print(f\"   Processing paragraph {idx+1}/{min(5, len(correctly_classified_texts))}...\", end=\"\")\n",
    "    \n",
    "    # Tokenize\n",
    "    words_list = text.lower().split()\n",
    "    \n",
    "    # Get prediction\n",
    "    pred = word_explainer.predict_from_words(words_list)\n",
    "    pred_class = np.argmax(pred)\n",
    "    \n",
    "    # Convert paragraph to embedding\n",
    "    para_embedding = text_to_embedding(text, glove_embeddings).reshape(1, -1)\n",
    "    \n",
    "    # Compute SHAP values for this paragraph\n",
    "    shap_vals = explainer.shap_values(para_embedding)\n",
    "    \n",
    "    # Normalize SHAP format (same as Step 9)\n",
    "    shap_array = np.array(shap_vals)\n",
    "    if isinstance(shap_vals, list):\n",
    "        shap_array = np.stack(shap_vals, axis=-1)\n",
    "    else:\n",
    "        if len(shap_array.shape) == 2:\n",
    "            shap_array = shap_array[np.newaxis, ...]\n",
    "    \n",
    "    # Extract first sample: (300, 3)\n",
    "    shap_per_feature = shap_array[0]\n",
    "    \n",
    "    # Map SHAP values back to words\n",
    "    word_embs = []\n",
    "    for word in words_list:\n",
    "        if word in glove_embeddings:\n",
    "            word_embs.append(glove_embeddings[word])\n",
    "        else:\n",
    "            word_embs.append(np.zeros(EMBEDDING_DIM))\n",
    "    word_embs = np.array(word_embs)\n",
    "    \n",
    "    # Compute word importance for predicted class\n",
    "    # Get SHAP values for predicted class across all 300 dimensions\n",
    "    class_shap = shap_per_feature[:, pred_class]  # Shape: (300,)\n",
    "    word_scores = np.dot(word_embs, class_shap) / len(words_list)\n",
    "    \n",
    "    # Store word scores\n",
    "    for word, score in zip(words_list, word_scores):\n",
    "        if word not in all_word_scores:\n",
    "            all_word_scores[word] = []\n",
    "        all_word_scores[word].append(score)\n",
    "    \n",
    "    print(\" âœ…\")\n",
    "\n",
    "print(\"\\nâœ… Analysis complete!\\n\")\n",
    "\n",
    "# Calculate average scores per word\n",
    "word_avg_scores = {\n",
    "    word: {\n",
    "        'mean': np.mean(scores),\n",
    "        'count': len(scores),\n",
    "        'std': np.std(scores)\n",
    "    }\n",
    "    for word, scores in all_word_scores.items()\n",
    "}\n",
    "\n",
    "# Convert to dataframe\n",
    "word_stats_df = pd.DataFrame([\n",
    "    {'Word': word, **stats}\n",
    "    for word, stats in word_avg_scores.items()\n",
    "])\n",
    "\n",
    "# Filter: Words that appear at least 2 times\n",
    "word_stats_df = word_stats_df[word_stats_df['count'] >= 2]\n",
    "\n",
    "# Sort by mean SHAP value (positive = AI signal)\n",
    "word_stats_df = word_stats_df.sort_values('mean', ascending=False)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TOP 30 AI-SIGNALING WORDS (across all analyzed paragraphs)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Rank':<6} {'Word':<20} {'Avg SHAP':<12} {'Count':<8} {'Effect'}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for rank, (idx, row) in enumerate(word_stats_df.head(30).iterrows(), 1):\n",
    "    word = row['Word']\n",
    "    mean_shap = row['mean']\n",
    "    count = int(row['count'])\n",
    "    \n",
    "    if mean_shap > 0:\n",
    "        effect = \"Strong AI signal ğŸ”´\"\n",
    "    else:\n",
    "        effect = \"Human signal ğŸŸ¢\"\n",
    "    \n",
    "    print(f\"{rank:<6} {word:<20} {mean_shap:>+.6f}    {count:<8} {effect}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Save results\n",
    "output_csv = Path('../results/top_ai_words.csv')\n",
    "word_stats_df.to_csv(output_csv, index=False)\n",
    "print(f\"\\nâœ… Full results saved to: {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8184d5b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 13: Categorize Words by Type\n",
    "\n",
    "Are they AI-isms, Victorian words, or structural markers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df2f3bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "WORD CATEGORIZATION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ¤– AI CLICHÃ‰S DETECTED (0 found):\n",
      "\n",
      "   None found in top 50 AI-signaling words.\n",
      "\n",
      "ğŸ“š VICTORIAN VOCABULARY (0 found):\n",
      "\n",
      "   None found in top 50 AI-signaling words.\n",
      "\n",
      "ğŸ“ OTHER HIGH-IMPORTANCE WORDS (Top 10):\n",
      "\n",
      "   â€¢ 'build' - SHAP: +0.001251 (appears 2x)\n",
      "   â€¢ 'value' - SHAP: +0.001086 (appears 2x)\n",
      "   â€¢ 'place' - SHAP: +0.000902 (appears 2x)\n",
      "   â€¢ 'redemption' - SHAP: +0.000826 (appears 3x)\n",
      "   â€¢ 'myth' - SHAP: +0.000811 (appears 2x)\n",
      "   â€¢ 'as' - SHAP: +0.000758 (appears 4x)\n",
      "   â€¢ 'their' - SHAP: +0.000558 (appears 6x)\n",
      "   â€¢ 'profound' - SHAP: +0.000547 (appears 2x)\n",
      "   â€¢ 'human' - SHAP: +0.000545 (appears 3x)\n",
      "   â€¢ 'our' - SHAP: +0.000465 (appears 3x)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ’¡ KEY FINDING:\n",
      "   âš ï¸ Known AI-isms not prominent. Model relies on OTHER semantic signals.\n",
      "   â†’ Likely detecting: Era-specific context, not specific buzzwords.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Define word categories\n",
    "AI_CLICHES = [\n",
    "    'tapestry', 'delve', 'testament', 'navigate', 'landscape', \n",
    "    'robust', 'comprehensive', 'intricate', 'nuanced', 'leverage',\n",
    "    'framework', 'paradigm', 'multifaceted', 'holistic', 'resonate',\n",
    "    'transformative', 'dynamic', 'innovative', 'strategic', 'synergy'\n",
    "]\n",
    "\n",
    "VICTORIAN_WORDS = [\n",
    "    'countenance', 'parlour', 'hitherto', 'ere', 'whence', 'thee',\n",
    "    'thou', 'thy', 'therein', 'thereof', 'wherein', 'henceforward',\n",
    "    'forthwith', 'betwixt', 'heretofore', 'perchance'\n",
    "]\n",
    "\n",
    "# Categorize top AI-signaling words\n",
    "top_ai_words = word_stats_df[word_stats_df['mean'] > 0].head(50)\n",
    "\n",
    "ai_cliches_found = []\n",
    "victorian_found = []\n",
    "other_words = []\n",
    "\n",
    "for idx, row in top_ai_words.iterrows():\n",
    "    word = row['Word']\n",
    "    if word in AI_CLICHES:\n",
    "        ai_cliches_found.append((word, row['mean'], row['count']))\n",
    "    elif word in VICTORIAN_WORDS:\n",
    "        victorian_found.append((word, row['mean'], row['count']))\n",
    "    else:\n",
    "        other_words.append((word, row['mean'], row['count']))\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"WORD CATEGORIZATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nğŸ¤– AI CLICHÃ‰S DETECTED ({len(ai_cliches_found)} found):\\n\")\n",
    "if ai_cliches_found:\n",
    "    for word, shap, count in ai_cliches_found:\n",
    "        print(f\"   ğŸ”´ '{word}' - SHAP: {shap:+.6f} (appears {int(count)}x)\")\n",
    "else:\n",
    "    print(\"   None found in top 50 AI-signaling words.\")\n",
    "\n",
    "print(f\"\\nğŸ“š VICTORIAN VOCABULARY ({len(victorian_found)} found):\\n\")\n",
    "if victorian_found:\n",
    "    for word, shap, count in victorian_found:\n",
    "        print(f\"   ğŸŸ¢ '{word}' - SHAP: {shap:+.6f} (appears {int(count)}x)\")\n",
    "else:\n",
    "    print(\"   None found in top 50 AI-signaling words.\")\n",
    "\n",
    "print(f\"\\nğŸ“ OTHER HIGH-IMPORTANCE WORDS (Top 10):\\n\")\n",
    "for word, shap, count in other_words[:10]:\n",
    "    print(f\"   â€¢ '{word}' - SHAP: {shap:+.6f} (appears {int(count)}x)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nğŸ’¡ KEY FINDING:\")\n",
    "if len(ai_cliches_found) > 0:\n",
    "    print(f\"   âœ… AI-isms ARE detectable! Found {len(ai_cliches_found)} known AI clichÃ©s.\")\n",
    "else:\n",
    "    print(f\"   âš ï¸ Known AI-isms not prominent. Model relies on OTHER semantic signals.\")\n",
    "    print(f\"   â†’ Likely detecting: Era-specific context, not specific buzzwords.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64395d80",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Next Steps\n",
    "\n",
    "### What We've Done âœ…\n",
    "\n",
    "1. **Loaded Tier B model** (95% accuracy - the winner)\n",
    "2. **Selected imposter paragraphs** (AI text correctly classified)\n",
    "3. **Implemented word-level SHAP explainer** (overcame averaged embedding challenge)\n",
    "4. **Generated saliency maps** (color-coded word importance)\n",
    "5. **Identified top AI-signaling words** (across multiple paragraphs)\n",
    "6. **Categorized words** (AI-isms vs Victorian vs other)\n",
    "\n",
    "### Key Findings ğŸ”\n",
    "\n",
    "**From the analysis above:**\n",
    "- Top AI-signaling words revealed\n",
    "- AI clichÃ©s detected (if present)\n",
    "- Vocabulary-based vs context-based detection distinguished\n",
    "\n",
    "### Next Steps ğŸ“‹\n",
    "\n",
    "**Part 2: The Findings**\n",
    "- Vocabulary vs Structure: What's the primary signal?\n",
    "- AI-isms analysis: Are they real?\n",
    "- Victorian semantic gap: Quantify the temporal effect\n",
    "\n",
    "**Part 3: Error Analysis**\n",
    "- Find 3 human paragraphs misclassified as AI\n",
    "- Analyze WHY the model failed\n",
    "- Generate saliency maps for errors\n",
    "\n",
    "---\n",
    "\n",
    "**Next notebook:** `task3_findings_analysis.ipynb`\n",
    "\n",
    "**Status:** Saliency Mapping âœ… COMPLETE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
