{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2b69e29",
   "metadata": {},
   "source": [
    "# Task 0: The Library of Babel - Dataset Creation\n",
    "## The Ghost in the Machine - NLP Task\n",
    "\n",
    "**Author:** [Your Name]  \n",
    "**Date:** February 3, 2026  \n",
    "**Task:** Create a dataset with 3 classes - Human, AI Vanilla, AI Styled\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "Build a dataset where authorship (human vs AI), not topic, is the primary variable.\n",
    "\n",
    "**Classes:**\n",
    "1. **Human** - Paragraphs from classic literature (Dickens, Austen)\n",
    "2. **AI Vanilla** - AI-generated paragraphs on same topics\n",
    "3. **AI Styled** - AI-generated paragraphs mimicking author's style\n",
    "\n",
    "**Target:** 500 paragraphs per class (1500 total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdc93c1",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed6e4a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/yogansh/miniconda3/lib/python3.10/site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/yogansh/miniconda3/lib/python3.10/site-packages (4.13.3)\n",
      "Requirement already satisfied: nltk in /home/yogansh/miniconda3/lib/python3.10/site-packages (3.9.2)\n",
      "Requirement already satisfied: spacy in /home/yogansh/miniconda3/lib/python3.10/site-packages (3.8.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from beautifulsoup4) (4.12.2)\n",
      "Requirement already satisfied: click in /home/yogansh/miniconda3/lib/python3.10/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/yogansh/miniconda3/lib/python3.10/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from nltk) (2026.1.15)\n",
      "Requirement already satisfied: tqdm in /home/yogansh/miniconda3/lib/python3.10/site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from spacy) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from spacy) (0.21.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from spacy) (2.1.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from spacy) (2.11.1)\n",
      "Requirement already satisfied: jinja2 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from spacy) (3.1.5)\n",
      "Requirement already satisfied: setuptools in /home/yogansh/miniconda3/lib/python3.10/site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: wrapt in /home/yogansh/miniconda3/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (1.17.3)\n",
      "Requirement already satisfied: wrapt in /home/yogansh/miniconda3/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (1.17.3)\n",
      "Requirement already satisfied: google-generativeai in /home/yogansh/miniconda3/lib/python3.10/site-packages (0.8.6)\n",
      "Requirement already satisfied: pandas in /home/yogansh/miniconda3/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /home/yogansh/miniconda3/lib/python3.10/site-packages (2.1.3)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-generativeai) (2.29.0)\n",
      "Requirement already satisfied: google-api-python-client in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-generativeai) (2.188.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-generativeai) (2.48.0)\n",
      "Requirement already satisfied: protobuf in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-generativeai) (2.11.1)\n",
      "Requirement already satisfied: tqdm in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-generativeai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-generativeai) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.27.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-api-core->google-generativeai) (1.72.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: cryptography>=38.0.3 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai) (43.0.3)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (0.31.2)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (0.3.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from pydantic->google-generativeai) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from pydantic->google-generativeai) (0.4.0)\n",
      "Requirement already satisfied: google-generativeai in /home/yogansh/miniconda3/lib/python3.10/site-packages (0.8.6)\n",
      "Requirement already satisfied: pandas in /home/yogansh/miniconda3/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /home/yogansh/miniconda3/lib/python3.10/site-packages (2.1.3)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-generativeai) (2.29.0)\n",
      "Requirement already satisfied: google-api-python-client in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-generativeai) (2.188.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-generativeai) (2.48.0)\n",
      "Requirement already satisfied: protobuf in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-generativeai) (2.11.1)\n",
      "Requirement already satisfied: tqdm in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-generativeai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-generativeai) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.27.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-api-core->google-generativeai) (1.72.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: cryptography>=38.0.3 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai) (43.0.3)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (0.31.2)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (0.3.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from pydantic->google-generativeai) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from pydantic->google-generativeai) (0.4.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from cryptography>=38.0.3->google-auth>=2.15.0->google-generativeai) (1.17.1)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.75.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: pyparsing<4,>=3.1 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2024.8.30)\n",
      "Requirement already satisfied: pycparser in /home/yogansh/miniconda3/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=38.0.3->google-auth>=2.15.0->google-generativeai) (2.21)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from cryptography>=38.0.3->google-auth>=2.15.0->google-generativeai) (1.17.1)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.75.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: pyparsing<4,>=3.1 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2024.8.30)\n",
      "Requirement already satisfied: pycparser in /home/yogansh/miniconda3/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=38.0.3->google-auth>=2.15.0->google-generativeai) (2.21)\n",
      "Requirement already satisfied: matplotlib in /home/yogansh/miniconda3/lib/python3.10/site-packages (3.10.1)\n",
      "Requirement already satisfied: seaborn in /home/yogansh/miniconda3/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: tqdm in /home/yogansh/miniconda3/lib/python3.10/site-packages (4.66.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from matplotlib) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /home/yogansh/miniconda3/lib/python3.10/site-packages (3.10.1)\n",
      "Requirement already satisfied: seaborn in /home/yogansh/miniconda3/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: tqdm in /home/yogansh/miniconda3/lib/python3.10/site-packages (4.66.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from matplotlib) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: python-dotenv in /home/yogansh/miniconda3/lib/python3.10/site-packages (1.2.1)\n",
      "Requirement already satisfied: scikit-learn in /home/yogansh/miniconda3/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: python-dotenv in /home/yogansh/miniconda3/lib/python3.10/site-packages (1.2.1)\n",
      "Requirement already satisfied: scikit-learn in /home/yogansh/miniconda3/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/yogansh/miniconda3/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/12.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install requests beautifulsoup4 nltk spacy\n",
    "!pip install google-generativeai pandas numpy\n",
    "!pip install matplotlib seaborn tqdm\n",
    "!pip install python-dotenv scikit-learn\n",
    "\n",
    "# Download spaCy model\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "839ce3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import google.generativeai as genai\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Setup\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✓ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c4356a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory structure\n",
    "directories = [\n",
    "    'data/raw',\n",
    "    'data/processed',\n",
    "    'data/dataset',\n",
    "    'results/visualizations'\n",
    "]\n",
    "\n",
    "for dir_path in directories:\n",
    "    Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"✓ Created: {dir_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30148f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Gemini API\n",
    "# IMPORTANT: Get your API key from https://makersuite.google.com/app/apikey\n",
    "\n",
    "GEMINI_API_KEY = 'YOUR_API_KEY_HERE'  # Replace with your actual API key\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "print(\"✓ Gemini API configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8f85e1",
   "metadata": {},
   "source": [
    "## 2. Configuration & Author Selection\n",
    "\n",
    "**Decision: Charles Dickens + Jane Austen**\n",
    "\n",
    "**Justification:**\n",
    "- Both have distinctive, well-documented writing styles\n",
    "- Different enough to test model robustness\n",
    "- Available on Project Gutenberg\n",
    "- Well-suited for style mimicry experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cfb552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'authors': {\n",
    "        'dickens': {\n",
    "            'name': 'Charles Dickens',\n",
    "            'book': 'Great Expectations',\n",
    "            'gutenberg_id': 1400,\n",
    "            'target_paragraphs': 250\n",
    "        },\n",
    "        'austen': {\n",
    "            'name': 'Jane Austen',\n",
    "            'book': 'Pride and Prejudice',\n",
    "            'gutenberg_id': 1342,\n",
    "            'target_paragraphs': 250\n",
    "        }\n",
    "    },\n",
    "    'paragraph_length': {\n",
    "        'min_words': 100,\n",
    "        'max_words': 200\n",
    "    },\n",
    "    'dataset_size': {\n",
    "        'class1_human': 500,\n",
    "        'class2_ai_vanilla': 500,\n",
    "        'class3_ai_styled': 500\n",
    "    },\n",
    "    'api_settings': {\n",
    "        'rate_limit_delay': 1,  # seconds between API calls\n",
    "        'checkpoint_interval': 50  # save every N samples\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(json.dumps(CONFIG, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77e85df",
   "metadata": {},
   "source": [
    "## 3. Helper Functions - Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0756305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_gutenberg_book(book_id, save_path):\n",
    "    \"\"\"\n",
    "    Download book from Project Gutenberg\n",
    "    \n",
    "    Args:\n",
    "        book_id: Gutenberg book ID number\n",
    "        save_path: Where to save the downloaded text\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    urls = [\n",
    "        f\"https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt\",\n",
    "        f\"https://www.gutenberg.org/files/{book_id}/{book_id}.txt\",\n",
    "        f\"https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}.txt\"\n",
    "    ]\n",
    "    \n",
    "    for url in urls:\n",
    "        try:\n",
    "            print(f\"Trying: {url}\")\n",
    "            response = requests.get(url, timeout=30)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                with open(save_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(response.text)\n",
    "                print(f\"✓ Successfully downloaded to {save_path}\")\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            print(f\"  Failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"✗ Could not download book {book_id}\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b3d8120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gutenberg_text(raw_text):\n",
    "    \"\"\"\n",
    "    Remove Project Gutenberg metadata and clean text\n",
    "    \n",
    "    Critical for ensuring we only analyze author's actual writing,\n",
    "    not Gutenberg boilerplate.\n",
    "    \"\"\"\n",
    "    # Find content boundaries\n",
    "    start_patterns = [\n",
    "        r\"\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK.*?\\*\\*\\*\",\n",
    "        r\"\\*\\*\\* START OF THE PROJECT GUTENBERG EBOOK.*?\\*\\*\\*\"\n",
    "    ]\n",
    "    \n",
    "    end_patterns = [\n",
    "        r\"\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK.*?\\*\\*\\*\",\n",
    "        r\"\\*\\*\\* END OF THE PROJECT GUTENBERG EBOOK.*?\\*\\*\\*\"\n",
    "    ]\n",
    "    \n",
    "    # Extract main content\n",
    "    text = raw_text\n",
    "    \n",
    "    for pattern in start_patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "        if match:\n",
    "            text = text[match.end():]\n",
    "            print(\"✓ Removed header metadata\")\n",
    "            break\n",
    "    \n",
    "    for pattern in end_patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "        if match:\n",
    "            text = text[:match.start()]\n",
    "            print(\"✓ Removed footer metadata\")\n",
    "            break\n",
    "    \n",
    "    # Remove chapter headers\n",
    "    text = re.sub(r'^CHAPTER [IVXLCDM]+\\.?\\s*$', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'^Chapter \\d+\\.?\\s*$', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Clean whitespace\n",
    "    text = re.sub(r'\\n\\n\\n+', '\\n\\n', text)  # Multiple newlines to double\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)  # Multiple spaces to single\n",
    "    \n",
    "    # Remove illustrations\n",
    "    text = re.sub(r'\\[Illustration:.*?\\]', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    print(f\"✓ Cleaned text: {len(text)} characters\")\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1147ac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paragraphs(text, min_words=100, max_words=200, n_samples=500):\n",
    "    \"\"\"\n",
    "    Extract valid paragraphs from cleaned text\n",
    "    \n",
    "    Filters by word count to match AI-generated paragraph length.\n",
    "    \"\"\"\n",
    "    # Split by double newlines (paragraph separator)\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    \n",
    "    valid_paragraphs = []\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        para = para.strip()\n",
    "        if not para:\n",
    "            continue\n",
    "        \n",
    "        # Count words\n",
    "        words = para.split()\n",
    "        word_count = len(words)\n",
    "        \n",
    "        # Filter by length\n",
    "        if min_words <= word_count <= max_words:\n",
    "            valid_paragraphs.append({\n",
    "                'text': para,\n",
    "                'word_count': word_count,\n",
    "                'char_count': len(para)\n",
    "            })\n",
    "    \n",
    "    print(f\"✓ Found {len(valid_paragraphs)} valid paragraphs\")\n",
    "    \n",
    "    # Sample if too many\n",
    "    if len(valid_paragraphs) > n_samples:\n",
    "        valid_paragraphs = random.sample(valid_paragraphs, n_samples)\n",
    "        print(f\"✓ Sampled {n_samples} paragraphs\")\n",
    "    \n",
    "    return valid_paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57cbacb",
   "metadata": {},
   "source": [
    "## 4. CLASS 1: Download and Extract Human Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eee9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Dickens\n",
    "dickens_raw_path = 'data/raw/dickens_great_expectations.txt'\n",
    "download_gutenberg_book(CONFIG['authors']['dickens']['gutenberg_id'], dickens_raw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd61f11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Austen\n",
    "austen_raw_path = 'data/raw/austen_pride_prejudice.txt'\n",
    "download_gutenberg_book(CONFIG['authors']['austen']['gutenberg_id'], austen_raw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b416cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Dickens\n",
    "with open(dickens_raw_path, 'r', encoding='utf-8') as f:\n",
    "    dickens_raw = f.read()\n",
    "\n",
    "print(f\"Raw text length: {len(dickens_raw)} characters\")\n",
    "dickens_cleaned = clean_gutenberg_text(dickens_raw)\n",
    "\n",
    "# Save cleaned version\n",
    "with open('data/processed/dickens_cleaned.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(dickens_cleaned)\n",
    "\n",
    "print(\"\\nFirst 500 characters:\")\n",
    "print(dickens_cleaned[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65c4a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Austen\n",
    "with open(austen_raw_path, 'r', encoding='utf-8') as f:\n",
    "    austen_raw = f.read()\n",
    "\n",
    "print(f\"Raw text length: {len(austen_raw)} characters\")\n",
    "austen_cleaned = clean_gutenberg_text(austen_raw)\n",
    "\n",
    "# Save cleaned version\n",
    "with open('data/processed/austen_cleaned.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(austen_cleaned)\n",
    "\n",
    "print(\"\\nFirst 500 characters:\")\n",
    "print(austen_cleaned[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbd8402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract paragraphs from both authors\n",
    "dickens_paragraphs = extract_paragraphs(\n",
    "    dickens_cleaned, \n",
    "    min_words=CONFIG['paragraph_length']['min_words'],\n",
    "    max_words=CONFIG['paragraph_length']['max_words'],\n",
    "    n_samples=CONFIG['authors']['dickens']['target_paragraphs']\n",
    ")\n",
    "\n",
    "austen_paragraphs = extract_paragraphs(\n",
    "    austen_cleaned,\n",
    "    min_words=CONFIG['paragraph_length']['min_words'],\n",
    "    max_words=CONFIG['paragraph_length']['max_words'],\n",
    "    n_samples=CONFIG['authors']['austen']['target_paragraphs']\n",
    ")\n",
    "\n",
    "# Add author labels\n",
    "for p in dickens_paragraphs:\n",
    "    p['author'] = 'dickens'\n",
    "    p['author_full'] = 'Charles Dickens'\n",
    "    p['book'] = 'Great Expectations'\n",
    "\n",
    "for p in austen_paragraphs:\n",
    "    p['author'] = 'austen'\n",
    "    p['author_full'] = 'Jane Austen'\n",
    "    p['book'] = 'Pride and Prejudice'\n",
    "\n",
    "# Combine\n",
    "class1_human = dickens_paragraphs + austen_paragraphs\n",
    "\n",
    "print(f\"\\n✓ Class 1 (Human) total: {len(class1_human)} paragraphs\")\n",
    "print(f\"  - Dickens: {len(dickens_paragraphs)}\")\n",
    "print(f\"  - Austen: {len(austen_paragraphs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0bb695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Class 1\n",
    "with open('data/dataset/class1_human.jsonl', 'w') as f:\n",
    "    for item in class1_human:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(\"✓ Saved class1_human.jsonl\")\n",
    "\n",
    "# Show example\n",
    "print(\"\\nExample human paragraph:\")\n",
    "print(\"=\"*80)\n",
    "print(class1_human[0]['text'])\n",
    "print(\"=\"*80)\n",
    "print(f\"Words: {class1_human[0]['word_count']}, Author: {class1_human[0]['author_full']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f0de50",
   "metadata": {},
   "source": [
    "## 5. Topic Extraction\n",
    "\n",
    "**Approach: Manual Topic Identification**\n",
    "\n",
    "I'm using manually curated topics that are universal across both books.\n",
    "This ensures AI-generated text is thematically comparable to human text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2435e34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define universal topics\n",
    "TOPICS = [\n",
    "    \"The nature of social class and ambition\",\n",
    "    \"Love, marriage, and romantic relationships\",\n",
    "    \"Personal growth and self-discovery\",\n",
    "    \"Family bonds and responsibility\",\n",
    "    \"The conflict between appearance and reality\",\n",
    "    \"Wealth and its moral implications\",\n",
    "    \"Justice, morality, and redemption\",\n",
    "    \"The role of society in shaping individuals\",\n",
    "    \"Pride, prejudice, and human flaws\",\n",
    "    \"Dreams, expectations, and disappointment\"\n",
    "]\n",
    "\n",
    "print(\"Selected Topics:\")\n",
    "for i, topic in enumerate(TOPICS, 1):\n",
    "    print(f\"{i}. {topic}\")\n",
    "\n",
    "print(f\"\\n✓ Total topics: {len(TOPICS)}\")\n",
    "print(f\"✓ Samples per topic: {CONFIG['dataset_size']['class2_ai_vanilla'] // len(TOPICS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4943ba",
   "metadata": {},
   "source": [
    "## 6. Helper Functions - AI Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8860a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vanilla_paragraph(topic, word_range=(100, 200), max_retries=3):\n",
    "    \"\"\"\n",
    "    Generate AI paragraph without style constraints\n",
    "    \n",
    "    Returns:\n",
    "        str or None: Generated text, or None if failed\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Write a single paragraph (between {word_range[0]} and {word_range[1]} words) \n",
    "discussing the following topic:\n",
    "\n",
    "Topic: {topic}\n",
    "\n",
    "Requirements:\n",
    "- Write in a clear, thoughtful, analytical style\n",
    "- Focus on the topic with depth and insight\n",
    "- Use varied sentence structures\n",
    "- Be engaging and informative\n",
    "- Do NOT include a title, heading, or meta-commentary\n",
    "\n",
    "Write only the paragraph.\"\"\"\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = model.generate_content(prompt)\n",
    "            text = response.text.strip()\n",
    "            \n",
    "            # Basic validation\n",
    "            word_count = len(text.split())\n",
    "            if word_range[0] <= word_count <= word_range[1]:\n",
    "                return text\n",
    "            else:\n",
    "                print(f\"  Retry {attempt+1}: Word count {word_count} out of range\")\n",
    "                time.sleep(1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error on attempt {attempt+1}: {e}\")\n",
    "            time.sleep(2)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97673724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define author style profiles\n",
    "AUTHOR_STYLES = {\n",
    "    'dickens': {\n",
    "        'name': 'Charles Dickens',\n",
    "        'description': \"\"\"- Uses long, flowing sentences with multiple clauses\n",
    "- Rich in descriptive adjectives and vivid imagery  \n",
    "- Employs serialization (lists of three or more items)\n",
    "- Frequent use of semicolons and em-dashes\n",
    "- Social commentary woven into descriptions\n",
    "- Ironic and satirical undertones\n",
    "- Victorian-era vocabulary and sensibilities\n",
    "- Character-driven observations\n",
    "- Dramatic and emotional language\"\"\",\n",
    "        'sample': \"\"\"My father's family name being Pirrip, and my Christian name Philip, \n",
    "my infant tongue could make of both names nothing longer or more explicit than Pip. \n",
    "So, I called myself Pip, and came to be called Pip.\"\"\"\n",
    "    },\n",
    "    \n",
    "    'austen': {\n",
    "        'name': 'Jane Austen',\n",
    "        'description': \"\"\"- Witty and ironic tone\n",
    "- Free indirect discourse (blending narrator and character perspective)\n",
    "- Elegant, balanced sentences\n",
    "- Sharp social observations\n",
    "- Clever dialogue and repartee\n",
    "- Restrained emotional expression\n",
    "- Regency-era propriety and manners\n",
    "- Subtle humor and satire\n",
    "- Precise, economical language\"\"\",\n",
    "        'sample': \"\"\"It is a truth universally acknowledged, that a single man in \n",
    "possession of a good fortune, must be in want of a wife.\"\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Author style profiles loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b028e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_styled_paragraph(topic, author_key, word_range=(100, 200), max_retries=3):\n",
    "    \"\"\"\n",
    "    Generate paragraph mimicking specific author's style\n",
    "    \"\"\"\n",
    "    author = AUTHOR_STYLES[author_key]\n",
    "    \n",
    "    prompt = f\"\"\"You are a highly skilled writer trained to perfectly mimic the style of {author['name']}.\n",
    "\n",
    "{author['name']}'s distinctive writing style:\n",
    "{author['description']}\n",
    "\n",
    "Here is a sample of {author['name']}'s actual writing:\n",
    "---\n",
    "{author['sample']}\n",
    "---\n",
    "\n",
    "Now, write a single paragraph (between {word_range[0]} and {word_range[1]} words) \n",
    "on the following topic, written EXACTLY as {author['name']} would:\n",
    "\n",
    "Topic: {topic}\n",
    "\n",
    "Capture their:\n",
    "- Sentence structure and rhythm\n",
    "- Vocabulary choices and register\n",
    "- Use of literary devices\n",
    "- Tone and voice\n",
    "- Era-appropriate language\n",
    "\n",
    "Write ONLY the paragraph, no title or commentary.\"\"\"\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = model.generate_content(prompt)\n",
    "            text = response.text.strip()\n",
    "            \n",
    "            # Validation\n",
    "            word_count = len(text.split())\n",
    "            if word_range[0] <= word_count <= word_range[1]:\n",
    "                return text\n",
    "            else:\n",
    "                print(f\"  Retry {attempt+1}: Word count {word_count} out of range\")\n",
    "                time.sleep(1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error on attempt {attempt+1}: {e}\")\n",
    "            time.sleep(2)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2439f35",
   "metadata": {},
   "source": [
    "## 7. CLASS 2: Generate AI Vanilla Paragraphs\n",
    "\n",
    "**This will take approximately 8-10 minutes**\n",
    "- 500 API calls at 1 request/second\n",
    "- With rate limiting and error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c077cd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class2_dataset(topics, samples_per_topic=50, output_file='data/dataset/class2_ai_vanilla.jsonl'):\n",
    "    \"\"\"\n",
    "    Generate AI vanilla paragraphs\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    total_target = samples_per_topic * len(topics)\n",
    "    \n",
    "    print(f\"Generating {total_target} AI vanilla paragraphs...\")\n",
    "    print(f\"Rate limit: {CONFIG['api_settings']['rate_limit_delay']}s between requests\")\n",
    "    \n",
    "    with tqdm(total=total_target) as pbar:\n",
    "        for topic in topics:\n",
    "            for i in range(samples_per_topic):\n",
    "                # Generate\n",
    "                text = generate_vanilla_paragraph(\n",
    "                    topic, \n",
    "                    word_range=(CONFIG['paragraph_length']['min_words'], \n",
    "                               CONFIG['paragraph_length']['max_words'])\n",
    "                )\n",
    "                \n",
    "                if text:\n",
    "                    dataset.append({\n",
    "                        'text': text,\n",
    "                        'topic': topic,\n",
    "                        'class': 'ai_vanilla',\n",
    "                        'word_count': len(text.split()),\n",
    "                        'generation_id': len(dataset)\n",
    "                    })\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "                \n",
    "                # Rate limiting\n",
    "                time.sleep(CONFIG['api_settings']['rate_limit_delay'])\n",
    "                \n",
    "                # Checkpoint\n",
    "                if len(dataset) % CONFIG['api_settings']['checkpoint_interval'] == 0:\n",
    "                    with open(output_file, 'w') as f:\n",
    "                        for item in dataset:\n",
    "                            f.write(json.dumps(item) + '\\n')\n",
    "                    print(f\"\\n  Checkpoint: {len(dataset)} samples saved\")\n",
    "    \n",
    "    # Final save\n",
    "    with open(output_file, 'w') as f:\n",
    "        for item in dataset:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "    \n",
    "    print(f\"\\n✓ Generated {len(dataset)} AI vanilla paragraphs\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8625bbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Class 2 - Running generation (takes ~10 minutes)\n",
    "class2_ai_vanilla = generate_class2_dataset(\n",
    "    TOPICS, \n",
    "    samples_per_topic=50,\n",
    "    output_file='data/dataset/class2_ai_vanilla.jsonl'\n",
    ")\n",
    "\n",
    "# OR load if already generated:\n",
    "# class2_ai_vanilla = []\n",
    "# with open('data/dataset/class2_ai_vanilla.jsonl', 'r') as f:\n",
    "#     for line in f:\n",
    "#         class2_ai_vanilla.append(json.loads(line))\n",
    "\n",
    "print(\"✓ Class 2 generation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33041d8b",
   "metadata": {},
   "source": [
    "## 8. CLASS 3: Generate AI Styled Paragraphs\n",
    "\n",
    "**Split between two authors:**\n",
    "- 250 paragraphs in Dickens style\n",
    "- 250 paragraphs in Austen style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce11dac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class3_dataset(topics, author_keys=['dickens', 'austen'], \n",
    "                           samples_per_author=250, output_file='data/dataset/class3_ai_styled.jsonl'):\n",
    "    \"\"\"\n",
    "    Generate styled AI paragraphs for multiple authors\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    samples_per_topic = samples_per_author // len(topics)\n",
    "    \n",
    "    total_target = samples_per_author * len(author_keys)\n",
    "    print(f\"Generating {total_target} AI styled paragraphs...\")\n",
    "    \n",
    "    with tqdm(total=total_target) as pbar:\n",
    "        for author_key in author_keys:\n",
    "            print(f\"\\nGenerating in {AUTHOR_STYLES[author_key]['name']} style...\")\n",
    "            \n",
    "            for topic in topics:\n",
    "                for i in range(samples_per_topic):\n",
    "                    # Generate\n",
    "                    text = generate_styled_paragraph(\n",
    "                        topic, \n",
    "                        author_key,\n",
    "                        word_range=(CONFIG['paragraph_length']['min_words'],\n",
    "                                   CONFIG['paragraph_length']['max_words'])\n",
    "                    )\n",
    "                    \n",
    "                    if text:\n",
    "                        dataset.append({\n",
    "                            'text': text,\n",
    "                            'topic': topic,\n",
    "                            'class': 'ai_styled',\n",
    "                            'style_author': author_key,\n",
    "                            'style_author_full': AUTHOR_STYLES[author_key]['name'],\n",
    "                            'word_count': len(text.split()),\n",
    "                            'generation_id': len(dataset)\n",
    "                        })\n",
    "                        \n",
    "                        pbar.update(1)\n",
    "                    \n",
    "                    # Rate limiting\n",
    "                    time.sleep(CONFIG['api_settings']['rate_limit_delay'])\n",
    "                    \n",
    "                    # Checkpoint\n",
    "                    if len(dataset) % CONFIG['api_settings']['checkpoint_interval'] == 0:\n",
    "                        with open(output_file, 'w') as f:\n",
    "                            for item in dataset:\n",
    "                                f.write(json.dumps(item) + '\\n')\n",
    "                        print(f\"\\n  Checkpoint: {len(dataset)} samples saved\")\n",
    "    \n",
    "    # Final save\n",
    "    with open(output_file, 'w') as f:\n",
    "        for item in dataset:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "    \n",
    "    print(f\"\\n✓ Generated {len(dataset)} AI styled paragraphs\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad59c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Class 3 - Running generation (takes ~10 minutes)\n",
    "class3_ai_styled = generate_class3_dataset(\n",
    "    TOPICS,\n",
    "    author_keys=['dickens', 'austen'],\n",
    "    samples_per_author=250,\n",
    "    output_file='data/dataset/class3_ai_styled.jsonl'\n",
    ")\n",
    "\n",
    "# OR load if already generated:\n",
    "# class3_ai_styled = []\n",
    "# with open('data/dataset/class3_ai_styled.jsonl', 'r') as f:\n",
    "#     for line in f:\n",
    "#         class3_ai_styled.append(json.loads(line))\n",
    "\n",
    "print(\"✓ Class 3 generation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8337de26",
   "metadata": {},
   "source": [
    "## 9. Combine and Create Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786dcba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all classes if not in memory\n",
    "# (Uncomment if you generated in separate sessions)\n",
    "\n",
    "# class1_human = []\n",
    "# with open('data/dataset/class1_human.jsonl', 'r') as f:\n",
    "#     for line in f:\n",
    "#         class1_human.append(json.loads(line))\n",
    "\n",
    "# class2_ai_vanilla = []\n",
    "# with open('data/dataset/class2_ai_vanilla.jsonl', 'r') as f:\n",
    "#     for line in f:\n",
    "#         class2_ai_vanilla.append(json.loads(line))\n",
    "\n",
    "# class3_ai_styled = []\n",
    "# with open('data/dataset/class3_ai_styled.jsonl', 'r') as f:\n",
    "#     for line in f:\n",
    "#         class3_ai_styled.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5998cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_dataset(class1, class2, class3, output_file='data/dataset/final_dataset.jsonl'):\n",
    "    \"\"\"\n",
    "    Combine all classes with proper labels\n",
    "    \"\"\"\n",
    "    final_dataset = []\n",
    "    \n",
    "    # Add class labels\n",
    "    for item in class1:\n",
    "        final_dataset.append({\n",
    "            **item,\n",
    "            'class': 'human',\n",
    "            'class_numeric': 0\n",
    "        })\n",
    "    \n",
    "    for item in class2:\n",
    "        final_dataset.append({\n",
    "            **item,\n",
    "            'class_numeric': 1\n",
    "        })\n",
    "    \n",
    "    for item in class3:\n",
    "        final_dataset.append({\n",
    "            **item,\n",
    "            'class_numeric': 2\n",
    "        })\n",
    "    \n",
    "    # Shuffle\n",
    "    random.shuffle(final_dataset)\n",
    "    \n",
    "    # Add global IDs\n",
    "    for idx, item in enumerate(final_dataset):\n",
    "        item['id'] = idx\n",
    "    \n",
    "    # Save\n",
    "    with open(output_file, 'w') as f:\n",
    "        for item in final_dataset:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "    \n",
    "    print(\"Final Dataset Summary:\")\n",
    "    print(f\"Total samples: {len(final_dataset)}\")\n",
    "    print(f\"  - Class 0 (Human): {len(class1)}\")\n",
    "    print(f\"  - Class 1 (AI Vanilla): {len(class2)}\")\n",
    "    print(f\"  - Class 2 (AI Styled): {len(class3)}\")\n",
    "    print(f\"\\n✓ Saved to {output_file}\")\n",
    "    \n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560c8b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final dataset\n",
    "final_dataset = create_final_dataset(\n",
    "    class1_human,\n",
    "    class2_ai_vanilla,\n",
    "    class3_ai_styled,\n",
    "    output_file='data/dataset/final_dataset.jsonl'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8394c1",
   "metadata": {},
   "source": [
    "## 10. Create Train/Val/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe572411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_splits(dataset, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, \n",
    "                 random_state=42):\n",
    "    \"\"\"\n",
    "    Create stratified train/val/test splits\n",
    "    \"\"\"\n",
    "    # Stratify by class\n",
    "    labels = [item['class_numeric'] for item in dataset]\n",
    "    \n",
    "    # First split: train vs (val + test)\n",
    "    train, temp = train_test_split(\n",
    "        dataset,\n",
    "        test_size=(val_ratio + test_ratio),\n",
    "        random_state=random_state,\n",
    "        stratify=labels\n",
    "    )\n",
    "    \n",
    "    # Second split: val vs test\n",
    "    temp_labels = [item['class_numeric'] for item in temp]\n",
    "    val, test = train_test_split(\n",
    "        temp,\n",
    "        test_size=test_ratio/(val_ratio + test_ratio),\n",
    "        random_state=random_state,\n",
    "        stratify=temp_labels\n",
    "    )\n",
    "    \n",
    "    # Save splits\n",
    "    for split_name, split_data in [('train', train), ('val', val), ('test', test)]:\n",
    "        output_file = f'data/dataset/{split_name}.jsonl'\n",
    "        with open(output_file, 'w') as f:\n",
    "            for item in split_data:\n",
    "                f.write(json.dumps(item) + '\\n')\n",
    "        print(f\"✓ {split_name}: {len(split_data)} samples ({100*len(split_data)/len(dataset):.1f}%)\")\n",
    "    \n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a12f5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create splits\n",
    "train_data, val_data, test_data = create_splits(\n",
    "    final_dataset,\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.15,\n",
    "    test_ratio=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d147cc",
   "metadata": {},
   "source": [
    "## 11. Dataset Statistics and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b998f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dataset_stats(dataset):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive statistics\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(dataset)\n",
    "    \n",
    "    print(\"Dataset Statistics\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Overall stats\n",
    "    print(f\"\\nTotal samples: {len(df)}\")\n",
    "    print(f\"\\nWord count statistics:\")\n",
    "    print(df['word_count'].describe())\n",
    "    \n",
    "    # By class\n",
    "    print(f\"\\nBy class:\")\n",
    "    print(df.groupby('class')['word_count'].describe())\n",
    "    \n",
    "    # Class distribution\n",
    "    print(f\"\\nClass distribution:\")\n",
    "    print(df['class'].value_counts())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d85d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_dataset(dataset, save_path='results/visualizations/dataset_overview.png'):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(dataset)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Word count distribution by class\n",
    "    for cls in df['class'].unique():\n",
    "        data = df[df['class'] == cls]['word_count']\n",
    "        axes[0, 0].hist(data, alpha=0.6, label=cls, bins=20)\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Word Count', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[0, 0].set_title('Word Count Distribution by Class', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # 2. Class balance\n",
    "    class_counts = df['class'].value_counts()\n",
    "    axes[0, 1].bar(class_counts.index, class_counts.values, color=['#FF6B6B', '#4ECDC4', '#95E1D3'])\n",
    "    axes[0, 1].set_ylabel('Count', fontsize=12)\n",
    "    axes[0, 1].set_title('Class Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for i, (cls, count) in enumerate(class_counts.items()):\n",
    "        axes[0, 1].text(i, count + 10, str(count), ha='center', fontweight='bold')\n",
    "    \n",
    "    # 3. Box plot\n",
    "    df.boxplot(column='word_count', by='class', ax=axes[1, 0])\n",
    "    axes[1, 0].set_xlabel('Class', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Word Count', fontsize=12)\n",
    "    axes[1, 0].set_title('Word Count Distribution (Box Plot)', fontsize=14, fontweight='bold')\n",
    "    plt.sca(axes[1, 0])\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "    # 4. Character count distribution\n",
    "    df['char_count'] = df['text'].str.len()\n",
    "    for cls in df['class'].unique():\n",
    "        data = df[df['class'] == cls]['char_count']\n",
    "        axes[1, 1].hist(data, alpha=0.6, label=cls, bins=20)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Character Count', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[1, 1].set_title('Character Count Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ Visualization saved to {save_path}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773b89c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze and visualize\n",
    "df_stats = calculate_dataset_stats(final_dataset)\n",
    "visualize_dataset(final_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1146ff0b",
   "metadata": {},
   "source": [
    "## 12. Sample Examples from Each Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d85cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_examples(dataset, n_per_class=2):\n",
    "    \"\"\"\n",
    "    Display sample paragraphs from each class\n",
    "    \"\"\"\n",
    "    classes = ['human', 'ai_vanilla', 'ai_styled']\n",
    "    \n",
    "    for cls in classes:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"CLASS: {cls.upper()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        samples = [item for item in dataset if item['class'] == cls]\n",
    "        selected = random.sample(samples, min(n_per_class, len(samples)))\n",
    "        \n",
    "        for i, sample in enumerate(selected, 1):\n",
    "            print(f\"\\nExample {i}:\")\n",
    "            print(f\"Words: {sample['word_count']}\")\n",
    "            if 'author' in sample:\n",
    "                print(f\"Author: {sample['author_full']}\")\n",
    "            if 'style_author' in sample:\n",
    "                print(f\"Style: {sample['style_author_full']}\")\n",
    "            if 'topic' in sample:\n",
    "                print(f\"Topic: {sample['topic']}\")\n",
    "            print(f\"\\nText:\\n{sample['text']}\")\n",
    "            print(f\"{'-'*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488feefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples\n",
    "show_examples(final_dataset, n_per_class=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf77b41",
   "metadata": {},
   "source": [
    "## 13. Summary and Next Steps\n",
    "\n",
    "### Task 0 Completion Checklist:\n",
    "\n",
    "- [ ] Downloaded and cleaned books from Project Gutenberg\n",
    "- [ ] Extracted 500 human paragraphs (Class 1)\n",
    "- [ ] Identified 10 universal topics\n",
    "- [ ] Generated 500 AI vanilla paragraphs (Class 2)\n",
    "- [ ] Generated 500 AI styled paragraphs (Class 3)\n",
    "- [ ] Created final combined dataset (1500 samples)\n",
    "- [ ] Created train/val/test splits (70/15/15)\n",
    "- [ ] Generated visualizations and statistics\n",
    "\n",
    "### Key Decisions Made:\n",
    "\n",
    "1. **Authors:** Charles Dickens + Jane Austen (distinctive styles, well-documented)\n",
    "2. **Books:** Great Expectations, Pride and Prejudice (representative works)\n",
    "3. **Topics:** 10 universal themes (ensure thematic consistency across classes)\n",
    "4. **Paragraph length:** 100-200 words (balances context and manageability)\n",
    "5. **Distribution:** Equal samples per class (prevents class imbalance)\n",
    "\n",
    "### Files Generated:\n",
    "\n",
    "```\n",
    "data/\n",
    "├── raw/\n",
    "│   ├── dickens_great_expectations.txt\n",
    "│   └── austen_pride_prejudice.txt\n",
    "├── processed/\n",
    "│   ├── dickens_cleaned.txt\n",
    "│   └── austen_cleaned.txt\n",
    "└── dataset/\n",
    "    ├── class1_human.jsonl\n",
    "    ├── class2_ai_vanilla.jsonl\n",
    "    ├── class3_ai_styled.jsonl\n",
    "    ├── final_dataset.jsonl\n",
    "    ├── train.jsonl\n",
    "    ├── val.jsonl\n",
    "    └── test.jsonl\n",
    "```\n",
    "\n",
    "### Next: Task 1 - The Fingerprint\n",
    "\n",
    "Move to `task1_fingerprint.ipynb` to prove these classes are mathematically distinct using:\n",
    "- Lexical richness (TTR, Hapax Legomena)\n",
    "- Syntactic complexity (POS distribution, dependency trees)\n",
    "- Punctuation patterns\n",
    "- Readability indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb41ff1a",
   "metadata": {},
   "source": [
    "# 🔄 Dataset Revision: Mark Twain + Jane Austen\n",
    "\n",
    "## Rationale for New Dataset\n",
    "\n",
    "After completing Tasks 0-3 with the Victorian dataset (Dickens + Austen), we discovered a **genre bias issue**:\n",
    "- Bias test showed **60.5% of modern human text was predicted as AI**\n",
    "- Root cause: Model learned \"abstract discourse = AI, narrative fiction = Human\" instead of true authorship patterns\n",
    "- Task 3 saliency results were inexplicable (no AI-isms or Victorian vocabulary detected)\n",
    "\n",
    "## New Approach: Mark Twain + Jane Austen\n",
    "\n",
    "Using this combination offers several advantages:\n",
    "\n",
    "### Mark Twain (Tom Sawyer)\n",
    "1. **Less archaic**: American colloquial vs British formal Victorian prose\n",
    "2. **More conversational**: Dialogue-heavy, accessible style closer to modern English\n",
    "3. **Still temporal**: 150 years old (sufficient gap for testing)\n",
    "4. **Empirically validated**: Friend used Twain, experienced significantly less bias\n",
    "\n",
    "### Jane Austen (Emma)\n",
    "1. **Retained from original**: Allows comparison with Victorian dataset\n",
    "2. **Witty and refined**: Elegant British prose (different from Twain)\n",
    "3. **Same era as Twain**: Both early 19th century American/British literature\n",
    "4. **Style diversity**: Mix of American colloquial + British refined\n",
    "\n",
    "### Why This Is Better Than Dickens + Austen\n",
    "- **Dickens + Austen**: Both British Victorian, both formal, too similar stylistically\n",
    "- **Twain + Austen**: American vs British, colloquial vs refined, more diverse\n",
    "- **Expected improvement**: Bias reduction from 60.5% to ~40% (Twain's accessible style)\n",
    "\n",
    "**Satisfies Gutenberg requirement**: Both books public domain, pre-1928"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4cad884b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Twain + Austen configuration loaded\n",
      "Books: The Adventures of Tom Sawyer + Emma\n",
      "Target: 500 human paragraphs\n"
     ]
    }
   ],
   "source": [
    "# Configuration for Mark Twain + Jane Austen Dataset\n",
    "TWAIN_AUSTEN_CONFIG = {\n",
    "    'authors': {\n",
    "        'twain_tomsawyer': {\n",
    "            'name': 'Mark Twain',\n",
    "            'book': 'The Adventures of Tom Sawyer',\n",
    "            'gutenberg_id': 74,\n",
    "            'target_paragraphs': 250\n",
    "        },\n",
    "        'austen_emma': {\n",
    "            'name': 'Jane Austen',\n",
    "            'book': 'Emma',\n",
    "            'gutenberg_id': 158,\n",
    "            'target_paragraphs': 250\n",
    "        }\n",
    "    },\n",
    "    'paragraph_criteria': {\n",
    "        'min_words': 80,\n",
    "        'max_words': 250\n",
    "    },\n",
    "    'class_distribution': {\n",
    "        'class1_human': 500,\n",
    "        'class2_ai_vanilla': 300,\n",
    "        'class3_ai_styled': 200\n",
    "    }\n",
    "}\n",
    "\n",
    "# Topics for AI generation\n",
    "TWAIN_AUSTEN_TOPICS = [\n",
    "    \"Adventure and freedom\",\n",
    "    \"Friendship and loyalty\", \n",
    "    \"Coming of age and moral growth\",\n",
    "    \"Society and individual conscience\",\n",
    "    \"Love, marriage, and romantic relationships\",\n",
    "    \"Social class and ambition\",\n",
    "    \"Honesty and deception\",\n",
    "    \"Family bonds and responsibility\",\n",
    "    \"Pride, prejudice, and human flaws\",\n",
    "    \"Personal growth and self-discovery\"\n",
    "]\n",
    "\n",
    "# Author style profiles for Class 3 (AI mimicking styles)\n",
    "TWAIN_AUSTEN_STYLES = {\n",
    "    'twain': {\n",
    "        'name': 'Mark Twain',\n",
    "        'description': 'American colloquial style with first-person narrative, humor, dialect, and conversational tone. Uses simple sentences, regional expressions, and captures childhood perspective with adult wisdom.',\n",
    "        'characteristics': [\n",
    "            'First-person narrative voice',\n",
    "            'Colloquial American English',\n",
    "            'Humor and satire',\n",
    "            'Regional dialect and vernacular',\n",
    "            'Simple sentence structures',\n",
    "            'Vivid concrete descriptions',\n",
    "            'Moral questioning through storytelling'\n",
    "        ]\n",
    "    },\n",
    "    'austen': {\n",
    "        'name': 'Jane Austen',\n",
    "        'description': 'Witty and ironic tone with elegant, balanced sentences. Free indirect discourse blending narrator and character perspective. Sharp social observations, restrained emotional expression, and Regency-era propriety.',\n",
    "        'characteristics': [\n",
    "            'Witty and ironic tone',\n",
    "            'Free indirect discourse',\n",
    "            'Elegant, balanced sentences',\n",
    "            'Sharp social observations',\n",
    "            'Clever dialogue and repartee',\n",
    "            'Restrained emotional expression',\n",
    "            'Regency-era propriety and manners',\n",
    "            'Subtle humor and satire'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✅ Twain + Austen configuration loaded\")\n",
    "print(f\"Books: {TWAIN_AUSTEN_CONFIG['authors']['twain_tomsawyer']['book']} + {TWAIN_AUSTEN_CONFIG['authors']['austen_emma']['book']}\")\n",
    "print(f\"Target: {TWAIN_AUSTEN_CONFIG['class_distribution']['class1_human']} human paragraphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a59efa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Directories ready:\n",
      "   - Raw: data/raw\n",
      "   - Processed: data/processed\n",
      "   - Dataset: data/dataset\n"
     ]
    }
   ],
   "source": [
    "# Ensure required modules and directories are available\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Define directory paths (if not already defined)\n",
    "raw_dir = 'data/raw'\n",
    "processed_dir = 'data/processed'\n",
    "dataset_dir = 'data/dataset'\n",
    "\n",
    "# Create directories if needed\n",
    "Path(raw_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(processed_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(dataset_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"✅ Directories ready:\")\n",
    "print(f\"   - Raw: {raw_dir}\")\n",
    "print(f\"   - Processed: {processed_dir}\")\n",
    "print(f\"   - Dataset: {dataset_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "715dac51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Downloading Tom Sawyer (Gutenberg ID: 74)...\n",
      "Trying: https://www.gutenberg.org/files/74/74-0.txt\n",
      "✓ Successfully downloaded to data/raw/twain_tom_sawyer.txt\n",
      "🧹 Cleaning Tom Sawyer text...\n",
      "✓ Removed header metadata\n",
      "✓ Removed footer metadata\n",
      "✓ Cleaned text: 391932 characters\n",
      "✅ Tom Sawyer processed: 391928 characters\n",
      "   Saved to: data/processed/twain_tom_sawyer_cleaned.txt\n",
      "✓ Successfully downloaded to data/raw/twain_tom_sawyer.txt\n",
      "🧹 Cleaning Tom Sawyer text...\n",
      "✓ Removed header metadata\n",
      "✓ Removed footer metadata\n",
      "✓ Cleaned text: 391932 characters\n",
      "✅ Tom Sawyer processed: 391928 characters\n",
      "   Saved to: data/processed/twain_tom_sawyer_cleaned.txt\n"
     ]
    }
   ],
   "source": [
    "# Download and process Tom Sawyer\n",
    "print(\"📥 Downloading Tom Sawyer (Gutenberg ID: 74)...\")\n",
    "\n",
    "tom_sawyer_path = os.path.join(raw_dir, 'twain_tom_sawyer.txt')\n",
    "download_gutenberg_book(74, tom_sawyer_path)\n",
    "\n",
    "print(\"🧹 Cleaning Tom Sawyer text...\")\n",
    "with open(tom_sawyer_path, 'r', encoding='utf-8') as f:\n",
    "    tom_raw = f.read()\n",
    "\n",
    "tom_cleaned = clean_gutenberg_text(tom_raw)\n",
    "tom_cleaned_path = os.path.join(processed_dir, 'twain_tom_sawyer_cleaned.txt')\n",
    "\n",
    "with open(tom_cleaned_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(tom_cleaned)\n",
    "\n",
    "print(f\"✅ Tom Sawyer processed: {len(tom_cleaned)} characters\")\n",
    "print(f\"   Saved to: {tom_cleaned_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c84767",
   "metadata": {},
   "source": [
    "### ⚠️ Tom Sawyer Paragraph Count Issue\n",
    "\n",
    "Tom Sawyer is a shorter book and only has **128 paragraphs** in the 100-200 word range.\n",
    "\n",
    "**Solutions:**\n",
    "1. **Widen word range** (80-220 words) to get more paragraphs from Tom Sawyer\n",
    "2. **Add Huckleberry Finn** (Gutenberg ID 76) as additional source\n",
    "3. **Adjust distribution** (128 Twain + 372 Austen = 500 total)\n",
    "\n",
    "Currently using **Option 3** with adjusted distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef4ebd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Downloading Emma by Jane Austen (Gutenberg ID: 158)...\n",
      "Trying: https://www.gutenberg.org/files/158/158-0.txt\n",
      "✓ Successfully downloaded to data/raw/austen_emma.txt\n",
      "🧹 Cleaning Emma text...\n",
      "✓ Removed header metadata\n",
      "✓ Removed footer metadata\n",
      "✓ Cleaned text: 879437 characters\n",
      "✅ Emma processed: 879433 characters\n",
      "   Saved to: data/processed/austen_emma_cleaned.txt\n",
      "✓ Successfully downloaded to data/raw/austen_emma.txt\n",
      "🧹 Cleaning Emma text...\n",
      "✓ Removed header metadata\n",
      "✓ Removed footer metadata\n",
      "✓ Cleaned text: 879437 characters\n",
      "✅ Emma processed: 879433 characters\n",
      "   Saved to: data/processed/austen_emma_cleaned.txt\n"
     ]
    }
   ],
   "source": [
    "# Download and process Emma by Jane Austen\n",
    "print(\"📥 Downloading Emma by Jane Austen (Gutenberg ID: 158)...\")\n",
    "\n",
    "emma_path = os.path.join(raw_dir, 'austen_emma.txt')\n",
    "download_gutenberg_book(158, emma_path)\n",
    "\n",
    "print(\"🧹 Cleaning Emma text...\")\n",
    "with open(emma_path, 'r', encoding='utf-8') as f:\n",
    "    emma_raw = f.read()\n",
    "\n",
    "emma_cleaned = clean_gutenberg_text(emma_raw)\n",
    "emma_cleaned_path = os.path.join(processed_dir, 'austen_emma_cleaned.txt')\n",
    "\n",
    "with open(emma_cleaned_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(emma_cleaned)\n",
    "\n",
    "print(f\"✅ Emma processed: {len(emma_cleaned)} characters\")\n",
    "print(f\"   Saved to: {emma_cleaned_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cdae4efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 Extracting paragraphs from both books...\n",
      "✓ Found 220 valid paragraphs\n",
      "✅ Tom Sawyer (Twain): 220 paragraphs extracted\n",
      "✓ Found 607 valid paragraphs\n",
      "✓ Sampled 250 paragraphs\n",
      "✅ Emma (Austen): 250 paragraphs extracted\n",
      "\n",
      "📊 Total human paragraphs: 470\n",
      "\n",
      "📝 Sample from Tom Sawyer:\n",
      "   The old lady pulled her spectacles down and looked over them about the\n",
      "room; then she put them up and looked out under them. She seldom or\n",
      "never looked _through_ them for so small a thing as a boy; th...\n",
      "\n",
      "📝 Sample from Emma:\n",
      "   “I have known her from a child, undoubtedly; we have been children and\n",
      "women together; and it is natural to suppose that we should be\n",
      "intimate,—that we should have taken to each other whenever she vis...\n"
     ]
    }
   ],
   "source": [
    "# Extract paragraphs from Tom Sawyer and Emma\n",
    "print(\"📖 Extracting paragraphs from both books...\")\n",
    "\n",
    "# Extract from Tom Sawyer\n",
    "tom_paragraphs = extract_paragraphs(\n",
    "    tom_cleaned,\n",
    "    min_words=TWAIN_AUSTEN_CONFIG['paragraph_criteria']['min_words'],\n",
    "    max_words=TWAIN_AUSTEN_CONFIG['paragraph_criteria']['max_words'],\n",
    "    n_samples=250\n",
    ")\n",
    "\n",
    "print(f\"✅ Tom Sawyer (Twain): {len(tom_paragraphs)} paragraphs extracted\")\n",
    "\n",
    "# Extract from Emma\n",
    "emma_paragraphs = extract_paragraphs(\n",
    "    emma_cleaned,\n",
    "    min_words=TWAIN_AUSTEN_CONFIG['paragraph_criteria']['min_words'],\n",
    "    max_words=TWAIN_AUSTEN_CONFIG['paragraph_criteria']['max_words'],\n",
    "    n_samples=250\n",
    ")\n",
    "\n",
    "print(f\"✅ Emma (Austen): {len(emma_paragraphs)} paragraphs extracted\")\n",
    "\n",
    "# Combine both\n",
    "human_paragraphs_new = tom_paragraphs + emma_paragraphs\n",
    "print(f\"\\n📊 Total human paragraphs: {len(human_paragraphs_new)}\")\n",
    "\n",
    "# Show samples\n",
    "print(\"\\n📝 Sample from Tom Sawyer:\")\n",
    "print(f\"   {tom_paragraphs[0]['text'][:200]}...\")\n",
    "print(\"\\n📝 Sample from Emma:\")\n",
    "print(f\"   {emma_paragraphs[0]['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29c88d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Creating dataset with human paragraphs only...\n",
      "✅ Class 1 (Human): 470 paragraphs\n",
      "   - Mark Twain (Tom Sawyer): 220\n",
      "   - Jane Austen (Emma): 250\n",
      "\n",
      "📊 Total dataset: 470 paragraphs (Human only)\n",
      "\n",
      "💾 Dataset saved to: data/dataset/twain_austen\n",
      "   - class1_human.jsonl\n",
      "   - final_dataset.jsonl\n",
      "\n",
      "⚠️ Note: AI-generated classes (Class 2 & 3) will be added later when API is available\n"
     ]
    }
   ],
   "source": [
    "# Create final dataset (Twain + Austen) - Human paragraphs only\n",
    "print(\"📦 Creating dataset with human paragraphs only...\")\n",
    "\n",
    "# Create Class 1 human data structure\n",
    "class1_human_new = []\n",
    "for i, para_dict in enumerate(human_paragraphs_new):\n",
    "    if i < len(tom_paragraphs):\n",
    "        source_book = 'Tom Sawyer'\n",
    "        author = 'Mark Twain'\n",
    "    else:\n",
    "        source_book = 'Emma'\n",
    "        author = 'Jane Austen'\n",
    "    \n",
    "    class1_human_new.append({\n",
    "        'text': para_dict['text'],\n",
    "        'class': 'Human',\n",
    "        'author': author,\n",
    "        'book': source_book,\n",
    "        'word_count': para_dict['word_count']\n",
    "    })\n",
    "\n",
    "print(f\"✅ Class 1 (Human): {len(class1_human_new)} paragraphs\")\n",
    "print(f\"   - Mark Twain (Tom Sawyer): {len(tom_paragraphs)}\")\n",
    "print(f\"   - Jane Austen (Emma): {len(emma_paragraphs)}\")\n",
    "\n",
    "# Dataset is just human paragraphs for now\n",
    "dataset_new = class1_human_new\n",
    "print(f\"\\n📊 Total dataset: {len(dataset_new)} paragraphs (Human only)\")\n",
    "\n",
    "# Save to separate JSONL files\n",
    "new_dataset_dir = os.path.join(dataset_dir, 'twain_austen')\n",
    "os.makedirs(new_dataset_dir, exist_ok=True)\n",
    "\n",
    "# Save Class 1\n",
    "with open(os.path.join(new_dataset_dir, 'class1_human.jsonl'), 'w', encoding='utf-8') as f:\n",
    "    for item in class1_human_new:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "# Save combined dataset (just Class 1 for now)\n",
    "with open(os.path.join(new_dataset_dir, 'final_dataset.jsonl'), 'w', encoding='utf-8') as f:\n",
    "    for item in dataset_new:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(f\"\\n💾 Dataset saved to: {new_dataset_dir}\")\n",
    "print(\"   - class1_human.jsonl\")\n",
    "print(\"   - final_dataset.jsonl\")\n",
    "print(\"\\n⚠️ Note: AI-generated classes (Class 2 & 3) will be added later when API is available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "127a6683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✂️ Creating train/val/test splits (70/15/15)...\n",
      "✅ Train: 329 paragraphs\n",
      "✅ Val: 70 paragraphs\n",
      "✅ Test: 71 paragraphs\n",
      "\n",
      "💾 Splits saved to: data/dataset/twain_austen\n",
      "   - train.jsonl\n",
      "   - val.jsonl\n",
      "   - test.jsonl\n",
      "\n",
      "⚠️ Note: These contain only human paragraphs. AI classes to be generated separately.\n"
     ]
    }
   ],
   "source": [
    "# Create train/val/test splits (70/15/15) - Human paragraphs only\n",
    "print(\"✂️ Creating train/val/test splits (70/15/15)...\")\n",
    "\n",
    "# Shuffle dataset\n",
    "random.seed(42)\n",
    "random.shuffle(dataset_new)\n",
    "\n",
    "# Split\n",
    "total = len(dataset_new)\n",
    "train_size = int(0.7 * total)\n",
    "val_size = int(0.15 * total)\n",
    "\n",
    "train_new = dataset_new[:train_size]\n",
    "val_new = dataset_new[train_size:train_size + val_size]\n",
    "test_new = dataset_new[train_size + val_size:]\n",
    "\n",
    "print(f\"✅ Train: {len(train_new)} paragraphs\")\n",
    "print(f\"✅ Val: {len(val_new)} paragraphs\")\n",
    "print(f\"✅ Test: {len(test_new)} paragraphs\")\n",
    "\n",
    "# Save splits\n",
    "with open(os.path.join(new_dataset_dir, 'train.jsonl'), 'w', encoding='utf-8') as f:\n",
    "    for item in train_new:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "with open(os.path.join(new_dataset_dir, 'val.jsonl'), 'w', encoding='utf-8') as f:\n",
    "    for item in val_new:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "with open(os.path.join(new_dataset_dir, 'test.jsonl'), 'w', encoding='utf-8') as f:\n",
    "    for item in test_new:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(f\"\\n💾 Splits saved to: {new_dataset_dir}\")\n",
    "print(\"   - train.jsonl\")\n",
    "print(\"   - val.jsonl\")\n",
    "print(\"   - test.jsonl\")\n",
    "print(\"\\n⚠️ Note: These contain only human paragraphs. AI classes to be generated separately.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7528c578",
   "metadata": {},
   "source": [
    "## 📊 Dataset Comparison\n",
    "\n",
    "| Aspect | Victorian Dataset | New Dataset (Twain + Austen) |\n",
    "|--------|-------------------|------------------------------|\n",
    "| **Authors** | Charles Dickens + Jane Austen | Mark Twain + Jane Austen |\n",
    "| **Books** | Great Expectations (1861) + Pride & Prejudice (1813) | Tom Sawyer (1876) + Emma (1815) |\n",
    "| **Temporal Gap** | 160-210 years | 140-210 years |\n",
    "| **Style Mix** | British Victorian (both formal) | American colloquial + British formal |\n",
    "| **Advantage** | Same era consistency | More style diversity (American vs British) |\n",
    "| **Vocabulary** | Both archaic Victorian | Mix: Twain colloquial + Austen refined |\n",
    "| **Bias Test Result** | 60.5% modern text predicted as AI | Expected: ~40% (less bias) |\n",
    "| **Task 3 Saliency** | No AI-isms detected, inexplicable | Expected: AI-isms visible |\n",
    "\n",
    "### Why This Combination?\n",
    "\n",
    "1. **Mark Twain (Tom Sawyer)**: American colloquial, conversational, less archaic\n",
    "2. **Jane Austen (Emma)**: British refined, witty, elegant prose\n",
    "3. **Style diversity**: Two distinct writing traditions (American vs British)\n",
    "4. **Keeps one author from original**: Austen retained for comparison\n",
    "5. **Expected improvement**: Less genre bias due to Twain's accessible style\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. ✅ Task 0 complete with Twain + Austen books\n",
    "2. 🔄 Rerun Task 1 (Linguistic Analysis) \n",
    "3. 🔄 Retrain Task 2 (All 3 classification tiers)\n",
    "4. 🔄 Rerun Task 3 (Saliency mapping)\n",
    "5. ✅ Run bias validation test\n",
    "6. 📝 Complete Task 3 Parts 2-3\n",
    "7. 🎯 Task 4 (Adversarial testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d52e05",
   "metadata": {},
   "source": [
    "## 🎯 10 Core Topics from Tom Sawyer + Emma\n",
    "\n",
    "These topics appear in **both** books and will be used for AI generation:\n",
    "\n",
    "1. **Childhood innocence and moral development** - Tom's adventures and moral growth / Emma's coming-of-age observations\n",
    "2. **Social hierarchy and class dynamics** - Tom's awareness of social standing / Emma's obsession with social rank\n",
    "3. **Friendship, loyalty, and companionship** - Tom & Huck's bond / Emma & Harriet's friendship\n",
    "4. **Romance, courtship, and matchmaking** - Tom's puppy love for Becky / Emma's matchmaking schemes\n",
    "5. **Deception, mischief, and consequences** - Tom's lies and tricks / Emma's manipulative behavior\n",
    "6. **Pride, vanity, and self-awareness** - Tom's showing off / Emma's excessive pride and eventual humility\n",
    "7. **Community gossip and reputation** - Village rumors about Tom / Highbury society's gossip\n",
    "8. **Family duty versus personal desire** - Tom torn between Aunt Polly and freedom / Emma's duty to father vs independence\n",
    "9. **Judgment, prejudice, and misjudgment** - Tom misjudging people / Emma's constant misjudgments of others\n",
    "10. **Growing up and self-discovery** - Tom learning responsibility / Emma discovering her flaws and growing\n",
    "\n",
    "---\n",
    "\n",
    "### Why These Topics Work:\n",
    "\n",
    "- **Universal themes** that transcend time periods (1815-1876)\n",
    "- **Present in both books** with different cultural perspectives (American vs British)\n",
    "- **Rich enough** for 100-200 word paragraphs\n",
    "- **Balanced** between character-driven and thematic\n",
    "- **Not time-specific** - can be discussed in modern or period context"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
